---
title: "Accessibility Calculator for R"
author:
  - name: "Christopher D. Higgins"
    email: cd.higgins@utoronto.ca
    affiliation: University of Toronto
---

Template source: https://github.com/higgicd/Accessibility_Toolbox/

# ABSTRACT
Analyses of place-based accessibility undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. Moreover, questions persist over the selection of an impedance function and cost or cut-off parameters. In response, this paper details a new Accessibility Toolbox for R and ArcGIS that includes a Python tool for conducting accessibility analyses and an interactive R Notebook that enables the visualization and customization of impedance functions and parameters. Using this toolbox, researchers and practitioners can simplify their accessibility analysis workflow and make better decisions about the specification and customization of travel impedance for their study context.

# KEYWORDS
accessibility, spatial interaction, travel behavior, travel impedance, distance decay function

```{r setup, include=FALSE}
# the r5r package requires installation of Java Development Kit version 21
# Installation instructions available at https://github.com/ipeaGIT/r5r

# assign memory for r5r to work with
options(java.parameters = "-Xmx6G") 

# load packages
library(corrplot)
library(httr)
library(jsonlite)
library(knitr)
library(lehdr)
library(tigris)
library(tidyverse)
library(fs)
library(r5r)
library(sf)
library(tmap)
library(tidycensus)
library(tidytransit)
library(viridis)
library(progress)
library(arrow)
library(duckdb)

# setup options
tmap_mode("plot")

# setup keys, creating or editing .Renviron file as needed 
# Guidance on .Renviron: https://laurenilano.com/posts/api-keys/#creating-the-.renviron-file
census_api_key(Sys.getenv("CENSUS_API_KEY")) # get from https://api.census.gov/data/key_signup.html
interline_token <- Sys.getenv("OSM_KEY") # for OSM extract from https://www.interline.io/osm/extracts/
transitland_key <- Sys.getenv("TRANSITLAND_KEY") # to get GTFS feeds from Transitland (https://www.transit.land/terms)

# create data directories (does not overwrite if these folders are already present)
data_path <- dir_create("./data") # for storing data
ttm_path <- dir_create("./r5_ttm") # for storing od matrix
r5_path <- dir_create("./r5_graph") # for the r5 network graph
```

# RESEARCH QUESTIONS AND HYPOTHESES
Accessibility can be defined as the potential for reaching spatially distributed opportunities while considering the difficulty involved in traveling to them (Páez, Scott, and Morency, 2012). Several families of accessibility measures have been established since the pioneering work of Hansen (1959), including infrastructure-based, person-based, place-based, and utility-based (Geurs and van Wee, 2004). Of these, place-based measures are arguably the most common and can be operationalized as:

$$
A_i = \sum_{j}{O_jf(t_{ij})}
$$
where the accessibility $A$ of origin $i$ is the sum of all opportunities $O$ available at destinations $j$ weighted by some function of the travel time $t_{ij}$ between $i$ and $j$.

Despite several decades of research into place-based accessibility, researchers, students, and practitioners interested in accessibility analysis face practical and empirical challenges. On the practical side, compared to simple isochrones, analyses of spatial interaction undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. On the empirical side, questions persist over the selection of an impedance function and cost or cut-off parameters. Ideally, these statistical parameters should be derived from calibrated trip generation models, but in the absence of such data, Kwan (1998) argues that the use of customized functions and parameters based on theory is preferable to arbitrary assignment.

To promote a more "accessible" solution for accessibility analyses, this paper details a new Accessibility Toolbox for R and ArcGIS. The Python toolbox for ArcGIS simplifies the steps involved in a place-based accessibility workflow and comes coded with 5 impedance functions and 28 impedance measures for accessibility calculation. The interactive R Notebook version of this paper visualizes the function families and specifications and allows users to customize their parameters in accordance with theory and experience with their study area. These parameters can then be implemented in the ArcGIS tool’s Python code.

# METHODS
The accessibility toolbox implements the five different impedance functions from Kwan (1998):

$$
\begin{aligned}
  \text{Inverse Power: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} < 1 \\
          t_{ij}^{-\beta} & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Negative Exponential: } &f(t_{ij}) = e^{(-\beta t_{ij})} \\
  \text{Modified Gaussian: } &f(t_{ij})= e^{(-t_{ij}^2/\beta)} \\
  \text{Cumulative Opportunities Rectangular: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Cumulative Opportunities Linear: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          (1-t_{ij}/\bar{t}) & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.
\end{aligned}
$$

The inverse power, negative exponential, and modified Gaussian functions continuously discount the weight of opportunities as travel time increases using an impedance parameter $\beta$ that accounts for the cost of travel.

```{r travel time, echo=FALSE}
# first define the travel time increment, in this case from 0 to 60 minutes
t_ij <- data.frame(t_ij = seq(from = 0, to = 60, by=1))
```

With a foundation in early gravity models of spatial interaction (e.g. Stewart, 1948; Zipf, 1949), the inverse power function produces a rapid decline in the weight of opportunities as travel time increases. While power functions draw analogs to Newtonian physics, their theoretical relevance to human travel behavior has been questioned (Sen and Smith, 1995).

```{r inverse power function, echo=FALSE}
power_f <- function(t_ij,b0){case_when(t_ij < 1 ~ 1,
                                       TRUE ~ t_ij^-b0)}

POW <- list(POW0_8 = function(t_ij){power_f(t_ij, b0 = 0.8)},
            POW1_0 = function(t_ij){power_f(t_ij, b0 = 1.0)},
            POW1_5 = function(t_ij){power_f(t_ij, b0 = 1.5)},
            POW2_0 = function(t_ij){power_f(t_ij, b0 = 2.0)},
            POW_CUS = function(t_ij){power_f(t_ij, b0 = 0.5)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW0_8, aes(colour="POW0_8"), size=1) +
  stat_function(fun=POW$POW1_0, aes(colour="POW1_0"), size=1) +
  stat_function(fun=POW$POW1_5, aes(colour="POW1_5"), size=1) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=POW$POW_CUS, aes(colour="POW_CUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(POW)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Inverse Power Function")
```

The negative exponential function is more gradual and based on its strong theoretical foundations in entropy maximization (Wilson, 1971) and choice behavior theory (Fotheringham and O’Kelly, 1989), this function appears to have become somewhat of a de-facto standard in applied accessibility analysis.

```{r negative exponential function, echo=FALSE}
neg_exp_f = function(t_ij,b0){exp(-b0*t_ij)}

NEG_EXP <- list(EXP0_12 = function(t_ij){neg_exp_f(t_ij, b0 = 0.12)},
                EXP0_15 = function(t_ij){neg_exp_f(t_ij, b0 = 0.15)},
                EXP0_22 = function(t_ij){neg_exp_f(t_ij, b0 = 0.22)},
                EXP0_45 = function(t_ij){neg_exp_f(t_ij, b0 = 0.45)},
                EXP_CUS = function(t_ij){neg_exp_f(t_ij, b0 = 0.10)}, # custom - set your own parameter
                HN1997 = function(t_ij){neg_exp_f(t_ij, b0 = 0.1813)}) # from Handy and Niemeier (1997)

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=NEG_EXP$EXP0_12, aes(colour="EXP0_12"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_22, aes(colour="EXP0_22"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_45, aes(colour="EXP0_45"), size=1) +
  stat_function(fun=NEG_EXP$EXP_CUS, aes(colour="EXP_CUS"), size=1, linetype="dashed") +
  stat_function(fun=NEG_EXP$HN1997, aes(colour="HN1997"), size=1, linetype="longdash") +
  scale_color_discrete(limits = names(NEG_EXP)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Negative Exponential Function")
```

The modified Gaussian function exhibits a much more gradual rate of decline around its origin and a slower rate of decline overall. While Ingram (1971) argues these properties make the function superior to its inverse power and negative exponential counterparts for explaining observed travel behavior, it appears rarely used in the applied literature.

```{r modified gaussian function, echo=FALSE}
mgaus_f = function(t_ij,b0){exp(-t_ij^2/b0)}

MGAUS <- list(MGAUS10 = function(t_ij){mgaus_f(t_ij, b0 = 10)},
              MGAUS40 = function(t_ij){mgaus_f(t_ij, b0 = 40)},
              MGAUS100 = function(t_ij){mgaus_f(t_ij, b0 = 100)},
              MGAUS180 = function(t_ij){mgaus_f(t_ij, b0 = 180)},
              MGAUSCUS = function(t_ij){mgaus_f(t_ij, b0 = 360)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=MGAUS$MGAUS10, aes(colour="MGAUS10"), size=1) +
  stat_function(fun=MGAUS$MGAUS40, aes(colour="MGAUS40"), size=1) +
  stat_function(fun=MGAUS$MGAUS100, aes(colour="MGAUS100"), size=1) +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1) +
  stat_function(fun=MGAUS$MGAUSCUS, aes(colour="MGAUSCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(MGAUS)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Modified Gaussian Function")
```

The cumulative rectangular function is an isochronic measure that applies a constant weight to all opportunities reachable within some travel time window whose maximum is defined by $\bar{t}$. Although the application of a constant weight runs counter to the geographic principle of distance deterrence or decay that underpins travel behavior theory, such functions remain popular due to their ease of interpretation.

```{r cumulative rectangular function, echo=FALSE}
cumr_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1,
                                        TRUE ~ 0)}

CUMR <- list(CUMR10 = function(t_ij){cumr_f(t_ij, t_bar = 10)},
             CUMR20 = function(t_ij){cumr_f(t_ij, t_bar = 20)},
             CUMR30 = function(t_ij){cumr_f(t_ij, t_bar = 30)},
             CUMR40 = function(t_ij){cumr_f(t_ij, t_bar = 40)},
             CUMRCUS = function(t_ij){cumr_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUMR$CUMR10, aes(colour="CUMR10"), size=1) +
  stat_function(fun=CUMR$CUMR20, aes(colour="CUMR20"), size=1) +
  stat_function(fun=CUMR$CUMR30, aes(colour="CUMR30"), size=1) +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1) +
  stat_function(fun=CUMR$CUMRCUS, aes(colour="CUMRCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUMR)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Rectangular Function")
```

Finally, the cumulative linear function is a hybrid of the continuous and cumulative approaches, linearly discounting opportunities within an isochrone.

```{r cumulative linear function, echo=FALSE}
cuml_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1-t_ij/t_bar,
                                        TRUE ~ 0)}

CUML <- list(CUML10 = function(t_ij){cuml_f(t_ij, t_bar = 10)},
             CUML20 = function(t_ij){cuml_f(t_ij, t_bar = 20)},
             CUML30 = function(t_ij){cuml_f(t_ij, t_bar = 30)},
             CUML40 = function(t_ij){cuml_f(t_ij, t_bar = 40)},
             CUMLCUS = function(t_ij){cuml_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUML$CUML10, aes(colour="CUML10"), size=1) +
  stat_function(fun=CUML$CUML20, aes(colour="CUML20"), size=1) +
  stat_function(fun=CUML$CUML30, aes(colour="CUML30"), size=1) +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1) +
  stat_function(fun=CUML$CUMLCUS, aes(colour="CUMLCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUML)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Linear Function")
```

This set of impedance functions is by no means exhaustive. Numerous alternatives have been proposed, such as the exponential-normal, exponential-square root, and log-normal functions reviewed by Reggiani et al. (2011) and the Box-Cox, Tanner, and Richards functions reviewed by Martínez and Viegas (2013). Although these functions could be implemented in future iterations of the tool, the present paper’s focus on the functions specified in Kwan (1998) introduces some of the most widely used measures of impedance in applied accessibility analysis.

Kwan (1998) sets four impedance parameters for each continuous function designed to produce a weight of about 0.1 at travel times of 5, 10, 15, and 20 minutes respectively. Figure 1 recreates a figure from Kwan (1998) to visualize parameter values for the 5 functions: the inverse power function with $\beta = 2$ (POW2_0); the negative exponential function with $\beta = 0.15$ (EXP0_15); the modified Gaussian function with $\beta = 180$ (MGAUS180); and the cumulative rectangular (CUMR40) and linear (CUML40) functions with $\bar{t}$ set to 40 minutes.

```{r figure1, echo=FALSE, fig.cap="\\label{fig:figure1}Figure 1. Impedance Function Comparison"}
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1, linetype="dashed") +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1, linetype="dotdash") +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1, linetype="twodash") +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1, linetype="longdash") +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Figure 1. Impedance Function Comparison")
```

Calculating place-based accessibility using the `R5R` package below requires the creation of a network dataset using data from Open Street Map (OSM) and General Transit Feed Specification (GTFS) files; input origins and destinations (point or polygon); a numerical attribute representing destination opportunities, and the selection of one or more of the 28 impedance functions implemented in the tool to weight the attractiveness of the opportunities. In addition to Kwan’s (1998) impedance specifications, the toolbox also implements Handy and Niemeier's (1997) negative exponential specification calibrated to walking trips for convenience shopping in Oakland, CA in 1980 and several additional popular cumulative rectangular measures.

# DATA PROCESSING: MBTA
## Download and Prepare Census Data

```{r load RData, echo = FALSE}
# you can run the code below to download the data (remember your api keys!)
# or load it pre-processed from here:
mbta_cb_poly <- readRDS(file.path(data_path, "mbta_cb_poly.rds"))
```

The first step is to download the census data that will be used as indicators of opportunities, including census population and employment data at the block level using the `tigris` and `lodes` packages respectively.

```{r download census employment data, include = FALSE, eval = FALSE}
# include all MA counties with year-round MBTA service
mbta_counties = c("005", "009", "017", "021", "023", "025", "027")

# download longitudinal origin-destination employment survey data
ma_lodes <- grab_lodes(state = "ma", 
                       year = 2020, 
                       lodes_type = "wac", 
                       job_type = "JT00",
                       segment = "S000", 
                       state_part = "main", 
                       agg_geo = "block") %>%
  rename(Block_GEOID = w_geocode, total_emp = C000) %>% 
  mutate(Block_GEOID = as.character(Block_GEOID)) %>%
  select(Block_GEOID, total_emp)
```

Next, download census block geographic boundaries from New York City's Open Data catalogue, prepare standardized GEOIDs for joining, and join the population and employment data:

```{r download and prepare census boundaries, include = FALSE, eval = FALSE}
# download census block boundaries, including their populations
mbta_blkshp <- blocks("MA", mbta_counties, year=2020) %>% 
  st_transform(crs = 26986) %>% # project to NAD 1983 Massachusetts mainland
  mutate(Block_GEOID = GEOID20, total_pop = POP20, geometry=geometry, .keep='none')

# generate census blocks poly
mbta_cb_poly <- mbta_blkshp %>% 
  
  # join employment
  left_join(ma_lodes, by = "Block_GEOID") %>%
  
  # zero-out any NAs
  mutate(total_pop = ifelse(is.na(total_pop), 0, total_pop),
         total_emp = ifelse(is.na(total_emp), 0, total_emp))

# save census blocks poly
mbta_cb_poly %>% saveRDS(path(data_path, "mbta_cb_poly.rds"))
```

## Download Travel Network Data

With the census data prepared, the second step is to download the travel network data. This consists of an extract of Open Street Map (OSM) data from [geofabrik](https://download.geofabrik.de/):

```{r download osm, include=FALSE, eval = FALSE}
# Download OpenStreetMap data for the state of Massachusetts from Geofabrik
download.file(url = "https://download.geofabrik.de/north-america/us/massachusetts-latest.osm.pbf",
              destfile = file.path(r5_path, "osm.pbf"), mode = "wb")
```

And extract General Transit Feed Specification (GTFS) files from [Transitland](https://www.transit.land/documentation/rest-api):

```{r download gtfs, include = FALSE, eval = FALSE}
# transitland also has a feeds api that allows bounding box searches for feeds, but
# at first glance, it will also include a lot of stuff we care less about, like peter pan buses

mbta_req <- request("https://transit.land/api/v2/rest/feeds/f-drt-mbta/download_latest_feed_version") %>%
  req_headers(apikey = transitland_key)
mbta_resp <- req_perform(mbta_req)
mbta_resp %>%
  resp_body_raw() %>%
  brio::write_file_raw(path=file.path(r5_path, "mbta_recent.zip"))

# TODO: pending transitland correspondence, maybe replace them with Mobility Database
```

Or extract General Transit Feed Specification (GTFS) files from [Mobility Database](https://mobilitydatabase.org/feeds):

We can download other GTFS feeds to the same location to analyze additional services.

```{r extra gtfs packages, include = FALSE, eval = FALSE}
# for bus and commuter rail you can add these by un-commenting the code
#download.file(url = "http://web.mta.info/developers/data/lirr/google_transit.zip", 
#              destfile = file.path(r5_path, "lirr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/mnr/google_transit.zip", 
#              destfile = file.path(r5_path, "mnr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_bronx.zip", 
#              destfile = file.path(r5_path, "nyct_bus_bronx.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_brooklyn.zip", 
#              destfile = file.path(r5_path, "nyct_bus_brooklyn.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_manhattan.zip", 
#              destfile = file.path(r5_path, "nyct_bus_manhattan.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_queens.zip", 
#              destfile = file.path(r5_path, "nyct_bus_queens.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_staten_island.zip", 
#              destfile = file.path(r5_path, "nyct_bus_staten.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/busco/google_transit.zip", 
#              destfile = file.path(r5_path, "nyc_busco.zip"), mode = "wb")
```

```{r mbta gtfs}
mbta_feed_index <- read.csv('https://cdn.mbta.com/archive/archived_feeds.txt')

date_start <- "2024-10-29"
date_end <- "2025-02-01"

# get list of dates for selected days of week before and after 12/15/2025
dtrange <- seq(as.Date(date_start), as.Date(date_end), by="days")
tuesdays <- dtrange[weekdays(dtrange) == 'Tuesday'] %>% format('%Y%m%d') %>% as.integer()
saturdays <- dtrange[weekdays(dtrange) == 'Saturday'] %>% format('%Y%m%d') %>% as.integer()

# only downloads the feed for a given date if that feed is not already present in data_path
lookup_mbta_feed_from_date <- function(yyyymmdd) {
  feed_url <- mbta_feed_index[mbta_feed_index$feed_start_date <= yyyymmdd 
                         & mbta_feed_index$feed_end_date >= yyyymmdd,'archive_url']
  fname <- sub(".*/", "", feed_url)
  
  if (!file.exists(file.path(data_path, fname))) {
    download.file(url = feed_url,
              destfile = file.path(data_path, fname), mode = "wb")
  }
  lookup <- list()
  lookup[[as.character(yyyymmdd)]] <- fname
  return(lookup)
}

feed_list <- function(dates) {
  feed_lookup <- list()
  
  for (dt in dates) {
    feed_lookup <- c(feed_lookup, lookup_mbta_feed_from_date(dt))
  }
  
  return(feed_lookup)
}

mbta_tues_lookup <- feed_list(tuesdays)
mbta_sat_lookup <- feed_list(saturdays)

# To actually run them in r5 though, each feed has to be moved one at a time into r5_path and the r5 core set up with overwrite=TRUE. The travel time matrix outputs should also now be saved in a series of folders per agency, date, and time
```

We can query the characteristics of the subway GTFS data using the `summary.gtfs()` function from the `tidytransit` package:

```{r}
run_duckdb <- function(query) {
  con <- dbConnect(duckdb()) # set up duckdb connection
  
  # load httpfs extension to query URL sources
  dbExecute(con, "INSTALL httpfs;")
  dbExecute(con, "LOAD httpfs;")
  
  # Execute the query and fetch results
  result <- dbGetQuery(con, query)
  
  # Disconnect from DuckDB
  dbDisconnect(con, shutdown = TRUE)
  
  return(result)
}
```

```{r}
mbta_gtfs_metrics <- function(date_start, date_end, year) {
  gtfs_metrics_query <- paste0("
      -- First, assemble the list of all days between the specified start and end dates, using YYYYMMDD format
      with service_days as (
        select 
          strftime(generate_series, '%Y%m%d')::INTEGER as service_day
          , date_part('weekday', generate_series) as dow
        from generate_series(timestamp '", date_start, "', timestamp '", date_end, "', interval '1 day')
      ),
      
      -- Then, get all the IDs of the scheduled on each day, making sure to
      -- incorporate both typical and 'exception' service (e.g. holidays, diversions)
      services_scheduled as (
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar.parquet') AS c
          ON end_date >= service_day AND start_date <= service_day
          AND c.gtfs_end_date >= service_day AND c.gtfs_active_date <= service_day
          AND CASE
            WHEN dow = 0 and sunday = 1 then 1
            WHEN dow = 1 and monday = 1 then 1
            WHEN dow = 2 and tuesday = 1 then 1
            WHEN dow = 3 and wednesday = 1 then 1
            WHEN dow = 4 and thursday = 1 then 1
            WHEN dow = 5 and friday = 1 then 1
            WHEN dow = 6 and saturday = 1 then 1
            ELSE 0 END = 1
            
        -- exception_type = 2 are days when a given service was removed from the schedule
        ANTI JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON c.service_id = cd.service_id AND cd.exception_type = 2 and cd.date = service_day
          
        -- exception_type = 1 represents days when a given service was added to the schedule
        UNION 
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON cd.exception_type = 1 and cd.date = service_day
      ),
      
      -- get trip durations for all scheduled services on each day
      trip_info AS (
        SELECT 
              service_day
              , st.trip_id
              , t.route_id
              , r.route_desc
              , min(arrival_time) as trip_start
              , max(arrival_time) as trip_end
              , 60*datepart('hour', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL)
                + datepart('minute', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL) as trip_duration_minutes
        FROM services_scheduled ss
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_attributes.parquet') AS ca
          ON ss.service_id = ca.service_id AND ca.gtfs_end_date >= service_day AND ca.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/trips.parquet') AS t
          ON ss.service_id = t.service_id AND t.gtfs_end_date >= service_day AND t.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/stop_times.parquet') AS st
          ON st.trip_id = t.trip_id AND st.gtfs_end_date >= service_day AND st.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/routes.parquet') AS r
          ON t.route_id = r.route_id AND r.gtfs_end_date >= service_day AND r.gtfs_active_date <= service_day
        GROUP BY 1,2,3,4
      ),
      
      -- deduplicate trip IDs with same start and end times on same routes and days
      trips_unique AS (
        SELECT DISTINCT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
        FROM trip_info
      )
  
      SELECT
        service_day
        , route_id
        , route_desc
        , count(*) as num_trips
        , sum(trip_duration_minutes) as total_runtime_minutes
        , min(trip_start) as first_trip_start
        , max(trip_end) as last_trip_end
      FROM trips_unique
      GROUP BY 1,2,3;
    ")
  
  return(run_duckdb(gtfs_metrics_query))
}
```

```{r}
mbta_gtfs_metrics_between <- function(date_start, date_end) {
  yyyymmdd_st <- as.integer(strftime(strptime(date_start, '%Y-%m-%d'), '%Y%m%d'))
  yyyymmdd_end <- as.integer(strftime(strptime(date_end, '%Y-%m-%d'), '%Y%m%d'))
  yyyy_st <- as.integer(yyyymmdd_st/10000)
  yyyy_end <- as.integer(yyyymmdd_end/10000)
  
  df <- data.frame()
  
  for (yyyy in yyyy_st:yyyy_end) {
    dt_st <- strftime(strptime(max(yyyy*10000+101, yyyymmdd_st), '%Y%m%d'), '%Y-%m-%d')
    dt_end <- strftime(strptime(min(yyyy*10000+1231, yyyymmdd_end), '%Y%m%d'), '%Y-%m-%d')
    
    df <- bind_rows(df, mbta_gtfs_metrics(dt_st, dt_end, yyyy))
  }

  return(df)
}
```

```{r}
gtfs_metrics <- mbta_gtfs_metrics_between(date_start, date_end)
```

```{r}
gtfs_metrics %>%
  filter(service_day %in% tuesdays, route_id == '86') %>%
  arrange(service_day) %>%
  head(15)
```

```{r}
year <- 2024
test_query <- paste0("
    -- First, assemble the list of all days between the specified start and end dates, using YYYYMMDD format
    with service_days as (
      select 
        strftime(generate_series, '%Y%m%d')::INTEGER as service_day
        , date_part('weekday', generate_series) as dow
      from generate_series(timestamp '", date_start, "', timestamp '", date_end, "', interval '1 day')
    ),
    
    -- Then, get all the IDs of the scheduled on each day, making sure to
    -- incorporate both typical and 'exception' service (e.g. holidays, diversions)
    services_scheduled as (
      SELECT 
            service_day
            , service_id
      FROM service_days
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar.parquet') AS c
        ON end_date >= service_day AND start_date <= service_day
        AND c.gtfs_end_date >= service_day AND c.gtfs_active_date <= service_day
        AND CASE
          WHEN dow = 0 and sunday = 1 then 1
          WHEN dow = 1 and monday = 1 then 1
          WHEN dow = 2 and tuesday = 1 then 1
          WHEN dow = 3 and wednesday = 1 then 1
          WHEN dow = 4 and thursday = 1 then 1
          WHEN dow = 5 and friday = 1 then 1
          WHEN dow = 6 and saturday = 1 then 1
          ELSE 0 END = 1
          
      -- exception_type = 2 are days when a given service was removed from the schedule
      ANTI JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
        ON c.service_id = cd.service_id AND cd.exception_type = 2 and cd.date = service_day
        
      -- exception_type = 1 represents days when a given service was added to the schedule
      UNION 
      SELECT 
            service_day
            , service_id
      FROM service_days
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
        ON cd.exception_type = 1 and cd.date = service_day
    )
    

      SELECT 
            service_day
            , ca.service_id
            , ca.service_schedule_typicality
            , st.trip_id
            , t.route_id
            , r.route_desc
            , min(arrival_time) as trip_start
            , max(arrival_time) as trip_end
            , 60*datepart('hour', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL)
              + datepart('minute', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL) as trip_duration_minutes
      FROM services_scheduled ss
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_attributes.parquet') AS ca
        ON ss.service_id = ca.service_id AND ca.gtfs_end_date >= service_day AND ca.gtfs_active_date <= service_day
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/trips.parquet') AS t
        ON ss.service_id = t.service_id AND t.gtfs_end_date >= service_day AND t.gtfs_active_date <= service_day
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/stop_times.parquet') AS st
        ON st.trip_id = t.trip_id AND st.gtfs_end_date >= service_day AND st.gtfs_active_date <= service_day
      JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/routes.parquet') AS r
        ON t.route_id = r.route_id AND r.gtfs_end_date >= service_day AND r.gtfs_active_date <= service_day
      WHERE r.route_id = '66'
      GROUP BY 1,2,3,4,5,6;
  ")
  
t <- run_duckdb(test_query)
```

This is useful for ascertaining the service dates contained within the GTFS calendar. Take note of the calendar range, as you will have to specify a date and time that falls within this interval when running the origin-destination matrix calculation below.

## Define Accessibility Parameters

Using this set of variable definitions, users can set the key parameters that define how the origin-destination matrix and accessibilities are calculated. This includes supplying the input origins and destinations (as `sf` objects), defining relevant ID and destination opportunities fields, and selecting one or more impedance functions, travel modes, a date and time of departure (within the calendar dates of the supplied GTFS files), and maximum walking distance and trip duration.

In addition, a `chunk_size` parameter is used to control batching as the base `travel_time_matrix` function from `R5R` is employed here within a batching work flow built around `disk.frame`. In this formulation, the matrix is calculated for batches of origins to all destinations reachable within the travel time window for the selected modes. While `r5r`` works fastest with a single set of origins, this batching is implemented to save memory and enable the calculation of very large matrices. In addition, the resulting matrix is compressed and saved to disk for future use. Depending on your particular hardware configuration, you can change the number of origins considered in each batch.

```{r define parameters, warning = FALSE}
# 1. Generalize calls to input simple features
# origins
origins_i <-  mbta_cb_poly
origins_id_field <- "Block_GEOID"

# destinations
destinations_j <-  mbta_cb_poly
destinations_id_field <- "Block_GEOID"
opportunities_j_field <- "total_emp"

# select your impedance functions
selected_f <- c("POW2_0", "EXP0_15", "MGAUS180", "CUML40", "CUMR40") # pick one or more impedance functions

# 2. R5 Travel Time Matrix Options. See https://rdrr.io/cran/r5r/man/travel_time_matrix.html for more detail
# R5 allows for multiple combinations of transport modes. The options include:
## Transit modes
# TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR. 
# The option 'TRANSIT' automatically considers all public transport modes available.

## Non transit modes
# WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK

# define your travel modes
mode <- c("WALK", "TRANSIT")

# egress mode
#mode_egress = "WALK" # Transport mode used after egress from public transport; it can be either 'WALK', 'BICYCLE', or 'CAR'. Defaults to "WALK"

# walk speed
#walk_speed = 3.6 #Average walk speed in km/h. Defaults to 3.6 km/h

# bike speed
#bike_speed = 12 # Average cycling speed in km/h. Defaults to 12 km/h
  
# max rides
#max_rides = 3 # The max number of public transport rides allowed in the same trip. Defaults to 3

# level of traffic stress (cycling)
#max_lts = 2 # The maximum level of traffic stress that cyclists will tolerate. A value of 1 means cyclists will only travel through the quietest streets, while a value of 4 indicates cyclists can travel through any road; defaults to 2

# define trip start datetime
departure_datetime <- as.POSIXct("2025-01-29 08:00:00",  # must be within calendar range of gtfs 
                                 format = "%Y-%m-%d %H:%M:%S", 
                                 tz = "America/New_York") # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for list of time zone codes

# max trip duration (minutes)
max_trip_duration = 45

# processing batch size - how many origin rows to process at one time
chunksize = 500 # how many origins to consider at one time; set to nrow(origins_sf) if you don't want to use

# multithreading
n_threads = Inf # The number of threads to use in parallel computing; defaults to use all available threads (Inf)
```

# Calculate Accessibility

With the input data collected and parameters set, the next step is to calculate accessibility. This occurs over three steps: first, building the network for routing with R5; second, preparing the input data into a format that R5 expects; third, calculating the origin-destination matrix; and fourth, calculating and summarizing accessibility for the selected impedance functions. Everything is generalized/automated after this point - just click run!

## Set Up R5 Routing

First, the network graph will be created.

```{r build graph, include = FALSE}
r5r_core <- setup_r5(data_path = r5_path, verbose = FALSE)
```

## Prepare Input Data for Analysis

Second, the input data are converted to centroids and transformed to the WGS 1984 geographic coordinate system. To save computational time, only destinations with opportunities are considered. 

```{r prepare input data for r5r, warning = FALSE, include = FALSE}
# assemble all impedance functions into a list
impedance_f <- c(POW, NEG_EXP, MGAUS, CUMR, CUML)

# ORIGINS
# create input sf object and change to crs 4326 for origins
origins_i_sf <- origins_i %>% 
  select(all_of(origins_id_field)) %>% 
  rename(id = all_of(origins_id_field))

origins_i <- origins_i_sf %>% 
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) %>% # transform to lat-longs
  mutate(batch_id = ceiling(row_number()/chunksize))

# DESTINATIONS
# create input sf object and change to crs 4326 for destinations
destinations_j <- destinations_j %>% 
  select(all_of(c(destinations_id_field, opportunities_j_field))) %>% 
  rename(id = all_of(destinations_id_field), o_j = all_of(opportunities_j_field)) %>%
  filter(o_j > 0) %>%
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) # transform to lat-longs

opportunities_j <- destinations_j %>%
  st_drop_geometry() %>% 
  select(id, o_j) %>% 
  rename(toId = id)
```

## Calculate Origin-Destination Matrix

Third, the origin-destination matrix is calculated in accordance with the travel and batching parameters set earlier. In this case, I have set the ODCM to save to disk as a parquet dataset, which is a great format for compressed storage and for larger-than-memory calculations. Because the travel time matrix is stored on disk already, I have set this chunk to `eval = FALSE`.

TODO: use time_window and percentiles parameters as described here: https://ipeagit.github.io/r5r/articles/time_window.html 

```{r calculate od matrix, echo = FALSE, eval = FALSE}
# set up batching 
num_chunks = origins_i %>% st_drop_geometry() %>% summarize(num_chunks = max(batch_id)) %>% pull(num_chunks)

# compute travel time matrix
start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)

for (i in 1:num_chunks){
  origins_i_chunk <- origins_i %>% filter(batch_id == i)
  
  ttm_chunk <- travel_time_matrix(
    r5r_core = r5r_core,
    origins = origins_i_chunk,
    destinations = destinations_j,
    mode = mode,
    departure_datetime = departure_datetime,
    max_trip_duration = max_trip_duration,
    verbose = FALSE,
    progress = FALSE) %>%
    
  mutate(batch_id = i)
  
  # export output as parquet
  write_dataset(ttm_chunk, path = ttm_path, partitioning = "batch_id")
  
  setTxtProgressBar(pb, i)}

end.time <- Sys.time()
print(paste0("OD matrix calculation took ", round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."))
```

## Calculate and Summarize Accessibility

Finally, accessibilities are calculated using the selected impedance function(s) by iterating over all the batches of the origin-destination matrix. There's a few ways of doing this.

TODO: use accessibility package to aggregate opportunities accessible from a ttm as shown here: https://cran.r-project.org/web/packages/r5r/vignettes/accessibility.html

### Arrow

The first way to do this is straightforward using `arrow`: calculate over the whole dataset using a single function, one-by-one. This makes use of the larger-than-memory processing capabilities of `arrow`. However, core math functions like `exp()` are not yet implemented for dataset calculations (they should be in version 9). So that does not work at the moment. `arrow` does also not appear to play well with defined functions, so one has to put them all in manually, like so:

```{r calculate accessibility using arrow, echo = FALSE}
ttm <- open_dataset(path(ttm_path)) %>%
  # insert accessibility functions here?
  collect()
```

terminate r5r instance to clear up memory

```{r}
r5r::stop_r5(r5r_core)
rJava::.jgc(R.gc = TRUE)
```

This works well with one or a few selected functions, but does not scale well to using many functions.

### Mapping Batches with Arrow

The second option is to use the `map_batches` functionality in `arrow` to map a function to the individual dataset batches. This is because `dplyr` verbs such as `across` are not yet implemented in `arrow`, so the batch must first be brought into memory. However, in my testing with large travel time matrices, `map_batches` as used below can result in very high memory usage. Running this code on the saved `ttm` dataset uses many gigabytes of memory on my PC, so I believe this solution is **not stable**. As they note on the [website](https://arrow.apache.org/docs/r/articles/dataset.html): `map_batches` is experimental and not recommended for production use. I will keep this code for now to revisit in the future as the `arrow` package matures.

```{r calculate accessibility with map_batches, echo = FALSE, eval = FALSE}
#ttm <- open_dataset(path(ttm_path))

#accessibility <- ttm %>%
  
#  map_batches(function(batch) {
#    batch %>%
#      as.data.frame() %>%
      
      # join opportunities
#      left_join(opportunities_j, by = "toId") %>%
      
      # mutate across the selected functions
#      mutate(across(travel_time, .fns= impedance_f[selected_f], .names = "{fn}")) %>%
   
      # multiply weights by opportunities
#      mutate(across(all_of(selected_f), .fns = function(f) f*.$o_j)) %>%
      
      # summarize results
#      group_by(fromId) %>% 
#      summarize(across(all_of(selected_f), sum))
#  })
```  

### Mapping Batches Manually

A third way that does scale well for calculating results for all functions is to map batches manually by getting a list of all the `.parquet` files in the dataset and iterate over them. This works, but is not as clean as a pure `arrow` solution. I include it below for completeness.

```{r calculate accessibility with purrr, echo = FALSE, eval = FALSE}
pq_files <- dir_ls(path(ttm_path), glob = "*.parquet", recurse = TRUE)

accessibility <- purrr::map_dfr(pq_files, function(batch){
  access <- read_parquet(batch) %>% 
    collect() %>%
    
    # join opportunities
    left_join(opportunities_j, by = "toId") %>%
    
    # mutate across the selected functions
    mutate(across(travel_time, .fns= impedance_f[selected_f], .names = "{fn}")) %>%
 
    # multiply weights by opportunities
    mutate(across(all_of(selected_f), .fns = function(f) f*.$o_j)) %>%
    
    # summarize results
    group_by(fromId) %>% 
    summarize(across(all_of(selected_f), sum))
})
```  

### Accessibility through R5R

As of a recent update, the R5R package itself can calculate accessibility natively. Compared to the manual workflow of calculating the travel time matrix followed by accessibility, this function does it all in one step. The impedance functions and their specifications do not align with the focus of this paper though.

```{r}
?r5r::accessibility
```

### Accessibility through the Upcoming Accessibility Package

The core aspects of place-based accessibility analysis have become significantly more "accessible" since I started working on this toolbox in 2019. The team behind `R5R` at the Institute for Applied Economic Research (Ipea) in Brazil is working on a new `accessibility` [package](https://github.com/ipeaGIT/accessibility) for R that will offer these functions and more within an easy-to-use R package. For most R users, this package will likely supersede the more manual approach adopted by this tutorial when it becomes available. Nevertheless, the workflows in this document are likely to continue to have a place for large analyses.

# FINDINGS

In the sample analysis, the `travel_time_matrix()` function from the `r5r` package to calculate accessibility to employment reachable within 45 minutes from each of New York City’s 38,800 census blocks using walking and transit and a selection of 5 impedance measures from Kwan (1998). Mapped results are shown in Figure 2. The use of multiprocessing by R5 offers fast calculation of the OD matrix while the implementation of a batching workflow using `arrow` limits memory use for very large analyses and stores results on disk. 

```{r figure2, fig.height=3.5, fig.width=12.5, fig.cap="\\label{fig:figure2}Figure 2. Transit Employment Accessibility Comparison", message=FALSE, warning=FALSE, echo=FALSE}
# join results to input_sf object
input_sf_accessibility <- left_join(origins_i_sf, accessibility, by=c("id" = "fromId"))

# iterate through selected functions to create maps
accessibility_maps <- list()

for (i in selected_f){
  map <- tm_shape(input_sf_accessibility) +
    tm_fill(col = i, 
            title = i, 
            style = "cont", 
            palette = "viridis") + 
    tm_layout(frame = FALSE,
              bg.color = "grey85",
              legend.position = c("left", "top"),
              legend.text.color = "white",
              legend.title.color = "white")
  
  accessibility_maps[[i]] <- map}

names(accessibility_maps) <- selected_f

# plot the maps
tmap_arrange(accessibility_maps)
```

Similar to Kwan (1998) and Vale and Pereira (2017), correlations in accessibility across measures are generally strong (Figure 3), indicating many capture similar spatial processes. Of those used in Figures 1 and 2 for example, results from the negative exponential (`EXP0_15`), modified Gaussian (`MGAUS180`), cumulative rectangular (`CUMR40`), and cumulative linear (`CUML40`) measures of impedance all show correlation coefficients of at least 0.75. In particular, the correlation between the `EXP0_15` and `MGAUS180` measures is 0.99. Results from the inverse power (`POW2_0`) measure are more unique, with correlations ranging from 0.72 to 0.85. It should be emphasized that such outcomes are not a product of similar functional forms alone; rather, the correlations reflect an interaction between the different impedance measures and the spatial distribution of opportunities on the travel network in the study area. Furthermore, absolute accessibility totals differ across each, suggesting the choice of a suitable impedance function and specification remains an important issue that should be guided by theory and assumptions about travel behavior.

```{r figure3, fig.height=10, fig.width=10, fig.cap="\\label{fig:figure3}Figure 3. Impedance Measure Correlations", warning = FALSE, echo=FALSE}
# create dataframe of variables for correlation analysis
correlations <- input_sf_accessibility %>% select(all_of(selected_f)) %>% st_drop_geometry() %>% drop_na() %>% cor(.)

corrplot(correlations,
         method = "color", # corrplot method
         type = "upper", # upper triangle matrix
         addCoef.col = "white", # correlation text colour
         number.cex = 0.8, # correlation text size
         tl.col = "black", # text label colour
         tl.srt = 45, # text label angle
         tl.cex = 0.9, # text label size
         cl.lim = c(min(correlations), 1), # colour label limits
         #diag = FALSE, # turn off diagonal
         is.corr=FALSE, # because all coefficients are positive
         col = viridis::viridis(100)) # custom colour scheme based on viridis hex values
```

While the focus on walking and transit trips in this sample analysis does not provide a full picture of travel behavior in the study area, the `r5r` package enables users to run multiple analyses for different travel modes. Moreover, the R notebook can be utilized to select or customize the implemented impedance measures in accordance with expectations about travel behavior for each mode. Taken together, this toolbox enables researchers and practitioners to make better decisions about the specification and customization of travel impedance and simplify the calculation of place-based accessibility for their study context.

# REFERENCES

Fotheringham, A. S., & O’Kelly, M. E. (1989). *Spatial interaction models: Formulations and applications*. Boston: Kluwer Academic.

Geurs, K. T., & Van Wee, B. (2004). Accessibility evaluation of land-use and transport strategies: review and research directions. *Journal of Transport Geography*, 12(2), 127-140. https://doi.org/10.1016/j.jtrangeo.2003.10.005

Handy, S. L., & Niemeier, D. A. (1997). Measuring accessibility: An exploration of issues and alternatives. *Environment and Planning A*, 29(7), 1175-1194. https://doi.org/10.1068%2Fa291175

Hansen, W. G. (1959). How accessibility shapes land use. *Journal of the American Institute of Planners*, 25(2), 73-76. https://doi.org/10.1080/01944365908978307

Ingram, D. R. (1971). The concept of accessibility: A search for an operational form. *Regional Studies*, 5(2), 101-107. https://doi.org/10.1080/09595237100185131

Kwan, M. P. (1998). Space‐time and integral measures of individual accessibility: A comparative analysis using a point‐based framework. *Geographical Analysis*, 30(3), 191-216. https://doi.org/10.1111/j.1538-4632.1998.tb00396.x

Martínez, L. M., & Viegas, J. M. (2013). A new approach to modelling distance-decay functions for accessibility assessment in transport studies. *Journal of Transport Geography*, 26, 87-96. https://doi.org/10.1016/j.jtrangeo.2012.08.018

Páez, A., Scott, D. M., & Morency, C. (2012). Measuring accessibility: Positive and normative implementations of various accessibility indicators. *Journal of Transport Geography*, 25, 141-153. https://doi.org/10.1016/j.jtrangeo.2012.03.016

Reggiani, A., Bucci, P., & Russo, G. (2011). Accessibility and impedance forms: empirical applications to the German commuting network. *International Regional Science Review*, 34(2), 230-252. https://doi.org/10.1177/0160017610387296

Sen, A., & Smith, T. E. (1995). *Gravity models of spatial interaction behavior*. Berlin: Springer-Verlag.

Stewart, J. Q. (1948). Demographic gravitation: evidence and applications. *Sociometry*, 11(1/2), 31-58. https://doi.org/10.2307/2785468

Vale, D. S., & Pereira, M. (2017). The influence of the impedance function on gravity-based pedestrian accessibility measures: A comparative analysis. *Environment and Planning B: Urban Analytics and City Science*, 44(4), 740-763. https://doi.org/10.1177%2F0265813516641685

Wilson, A. G. (1971). A family of spatial interaction models, and associated developments. *Environment and Planning A*, 3(1), 1-32. https://doi.org/10.1068/a030001

Zipf, G. K. (1949). *Human behavior and the principle of least effort*. Cambridge: Addison-Wesley.
