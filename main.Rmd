---
title: "Measuring the Destination Access Impacts of Public Transit Service Adjustments"
author:
  - name: "Ethan McIntosh"
    affiliation: Northeastern University
date: '2025-04-23'
---

Note: some elements of this script were adapted from the Accessibility Calculator for R developed by Christopher Higgins at https://github.com/higgicd/Accessibility_Toolbox/

```{r setup, include=FALSE}
# the r5r package requires installation of Java Development Kit version 21
# Installation instructions available at https://github.com/ipeaGIT/r5r

# assign memory for r5r to work with
options(java.parameters = "-Xmx6G") 

# load other packages
library(lehdr) # for pulling US Census LODES data
library(tigris) # for pulling US Census shapefiles
library(tidyverse) # for tidy functions and syntax
library(fs) # for file path operations
library(r5r) # for calculating travel time matrices over transportation networks
library(sf) # for geospatial data processing
library(tmap) # for mapping
library(viridis)
library(progress) # for progress bars
library(arrow) # for batched read + write + transform of parquet files on disk
library(duckdb) # for querying MBTA LAMP
library(accessibility) # for calculating destination access measures
library(scales) # for chart axis formatting
library(Hmisc) # for wtd.quantile function

# setup options
tmap_mode("plot")

# create data directories (does not overwrite if these folders are already present)
data_path <- dir_create("./data") # for storing input data
outputs_path <- dir_create("./outputs") # for storing output data
ttm_path <- dir_create("./r5_ttm") # for storing od matrix
r5_path <- dir_create("./r5_graph") # for the r5 network graph
```

# DATA PROCESSING: MBTA
## Download and Prepare Census Data

The first step is to download the census data that will be used as indicators of opportunities, including census population and employment data at the block level using the `tigris` and `lodes` packages.

```{r download census employment data, include = FALSE, eval = FALSE}
# include all MA counties with year-round MBTA service
mbta_counties = c("005", "009", "017", "021", "023", "025", "027")

# download longitudinal origin-destination employment survey data
ma_lodes <- grab_lodes(state = "ma", 
                       year = 2020, 
                       lodes_type = "wac", 
                       job_type = "JT00",
                       segment = "S000", 
                       state_part = "main", 
                       agg_geo = "block") %>%
  rename(Block_GEOID = w_geocode, total_emp = C000) %>% 
  mutate(Block_GEOID = as.character(Block_GEOID)) %>%
  select(Block_GEOID, total_emp)
```

Next, download census block geographic boundaries and population data from the US Census, prepare standardized GEOIDs for joining, and join the population and employment data:

```{r download and prepare census boundaries, include = FALSE, eval = FALSE}
# download census block boundaries, including their populations
mbta_blkshp <- blocks("MA", mbta_counties, year=2020) %>% 
  st_transform(crs = 26986) %>% # project to NAD 1983 Massachusetts mainland
  mutate(Block_GEOID = GEOID20, total_pop = POP20, geometry=geometry, .keep='none')

# generate census blocks poly
mbta_cb_poly <- mbta_blkshp %>% 
  
  # join employment
  left_join(ma_lodes, by = "Block_GEOID") %>%
  
  # zero-out any NAs
  mutate(total_pop = ifelse(is.na(total_pop), 0, total_pop),
         total_emp = ifelse(is.na(total_emp), 0, total_emp))

# save census blocks poly
mbta_cb_poly %>% saveRDS(path(data_path, "mbta_cb_poly.rds"))
```

## Download Travel Network Data

With the census data prepared, the second step is to download the travel network data. This consists of an extract of Open Street Map (OSM) data from [geofabrik](https://download.geofabrik.de/):

```{r download osm, include=FALSE, eval = FALSE}
# Download OpenStreetMap data for the state of Massachusetts from Geofabrik
download.file(url = "https://download.geofabrik.de/north-america/us/massachusetts-latest.osm.pbf",
              destfile = file.path(r5_path, "osm.pbf"), mode = "wb")
```

## Pull MBTA GTFS metrics from LAMP

To decide which dates we want to pull to represent each service change, we first calculate Quantity of Service (QoS) metrics at the route-day level from the MBTA's compressed GTFS archive, mainly so we can identify and exclude dates whose schedules have a lot of temporary shuttling (Rail Replacement Bus service, in MBTA terminology). However, the QoS metrics will be useful context for the main analysis too.

```{r}
run_duckdb <- function(query) {
  con <- dbConnect(duckdb()) # set up duckdb connection
  
  # load httpfs extension to query URL sources - needed to query LAMP
  dbExecute(con, "INSTALL httpfs;")
  dbExecute(con, "LOAD httpfs;")
  
  # load spatial extension to use geographic functions - needed to calculate runtime distances
  dbExecute(con, "INSTALL spatial;")
  dbExecute(con, "LOAD spatial;")
  
  # Execute the query and fetch results
  result <- dbGetQuery(con, query)
  
  # Disconnect from DuckDB
  dbDisconnect(con, shutdown = TRUE)
  
  return(result)
}
```

SQL query to compute daily trip count, total runtime, and total runtime distance at the route-day level from the MBTA's compressed GTFS archive on LAMP

```{r}
mbta_gtfs_metrics <- function(date_start, date_end, year) {
  gtfs_metrics_query <- paste0("
      -- First, assemble the list of all days between the specified start and end dates, using YYYYMMDD format
      with service_days as (
        select 
          strftime(generate_series, '%Y%m%d')::INTEGER as service_day
          , date_part('weekday', generate_series) as dow
        from generate_series(timestamp '", date_start, "', timestamp '", date_end, "', interval '1 day')
      ),
      
      -- Then, get all the IDs of the scheduled on each day, making sure to
      -- incorporate both typical and 'exception' service (e.g. holidays, diversions)
      services_scheduled as (
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar.parquet') AS c
          ON end_date >= service_day AND start_date <= service_day
          AND c.gtfs_end_date >= service_day AND c.gtfs_active_date <= service_day
          AND CASE
            WHEN dow = 0 and sunday = 1 then 1
            WHEN dow = 1 and monday = 1 then 1
            WHEN dow = 2 and tuesday = 1 then 1
            WHEN dow = 3 and wednesday = 1 then 1
            WHEN dow = 4 and thursday = 1 then 1
            WHEN dow = 5 and friday = 1 then 1
            WHEN dow = 6 and saturday = 1 then 1
            ELSE 0 END = 1
            
        -- exception_type = 2 are days when a given service was removed from the schedule
        ANTI JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON c.service_id = cd.service_id AND cd.exception_type = 2 and cd.date = service_day
          
        -- exception_type = 1 represents days when a given service was added to the schedule
        UNION 
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON cd.exception_type = 1 and cd.date = service_day
      ),
      
      -- get trip durations for all scheduled services on each day
      trip_info AS (
        SELECT 
              service_day
              , st.trip_id
              , t.route_id
              , t.shape_id
              , r.route_desc
              , min(departure_time) as trip_start
              , max(arrival_time) as trip_end
              , 60*datepart('hour', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL)
                + datepart('minute', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL) as trip_duration_minutes
        FROM services_scheduled ss
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/trips.parquet') AS t
          ON ss.service_id = t.service_id AND t.gtfs_end_date >= service_day AND t.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/stop_times.parquet') AS st
          ON st.trip_id = t.trip_id AND st.gtfs_end_date >= service_day AND st.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/routes.parquet') AS r
          ON t.route_id = r.route_id AND r.gtfs_end_date >= service_day AND r.gtfs_active_date <= service_day
        GROUP BY 1,2,3,4,5
      ),
      
      -- deduplicate trip IDs with same start and end times on same routes and days
      trips_unique AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , min(shape_id) as shape_id
        FROM trip_info
        GROUP BY 1, 2, 3, 4, 5, 6
      ),
      
      prev_point_dists AS (
        SELECT
          shape_id
          , shape_pt_lat
          , shape_pt_lon
          , gtfs_active_date
          , gtfs_end_date
          , st_distance_sphere(
              st_point(shape_pt_lon, shape_pt_lat), 
              st_point(lag(shape_pt_lon) over prev, lag(shape_pt_lat) over prev)
            ) AS dist_from_prev
        FROM read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/shapes.parquet') 
        WINDOW prev AS (PARTITION BY shape_id ORDER BY shape_pt_sequence)
      ),
      
      trips_with_dists AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , sum(dist_from_prev) as dist_traveled_meters
        FROM trips_unique tu
        JOIN prev_point_dists ppd ON tu.shape_id = ppd.shape_id
          AND ppd.gtfs_end_date >= service_day AND ppd.gtfs_active_date <= service_day
        GROUP BY 1, 2, 3, 4, 5, 6
      )
  
      SELECT
        service_day
        , route_id
        , route_desc
        , count(*) as num_trips
        , sum(trip_duration_minutes) as total_runtime_minutes
        , sum(dist_traveled_meters)/1000 as total_runtime_km
        , min(trip_start) as first_trip_start
        , max(trip_end) as last_trip_end
      FROM trips_with_dists
      GROUP BY 1,2,3;
    ")
  
  return(run_duckdb(gtfs_metrics_query))
}
```

Wrapper function for calculating the daily QoS metrics for any date range, even ones that span multiple years. Iterates over years because the MBTA compressed GTFS archive tables are partitioned into separate years.

```{r}
mbta_gtfs_metrics_between <- function(date_start, date_end) {
  yyyymmdd_st <- as.integer(strftime(strptime(date_start, '%Y-%m-%d'), '%Y%m%d'))
  yyyymmdd_end <- as.integer(strftime(strptime(date_end, '%Y-%m-%d'), '%Y%m%d'))
  yyyy_st <- as.integer(yyyymmdd_st/10000)
  yyyy_end <- as.integer(yyyymmdd_end/10000)
  
  df <- data.frame()
  
  for (yyyy in yyyy_st:yyyy_end) {
    dt_st <- strftime(strptime(max(yyyy*10000+101, yyyymmdd_st), '%Y%m%d'), '%Y-%m-%d')
    dt_end <- strftime(strptime(min(yyyy*10000+1231, yyyymmdd_end), '%Y%m%d'), '%Y-%m-%d')
    
    df <- bind_rows(df, mbta_gtfs_metrics(dt_st, dt_end, yyyy))
  }

  return(df)
}
```

Set date range and run the data pull - code may take several minutes per calendar year

```{r}
date_start <- "2010-01-01"
date_end <- "2025-02-01"
gtfs_metrics <- mbta_gtfs_metrics_between(date_start, date_end)
```

```{r}
gtfs_metrics %>%
  write.csv(path(data_path, "mbta_gtfs_metrics_route.csv"), row.names=FALSE)
```

## Visualize MBTA GTFS metrics

Visualize the QoS "inputs" over time which represent the context / overall service levels & trends for the period of service changes being analyzed as a case study

```{r}
gtfs_metrics <- read.csv(path(data_path, "mbta_gtfs_metrics_route.csv"))
```

Identify service days with suspiciously low total runtimes (likely data anomalies) to filter out in the subsequent chart.

```{r}
sus_days <- gtfs_metrics %>%
  mutate(service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) >= 2019) %>%
  group_by(service_dt) %>%
  summarise(total_runtime_minutes_day = sum(total_runtime_minutes)) %>%
  filter(total_runtime_minutes_day < 300000) %>%
  select(service_dt) %>% unique()
```

Aggregation of SQoS metrics to the route category level

```{r}
yyyy <- 2019
mbta.colors <- c('Rail Replacement Bus' = "#444444", 'Rapid Transit' = "#FD8A03", 
                 'Regional Rail' ="#82076c", 'Ferry' = "#008eaa", 
                 'Frequent Bus' = "#9A9C9D", 'Other Bus' = "#ffc72c")

gmc <- gtfs_metrics %>%
  mutate(
    service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d')),
    route_category = factor(case_when(
      route_desc %in% c('Community Bus', 'Commuter Bus', 'Coverage Bus', 'Supplemental Bus', 'Local Bus') ~ 'Other Bus',
      route_desc %in% c('Frequent Bus', 'Key Bus') ~ 'Frequent Bus',
      route_desc %in% c('Commuter Rail', 'Regional Rail') ~ 'Regional Rail',
      .default = route_desc
      ), 
      levels=c('Rail Replacement Bus', 'Rapid Transit', 'Regional Rail', 'Ferry', 'Frequent Bus', 'Other Bus')
    )
    ) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) >= yyyy,
         !(service_dt %in% sus_days$service_dt), !is.na(route_category)) %>%
  group_by(route_category, service_dt) %>%
  summarise(across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), .groups='drop')
```

Chart of total daily runtime by MBTA route category going back multiple years for Tuesdays, annotated with GLX Medford Opening and BNR Phase 1

``` {r}
gmc %>%
  ggplot(aes(fill=route_category, y=total_runtime_minutes, x=service_dt)) +
  geom_bar(position="stack", stat="identity") + 
  scale_y_continuous(labels=comma) +
  scale_fill_manual(values=mbta.colors) +
  annotate("text", x=as.POSIXct('2022-07-12'), y=585000,
           label = 'GLX Medford Opening', color='red', size=3) +
  annotate("segment", x=as.POSIXct('2022-12-12'), y=570000, yend=490000, color='red',
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  annotate("text", x=as.POSIXct('2024-07-15'), y=585000,
           label = 'BNR Phase 1 Opening', color='red', size=3) +
  annotate("segment", x=as.POSIXct('2024-12-15'), y=570000, yend=510000, color='red',
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Service Date', y='Daily Minutes of Vehicle Runtime', fill='Route Category',
       title=paste0("Daily Scheduled MBTA Runtime by Route Category (Tuesdays, Jan. ",
                    as.character(yyyy), " - Jan. 2025)"))
```

Versions of this chart with shorter x-axes were also used to identify dates before and after the MBTA service changes of interest that had minimal (or roughly equivalent) Rail Replacement Bus service. This informed the exact dates that are hard-coded into the next step.

## Create Feed Lookups & Pull MBTA GTFS full feeds

Using the MBTA's GTFS feed index, create data pulling functions which will download a feed if it isn't already in the data_path but skip it if it's already there

```{r mbta gtfs}
mbta_feed_index <- read.csv('https://cdn.mbta.com/archive/archived_feeds.txt')

# only downloads the feed for a given date if that feed is not already present in data_path
lookup_mbta_feed_from_date <- function(yyyymmdd) {
  feed_url <- mbta_feed_index[mbta_feed_index$feed_start_date <= yyyymmdd 
                         & mbta_feed_index$feed_end_date >= yyyymmdd,'archive_url']
  fname <- sub(".*/", "", feed_url)
  
  if (!file.exists(file.path(data_path, fname))) {
    download.file(url = feed_url,
              destfile = file.path(data_path, fname), mode = "wb")
  }
  lookup <- list()
  lookup[[as.character(yyyymmdd)]] <- fname
  return(lookup)
}

feed_list <- function(dates) {
  feed_lookup <- list()
  
  for (dt in dates) {
    feed_lookup <- c(feed_lookup, lookup_mbta_feed_from_date(dt))
  }
  
  return(feed_lookup)
}
```

Then, define list of specific dates to analyze and pull the full GTFS feeds associated with those days.

```{r}
mbta_changes <- list( # the comments define the effective date for each change
  'GLX Medford Opening' = c(20221206, 20221213), # 20221212
  'Winter 2023' = c(20221213, 20230103), # 20221218
  # 'Spring 2023' = c(20230228, 20230321), # 20230312
  # 'Summer 2023' = c(20230627, 20230711), # 20230702
  'Spring and Summer 2023' = c(20230228, 20230711),
  'Fall 2023' = c(20230822, 20230829), # 20230827
  'Spring 2024' = c(20240402, 20240409), # 20240407
  'Summer 2024' = c(20240611, 20240618), # 20240616
  'Fall 2024' = c(20240813, 20240827), # 20240825
  'Winter 2025 (BNR Phase 1)' = c(20241203, 20250107) # 20241215
)

mbta_analysis_dates <- unique(unlist(mbta_changes))
mbta_analysis_lookup <- feed_list(mbta_analysis_dates)
```

# Scheduled quantity of service (SQoS) calculations (before/after)

Calculate the changes in SQoS that are associated with each service change

```{r}
# this function summarizes gtfs_metrics changes for a given element of the mbta_changes list
# also aggregates MBTA route categories and either calculates at that aggregated route
# category level or at the individual route level, based on the by_route flag
mbta_change_summary <- function(change_name, by_route=FALSE) {
  agg_cols <- 'route_category'
  if (by_route) agg_cols <- c('route_id', agg_cols)
  
  b4_day <- mbta_changes[[change_name]][1]
  aft_day <- mbta_changes[[change_name]][2]
  
  gtfs_metrics %>% 
    filter(service_day %in% c(b4_day, aft_day)) %>%
    mutate(
      route_category = case_when(
        route_desc %in% c('Community Bus', 'Commuter Bus', 'Coverage Bus', 
                          'Supplemental Bus', 'Local Bus') ~ 'Other Bus',
        route_desc %in% c('Frequent Bus', 'Key Bus') ~ 'Frequent Bus',
        route_desc %in% c('Commuter Rail', 'Regional Rail') ~ 'Regional Rail',
        .default = route_desc
        )
      ) %>%
    group_by(across(all_of(c('service_day', agg_cols)))) %>%
    summarise(
      across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), 
      .groups='drop'
      ) %>%
    pivot_wider(
      id_cols= all_of(agg_cols),
      values_from=c(num_trips, total_runtime_minutes, total_runtime_km),
      names_from=service_day,
      values_fill = 0
      ) %>%
    mutate(
      num_trips_b4 = get(paste0('num_trips_', as.character(b4_day))),
      runtime_min_b4 = get(paste0('total_runtime_minutes_', as.character(b4_day))),
      runtime_km_b4 = get(paste0('total_runtime_km_', as.character(b4_day))),
      chg_trips = get(paste0('num_trips_', as.character(aft_day))) - num_trips_b4,
      chg_min = get(paste0('total_runtime_minutes_', as.character(aft_day))) - runtime_min_b4,
      chg_km = round(
        get(paste0('total_runtime_km_', as.character(aft_day))) - runtime_km_b4, 0),
      pct_chg_trips = round(100*chg_trips / num_trips_b4, 1),
      pct_chg_min = round(100*chg_min / runtime_min_b4, 1),
      pct_chg_km = round(100*chg_km / runtime_km_b4, 1)
    ) %>%
    #filter(chg_trips != 0 | chg_min != 0 | chg_km != 0) %>%
    select(
      all_of(c(agg_cols, 'chg_trips', 'chg_min', 'chg_km', 
               'pct_chg_trips', 'pct_chg_min', 'pct_chg_km',
               'num_trips_b4', 'runtime_min_b4', 'runtime_km_b4'))
    )
}
```

Calculate the changes in SQoS at both the route level and at the aggregated route category level

```{r}
mbta_change_summaries_highlevel <- names(mbta_changes) %>%
  lapply(mbta_change_summary) %>%
  `names<-`(names(mbta_changes))

mbta_change_summaries_routelevel <- names(mbta_changes) %>%
  lapply(function(n) mbta_change_summary(n, by_route=T)) %>%
  `names<-`(names(mbta_changes))
```

Look for service changes that met the MBTA's mode-level threshold for defining a "major service change" (though the aggregated route categories defined here do not actually correspond with MBTA-defined modes)

```{r}
# this confirms that the only non-temporary service change meeting the mode threshold was BNR Phase 1,
# and that's only if you count Frequent Bus as a separate mode
mbta_change_summaries_highlevel %>%
  lapply(function(df) df %>% filter(abs(pct_chg_min) > 10))
```

Look for service changes that met the MBTA's route-level threshold for defining a "major service change."

```{r}
mbta_change_summaries_routelevel %>%
  lapply(function(df) df %>% filter(abs(pct_chg_min) > 25))
```

SQoS chart for selected routes (chosen based on above explorations)

```{r}
yyyy <- 2022

gtfs_metrics %>%
  mutate(service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  filter(route_id %in% c('34', '441442', '4'), weekdays(service_dt) == 'Tuesday', 
         year(service_dt) >= yyyy, !(service_dt %in% sus_days$service_dt)) %>%
  ggplot(aes(linetype=route_id, y=total_runtime_minutes, x=service_dt)) +
  geom_line() + 
  scale_y_continuous(labels=comma) +
  annotate("text", x=as.POSIXct('2023-07-06'), y=1800,
           label = 'Summer 2023', color='red', size=3) +
  annotate("segment", x=as.POSIXct('2023-07-06'), y=1900, yend=2100, color='red',
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  annotate("segment", x=as.POSIXct('2023-07-06'), y=1650, yend=1450, color='red',
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  annotate("text", x=as.POSIXct('2024-08-25'), y=1200,
           label = 'Fall 2024', color='red', size=3) +
  annotate("segment", x=as.POSIXct('2024-08-25'), y=1100, yend=900, color='red',
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x='Service Date', y='Daily Minutes of Vehicle Runtime', linetype='Route ID',
       title=paste0("Daily Scheduled MBTA Runtime for Selected Routes (Tuesdays, Jan. ",
                    as.character(yyyy), " - Jan. 2025)"))
```

These explorations give color / interpretation to the quantity-of-service changes at least for full days. The destination access calculations only consider certain times of day, but the only way to really filter out any "noise" would be to rerun gtfs_metrics to only pull statistics for those hours. Full-day statistics should be OK. 

The other piece is to create a table summarizing these changes in QoS for the report. I will tack on destination access rows to thse QoS rows in the "Summary stats per change" section.

```{r}
extract_table_info <- function(change_name) {
  tot <- mbta_change_summaries_highlevel[[change_name]] %>%
  summarise(route_category = 'Total', across(chg_trips:runtime_km_b4, sum)) %>%
  mutate(pct_chg_trips = formatC(100*chg_trips / num_trips_b4, format='f', digits = 1),
         pct_chg_min = formatC(100*chg_min / runtime_min_b4, format='f', digits = 1),
         pct_chg_km = formatC(100*chg_km / runtime_km_b4, format='f', digits = 1))
  
  selected_categories <- mbta_change_summaries_highlevel[[change_name]] %>% 
    filter(route_category %in% c('Rapid Transit', 'Frequent Bus', 'Other Bus')) %>%
    mutate(across(pct_chg_trips:pct_chg_km, ~ formatC(.x, format='f', digits = 1)))
  
  tot %>%
    bind_rows(selected_categories) %>%
    mutate(chg = change_name, category=route_category) %>%
    select(-c('num_trips_b4', 'runtime_min_b4', 'runtime_km_b4'))
}

service_table_info <- names(mbta_change_summaries_highlevel) %>%
  lapply(extract_table_info) %>%
  bind_rows() %>%
  pivot_longer(cols=c(#'chg_trips', 'chg_min', 
                      'pct_chg_trips', 'pct_chg_min'),
               names_to='statistic') %>%
  pivot_wider(id_cols=c('category', 'statistic'), names_from='chg', values_from='value') %>%
  mutate(across(names(mbta_changes), ~paste0(.x, '%'))) %>%
  arrange(
    factor(category, levels=c('Total', 'Rapid Transit', 'Frequent Bus', 'Other Bus')),
    desc(statistic))
```

# R5R Calculations

## Define Parameters and Prepare Data

Using this set of variable definitions, users can set the key parameters that define how the origin-destination matrix and accessibilities are calculated. This includes supplying the input origins and destinations (as `sf` objects), defining relevant ID and destination opportunities fields, and selecting one or more impedance functions, travel modes, a date and time of departure (within the calendar dates of the supplied GTFS files), and maximum walking distance and trip duration.

In addition, a `chunk_size` parameter is used to control batching as the base `travel_time_matrix` function from `R5R` is employed here within a batching work flow built around `disk.frame`. In this formulation, the matrix is calculated for batches of origins to all destinations reachable within the travel time window for the selected modes. While `r5r`` works fastest with a single set of origins, this batching is implemented to save memory and enable the calculation of very large matrices. In addition, the resulting matrix is compressed and saved to disk for future use. Depending on your particular hardware configuration, you can change the number of origins considered in each batch.

```{r}
# load in pre-processed census data:
mbta_cb_poly <- readRDS(file.path(data_path, "mbta_cb_poly.rds"))
```

```{r define parameters, warning = FALSE}
# 1. Generalize calls to input simple features
# origins
origins_i <-  mbta_cb_poly
origins_id_field <- "Block_GEOID"

# destinations
destinations_j <-  mbta_cb_poly
destinations_id_field <- "Block_GEOID"
opportunities_j_field <- "total_emp"

# 2. R5 Travel Time Matrix Options. See https://rdrr.io/cran/r5r/man/travel_time_matrix.html for more detail
# R5 allows for multiple combinations of transport modes. The options include:
## Transit modes
# TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR. 
# The option 'TRANSIT' automatically considers all public transport modes available.

## Non transit modes
# WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK

# define your travel modes
mode <- c("WALK", "TRANSIT")

# egress mode
#mode_egress = "WALK" # Transport mode used after egress from public transport; it can be either 'WALK', 'BICYCLE', or 'CAR'. Defaults to "WALK"

# walk speed
#walk_speed = 3.6 #Average walk speed in km/h. Defaults to 3.6 km/h

# bike speed
#bike_speed = 12 # Average cycling speed in km/h. Defaults to 12 km/h
  
# max rides
#max_rides = 3 # The max number of public transport rides allowed in the same trip. Defaults to 3

# level of traffic stress (cycling)
#max_lts = 2 # The maximum level of traffic stress that cyclists will tolerate. A value of 1 means cyclists will only travel through the quietest streets, while a value of 4 indicates cyclists can travel through any road; defaults to 2

# max trip duration (minutes)
max_trip_duration = 60

# set the time of day for the departure (date will be set later)
departure_time_hms <- "07:30:00"

# # of minutes starting from departure_time_hms within which to consider trip starts
time_window_minutes <- 30 

# travel time percentiles of the above time window (e.g., 85% of trips took X min or less)
time_window_percentiles <- c(15, 30, 50, 70, 85) 

# processing batch size - how many origin rows to process at one time
chunksize = 500 # how many origins to consider at one time; set to nrow(origins_sf) if you don't want to use

# multithreading
n_threads = Inf # The number of threads to use in parallel computing; defaults to use all available threads (Inf)
```

```{r prepare input data for r5r, warning = FALSE, include = FALSE}
# ORIGINS
# create input sf object and change to crs 4326 for origins
origins_i_sf <- origins_i %>% 
  select(all_of(origins_id_field)) %>% 
  rename(id = all_of(origins_id_field))

origins_i <- origins_i_sf %>% 
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) %>% # transform to lat-longs
  mutate(batch_id = ceiling(row_number()/chunksize))

# DESTINATIONS
# create input sf object and change to crs 4326 for destinations
destinations_j <- destinations_j %>% 
  select(all_of(c(destinations_id_field, opportunities_j_field))) %>% 
  rename(id = all_of(destinations_id_field), o_j = all_of(opportunities_j_field)) %>%
  filter(o_j > 0) %>%
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) # transform to lat-longs

opportunities_j <- destinations_j %>%
  st_drop_geometry() %>% 
  select(id, o_j) %>% 
  rename(toId = id)
```

## Calculate Origin-Destination Travel Time Matrices

Third, an origin-destination matrix is calculated for each service date in accordance with the travel and batching parameters set earlier. In this case, I have set the ODCM to save to disk as a parquet dataset, which is a great format for compressed storage and for larger-than-memory calculations. This process took about 35 minutes per date for MBTA blocks (n=~80k).

```{r}
lookup <- mbta_analysis_lookup
# loop through lookup and get feed_day (name) and feed_curr (value)
# name the ttm after the feed day, since different days on the same feed could (?) be different
for (i in 1:length(lookup)) {
  feed_day <- names(lookup)[[i]]
  feed_curr <- lookup[[i]]
  
  # move the current feed into the r5 path from the data path
  file_move(path(data_path, feed_curr), path(r5_path, feed_curr))
  
  # create the network graph using the new GTFS feed
  r5r_core <- setup_r5(data_path = r5_path, verbose = FALSE, overwrite=TRUE)
  
  ttm_path_curr <- dir_create(paste0("./r5_ttm/", feed_day))
  
  # define trip start datetime
  departure_datetime <- as.POSIXct(
    paste(strftime(strptime(feed_day, '%Y%m%d'), '%Y-%m-%d'), departure_time_hms), 
    format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York") 
  # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for list of time zone codes
  
  # set up batching 
  num_chunks = origins_i %>% st_drop_geometry() %>% summarize(num_chunks = max(batch_id)) %>% pull(num_chunks)
  
  # compute travel time matrix
  start.time <- Sys.time()
  pb <- txtProgressBar(0, num_chunks, style = 3)
  
  for (i in 1:num_chunks){
    origins_i_chunk <- origins_i %>% filter(batch_id == i)
    
    ttm_chunk <- travel_time_matrix(
      r5r_core = r5r_core,
      origins = origins_i_chunk,
      destinations = destinations_j,
      mode = mode,
      departure_datetime = departure_datetime,
      max_trip_duration = max_trip_duration,
      verbose = FALSE,
      progress = FALSE,
      time_window = time_window_minutes, 
      percentiles = time_window_percentiles
      ) %>%
      
    mutate(batch_id = i)
    
    # export output as parquet
    write_dataset(ttm_chunk, path = ttm_path_curr, partitioning = "batch_id")
    
    setTxtProgressBar(pb, i)}
  
  end.time <- Sys.time()
  print(paste0(
    "OD matrix calculation took ", 
    round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."
    ))
  
  # to wrap up, we move the current feed out of the r5 path...
  file_move(path(r5_path, feed_curr), path(data_path, feed_curr))
  
  # ...and terminate the r5r instance to clear memory
  r5r::stop_r5(r5r_core)
  rJava::.jgc(R.gc = TRUE)
}
```

## Calculate Accessibility

Finally, accessibilities are calculated using the selected impedance function(s) by iterating over all the batches of the origin-destination matrix per service date. There's a few ways of doing this.

TODO: try generating some of the accessibility outputs shown here: https://cran.r-project.org/web/packages/r5r/vignettes/accessibility.html

TODO: parameterize ttm_path
TODO: decide on data storage for access datasets
maybe /outputs -> (optional layers for agency, metric type, etc) -> csv per feed day?

The visualization I think I want next is a line chart with shaded areas representing the high and low percentage bounds of accessibility and dates on the x axis. To do that, I need to calculate accessibility from each ttm and then aggregate that, starting with just the population average or median but likely expanding to a bunch of different metrics at some point (grabbing a set of accessibility percentiles - distinct from within-window percentiles, grouping by population, calculating gini / palma / concentration indices for inequality, etc). There's also the calculation of before/after changes in access per block, but that kind of visualization will be limited to just examples since we're interested in lots of little changes. 

How can I measure if a schedule change makes the time-of-day percentiles less variable? Because what if there are changes for some people, but not for the median person? A given block could see higher accessibility levels but the same, smaller, or greater range of time-of-day accessibility values, either in absolute terms or relative to its level. That may be another example map worth creating, but for this study again I'll need to abstract or aggregate that a bit more. Probably at first just by calculating the population average for each time-of-day percentile and then comparing the spreads over time. 

I'm realizing that there is a large rabbit hole of possible metrics here, even in this, my initial test run. 

```{r}
lud <- mbta_cb_poly %>%
  rename(id = all_of(origins_id_field))

time_window_pctile_suffixes <- '_p' %>%
  paste0(time_window_percentiles %>% as.character())
```

```{r}
read_ttm_for_day <- function(feed_day) {
  open_dataset(path(paste0(ttm_path, '/', as.character(feed_day)))) %>%
    collect()
}

axs_at_pctile <- function(ttm, feed_day, pctile_suffix) {
  ttm_pctile <- ttm %>%
    select(all_of(
      c('from_id', 'to_id', paste0('travel_time', pctile_suffix))
      )) 
  
  print(paste("Calculating destination access for", pctile_suffix))
  axs_pctile <- ttm_pctile %>%
    cumulative_cutoff(
        land_use_data = lud, opportunity = 'total_emp',
        travel_cost = paste0('travel_time', pctile_suffix), 
        cutoff=max_trip_duration
      ) 
  
  axs_pctile %>%
    rename_with(~paste0('total_emp', pctile_suffix), total_emp) %>%
    mutate(service_day = feed_day)
}

axs_window_percentiles <- function(ttm, feed_day) {
   time_window_pctile_suffixes %>%
    # then calculate the access at each time window percentile for the given ttm
    lapply(function(p) axs_at_pctile(ttm, feed_day, p)) %>%
    reduce(inner_join, by=c('service_day', 'id')) # and join the results
}
```

Read the travel time matrices for a set of dates and calculate accessibility per percentile. This cell took ~2min per date for MBTA blocks (n=~80k).  

```{r}
for (feed_day in names(mbta_analysis_lookup)) {
  print(paste("Reading travel time matrix for", as.character(feed_day)))
  ttm <- read_ttm_for_day(feed_day)
  
  print(paste("Calculating destination access for", as.character(feed_day)))
  axs_window_percentiles(ttm, feed_day) %>%
    write.csv(
      path(outputs_path, paste0(as.character(feed_day), ".csv")), 
      row.names=F
    )
}
```

## Summarize Accessibility

Read in previously calculated accessibility dataframes

```{r}
axs_dfs <- names(mbta_analysis_lookup) %>%
  lapply(function(fd) read.csv(path(outputs_path, paste0(as.character(fd), ".csv")))) %>%
  `names<-`(names(mbta_analysis_lookup))
```

Get race/ethn data, clean it, and join it in

```{r}
ma_blocks_raceethn <- read.csv(paste0(data_path, '/raceethn_2020_block_MA.csv'))

block_raceethn <- ma_blocks_raceethn %>%
  mutate(block_id = as.numeric(substr(GEOID, 10, length(GEOID)))) %>%
  select(block_id, nh_white, nh_black, nh_aapi, nh_others, hisp_all)

block_pops <- mbta_cb_poly %>%
  st_drop_geometry() %>%
  rename(id = all_of(origins_id_field)) %>%
  mutate(id = as.numeric(id)) %>%
  select(id, total_pop) %>%
  inner_join(block_raceethn, by=join_by(id == block_id))
```

Join access data to block populations

```{r}
axs_all <- axs_dfs %>%
  bind_rows() %>%
  inner_join(block_pops, by='id')
```

Create dataframes of the before-and-after for each change

```{r}
# for each mbta_change to consider, create a before and after and then stack?
# given that a single service date can be part of multiple calcs, that be the best way
chg_axs <- function(chg_name) {
  axs_all %>%
    filter(service_day %in% mbta_changes[[chg_name]]) %>%
    mutate(period = case_when(
      service_day == mbta_changes[[chg_name]][2] ~ 'after',
      .default='before')) %>%
    pivot_longer(cols=total_emp_p15:total_emp_p85, names_to='tod_ptile', values_to='axs') %>%
    mutate(tod_ptile=substr(tod_ptile, 11, 14)) %>% # e.g., "total_emp_p50" -> "p50"
    pivot_wider(
      id_cols=c('id', 'total_pop', 'nh_white', 'nh_black', 'nh_aapi', 
                'nh_others', 'hisp_all', 'tod_ptile'), 
      names_from='period', 
      values_from='axs') %>%
    mutate(change_name = chg_name, delta = after - before,
           access_bin_before = before - (before %% 10000),
           access_bin_after = after - (after %% 10000)) %>%
    pivot_wider(id_cols=c('change_name', 'id', 'total_pop', 'nh_white', 
                          'nh_black', 'nh_aapi', 'nh_others', 'hisp_all'),
                names_from='tod_ptile', 
                values_from=c('before', 'after', 'delta', 
                              'access_bin_before', 'access_bin_after')) %>%
    mutate(non_white = nh_black + nh_aapi + nh_others + hisp_all) %>%
    arrange(before_p50) %>%
    mutate(pop_pctile_before = lag(cumsum(total_pop), default = 0)/(sum(total_pop) - 1)) %>%
    arrange(after_p50) %>%
    mutate(pop_pctile_after = lag(cumsum(total_pop), default = 0)/(sum(total_pop) - 1)) %>%
    mutate(delta_pop_pctile = pop_pctile_after - pop_pctile_before)
}

mbta_axs_changes <- names(mbta_changes) %>%
  lapply(chg_axs) %>%
  bind_rows()
```

### Compare distributions of access changes

The viz below compares the distributions of changes across scenarios. It shows that most of the distributions are quite similar, with all but GLX Medford Opening having a mix of winners and losers with tails leaning in the direction of the service changes.  I'm not sure if any of these will make it into the final paper, given that the untransformed-axis charts make it hard to compare across adjustments, while the transformed-axis charts imply a bimodal distribution that isn't really there. 

```{r}
# the first element of this will be plotted first (back), then the next overlaid in front of it, etc 
adjustments_to_plot <- c('Winter 2025 (BNR Phase 1)', 'Spring and Summer 2023', 'GLX Medford Opening')
#pseudolog10_breaks <- c(-1*(10^seq(1,5, by=1)), 10^seq(1,5, by=1))

mbta_axs_changes %>%
  filter(
    delta_p50 != 0, # only consider the affected population for each adjustment
    change_name %in% adjustments_to_plot) %>%
  mutate(change_name = factor(change_name, levels=adjustments_to_plot)) %>%
  ggplot(aes(x=delta_p50)) +
  geom_vline(xintercept = 0, linetype='longdash') +
  scale_x_continuous(label=comma, limits=c(-100000,100000)
                     #, transform=pseudolog10_trans, breaks=pseudolog10_breaks
                     ) +
  
  # OPTION 1: access changes per population (non-normalized - compare overall sizes)
  geom_area(
    aes(weight=total_pop, fill=change_name),
    position='identity', alpha=0.5,
    stat = "bin", binwidth=1000) +
  geom_freqpoly(
    aes(weight=total_pop, group=change_name),
    alpha=0.5, stat = "bin", binwidth=1000) +
  labs(x='Change in Jobs Reachable within 60 Minutes via Transit + Walking \nStarting from 7:30am-8:00am (Median Departure Timing)', y='Affected Population', fill='Service Change Name',
       title='Job Access Change Distributions (Absolute) per Service Change') +
  scale_y_continuous(label=comma)

  # # OPTION 2: access changes per population portion (normalized, compare concentrations)
  # geom_density(aes(weight=total_pop, fill=change_name, alpha=0.5)) +
  # labs(x='Change in Jobs Accessible', y='Portion of Affected Population per Adjustment', fill='Adjustment Name',
  #      title='Job Access Change Distributions (Proportional) per Service Change')
```

The viz below shows how much a given adjustment adds or subtracts access (on net) from communities with lower or higher pre-existing access levels. The Summer 2023 schedule decreased access on net for medium- and high-access communities while adding access for some previously low-access communities. Meanwhile, the GLX Medford opening added access in mostly medium-to-high access communities, while BNR Phase 1 netted large access gains across the board, but especially in previously low-access communities. This viz masks the variation within before-access bins, since we know from the previous chart that Winter 2025 changes had some losers, it's just that everyone's losses are netted out by the gains of other communities with similar pre-existing access levels.

```{r}
mbta_axs_changes %>%
  filter(change_name %in% adjustments_to_plot) %>%
  mutate(#pop_ptile_b4_bin = round(pop_pctile_before*100),
         change_name = factor(change_name, levels=adjustments_to_plot)) %>%
  group_by(change_name, access_bin_before_p50) %>%
  summarise(chg_personjob_access = sum(total_pop * delta_p50), .groups='drop') %>%
  ggplot() + 
  geom_col(aes(x=access_bin_before_p50, y=chg_personjob_access, fill=as.factor(change_name)),
           alpha=.5, position='identity') +
  scale_x_continuous(label=comma) + 
  scale_y_continuous(label=comma) +
  labs(x='Pre-Adjustment Jobs Accessible', 
       y='Aggregate Population-Weighted Change in Jobs Accessible', 
       fill='Adjustment Name',
       title='Changes in Population-Weighted Jobs Accessible by Pre-Adjustment Access Level')
```

Substituting pop percentile bins for access bins shows that >50% of the population of the 7 MA counties in my analysis have access to <10k jobs via MBTA+walk, and see barely any changes because those jobs are probably all walk and they're far away from MBTA service. I could conceivably do a post-hoc filter for blocks within a mile or two of MBTA stops, and that would be a better method for comparability across metros, but whatever filter I apply would both not fully solve that chart problem and be somewhat arbitrary.

I also tried graphing the changes in population per access bin for a given change ("an additional x people joined the cohort of people with access to y jobs or more") but it ends up looking like radio spaghetti due to populations moving from one group to another showing up as losses / gains in adjacent bins. I'm saving it as optional code in case some derivative of this becomes interesting, but I doubt it.

```{r}
# pop_per_bin_b4 <- mbta_axs_changes %>%
#   filter(change_name %in% adjustments_to_plot) %>%
#   group_by(change_name, access_bin_before) %>%
#   summarise(total_pop = sum(total_pop), .groups='drop')
# 
# pop_per_bin_aft <- mbta_axs_changes %>%
#   filter(change_name %in% adjustments_to_plot) %>%
#   group_by(change_name, access_bin_after) %>%
#   summarise(total_pop = sum(total_pop), .groups='drop')
# 
# pop_per_bin_b4 %>%
#   full_join(pop_per_bin_aft, by=join_by(
#     change_name == change_name, access_bin_before == access_bin_after
#     ), suffix=c('_b4', '_aft')) %>%
#   mutate(delta_pop = total_pop_aft - total_pop_b4) %>%
#   ggplot() + 
#   geom_col(aes(x=access_bin_before, y=delta_pop, fill=as.factor(change_name)),
#            alpha=.5, position='identity') +
#   scale_x_continuous(label=comma) + 
#   scale_y_continuous(label=comma) +
#   labs(x='Jobs Accessible', 
#        y='Change in Population', 
#        fill='Adjustment Name',
#        title='Changes in Population by Access Level')
```

### Summary stats per change

TODO: 
- p15-p85 swings before and after per block
- max or p99 change in axs per service change

```{r}
access_table_info <- mbta_axs_changes %>%
  # mutate to add
  group_by(change_name) %>%
  summarise(
    total_affected_pop = sum(case_when(delta_p50 != 0 ~ total_pop, .default=0)),
    across(before_p15:before_p85, ~ weighted.mean(.x, total_pop),
           .names='popmean_{.col}'),
    across(delta_p15:delta_p85, ~ weighted.mean(.x, total_pop),
           .names='popmean_{.col}'),
    across(delta_p15:delta_p85, ~ sqrt(wtd.var(.x, total_pop)),
           .names='popstdev_{.col}'),
    across(nh_white:hisp_all, ~ weighted.mean(before_p50, .x), 
         .names='popmean_before_{.col}'),
    across(nh_white:hisp_all, ~ weighted.mean(delta_p50, .x), 
         .names='popmean_delta_{.col}'),
    across(nh_white:hisp_all, ~ sqrt(wtd.var(delta_p50, .x)), 
         .names='popstdev_delta_{.col}'),
    .groups='drop'
  ) %>%
  pivot_longer(cols=total_affected_pop:popstdev_delta_hisp_all, names_to = 'category') %>%
  mutate(statistic = case_when(str_detect(category, "before") ~ 'before',
                              str_detect(category, "popmean_delta") ~ 'popmean_delta',
                              str_detect(category, "popstdev_delta") ~ 'popstdev_delta',
                              .default='standalone'),
         category = case_when(
           str_detect(category, "before") ~ str_replace(category, 'popmean_before_', ''),
           str_detect(category, "popmean_delta") ~ str_replace(category, 'popmean_delta_', ''),
           str_detect(category, "popstdev_delta") ~ str_replace(category, 'popstdev_delta_', ''),
           .default=category)) %>%
  pivot_wider(id_cols=c(change_name, category), names_from=statistic, values_from=value) %>%
  mutate(pct_chg = popmean_delta/before, pct_stdev = popstdev_delta/before,
         formatted = ifelse(is.na(pct_chg), NA, paste0(
           as.character(formatC(100*pct_chg, format='f', digits = 1)), '% (', 
           as.character(formatC(100*pct_stdev, format='f', digits = 1)), '%)'
           )),
         standalone = ifelse(is.na(standalone), NA,
                             prettyNum(standalone, big.mark=",", preserve.width="none")
                             )) %>%
  pivot_longer(cols=c(standalone, formatted), 
               names_to = 'statistic', values_drop_na = TRUE) %>%
  
  # # OPTIONAL: also calculate the ratio between p85 and p15 - could be calculated post-hoc too
  # pivot_wider(id_cols=c('change_name', 'statistic'), 
  #             names_from='category', values_from='value') %>%
  # mutate(ratio_p85_p15 = ifelse(p15 == 0, NA, p85 / p15)) %>%
  # pivot_longer(cols=c(p15:ratio_p85_p15), names_to = 'category') %>%
  
  pivot_wider(id_cols=c('statistic', 'category'), names_from='change_name',
              values_from='value') %>%
  select(category, statistic, all_of(names(mbta_changes)))
```

```{r}
service_table_info %>%
  bind_rows(access_table_info) %>%
  write.csv(paste0(outputs_path, '/summary_table.csv'), row.names=F)
```

TODO: quadrant or other viz of chg axs vs chg in tod ptile swing of axs ("consistency"?)

### Demographic breakdown visualizations

average change in job access per demographic group by adjustment. can graph before or after instead to get differences in average existing access

TODO: version by white/nonwhite only to reflect Title VI requirement; clean up axes

```{r}
mbta_axs_changes %>%
  mutate(change_name = factor(change_name, levels=names(mbta_changes))) %>%
  pivot_longer(
    cols=c(non_white, # OPTION 1: combine all non-white
           # nh_black, nh_aapi, nh_others, hisp_all, # OPTION 2: 5 major race/ethn groups
           nh_white),
    names_to = 'race', values_to = 'pop'
  ) %>% 
  mutate(
    race = case_match(race, 
                      'nh_white' ~ 'White (non-Hispanic)', 
                      'non_white' ~ 'Non-white (or Hispanic of any race)')
  ) %>%
  filter(pop != 0, delta_p50 != 0) %>%
  # pivot_longer(
  #   cols=c(before, after),
  #   names_to = 'stage', values_to = 'access'
  # ) %>% 
  group_by(change_name, race) %>%
  summarise(
    mean_axs = wtd.mean(delta_p50, weights=pop),
    stderr_axs = sqrt(wtd.var(delta_p50, weights=pop))/sqrt(sum(pop)),
    .gruops='drop'
    ) %>%
  ggplot(aes(fill=race, x=race, y=mean_axs)) + 
  geom_bar(position='dodge', stat='identity') +
  scale_y_continuous(labels=comma) +
  geom_hline(yintercept=0, linewidth=0.2) +
  #geom_bar(position=position_dodge(width=0.9)) +
  geom_errorbar(aes(ymax=mean_axs + stderr_axs, ymin = mean_axs - stderr_axs), 
                position=position_dodge(width=0.9), width=0.25) + 
  labs(x='', fill='', y='Change in Jobs Reachable in 60 min. via transit + walk',
       title='Average Changes in Job Access by Race & Ethnicity for MBTA Service Changes') +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
        legend.position = "top") +
  facet_wrap(vars(change_name))
```

### Statistical tests for race & ethnicity

```{r}
named_group_split <- function(.tbl, ...) {
  grouped <- group_by(.tbl, ...)
  names <- rlang::inject(paste(!!!group_keys(grouped), sep = " / "))

  grouped %>% 
    group_split() %>% 
    rlang::set_names(names)
}

axs_chgs_raceethn_aovs <- mbta_axs_changes %>%
  pivot_longer(
    cols=c(non_white, # OPTION 1: combine all non-white
           # nh_black, nh_aapi, nh_others, hisp_all, # OPTION 2: 5 major race/ethn groups
           nh_white),
    names_to = 'race', values_to = 'pop'
  ) %>% 
  filter(pop != 0, delta_p50 != 0) %>%
  named_group_split(change_name) %>%
  map(function(df) {aov(delta_p50 ~ race, data=df, weights=df$pop)})
```

ANOVA analyses

```{r}
aov_table <- axs_chgs_raceethn_aovs %>%
  map(tidy) %>%
  bind_rows(.id='id') %>%
  arrange(by=factor(id, levels=names(mbta_changes)))
```

```{r}
aov_table %>%
  write.csv(path(outputs_path, "anova_results.csv"), row.names=F)
```

Tukey post-hoc tests 

```{r}
tukey_table <- axs_chgs_raceethn_aovs %>%
  map(TukeyHSD) %>%
  map(tidy) %>%
  bind_rows(.id='id') %>%
  arrange(by=factor(id, levels=names(mbta_changes)))
```

```{r}
tukey_table %>%
  write.csv(path(outputs_path, "tukey_results.csv"), row.names=F)
```

### Maps & Spatial Analysis

TODO: map of snapshot access
TODO: map of change in 15-to-85 range

These tmap maps are useful for me as an exploration of various mappable metrics, but I'd rather just export to QGIS to make polished maps for publication rather than try to do it all here.

```{r}
sf_delta_axs <- origins_i_sf %>%
  mutate(id = as.numeric(id)) %>%
  inner_join(mbta_axs_changes, by='id') %>%
  mutate(change_name=factor(change_name, levels=names(mbta_changes))) %>%
  group_by(id) %>%
  filter(sum(abs(delta_p50)) > 0) %>%
  mutate(delta_p50 = case_when(delta_p50 == 0 ~ NA, .default=delta_p50))
```

Facet map of changes in access:

``` {r}
tm_shape(sf_delta_axs) +
  tm_polygons(fill = 'delta_p50',  
          fill.scale = tm_scale_continuous_pseudo_log(
            values='brewer.prgn', values.range = c(0, 1), base=2, sigma=20000,
            value.na = 'grey75', label.na = "Gray = Zero Change",
            ticks = c(-300000, -30000, 0, 30000, 300000)
            ), # rd_yl_gn
          fill.legend = tm_legend("Change in Job Access", reverse=TRUE, item.height=5),
          col_alpha = 0) +
  tm_facets("change_name", scale.factor=3) +
  tm_layout(frame = FALSE,
            bg.color = "grey85",
            legend.outside = TRUE
            ) #+
  # tm_title("Changes in jobs reachable in 60 min. via transit per census block for 50th 
  # \npercentile departure times from 7:30-8:00am on Tuesdays, Dec. 2022 to Jan. 2025")
```

TODO: decide on adding spatial autocorrelation or other statistics to aggregate_axs()

### Summary stats across dates

TODO: add summary statistics of social and geographic equity
- then, group befores, afters, and changes by race/ethn and do stderr / anova / tukey
- then add gini & palma to aggregate_axs()

```{r}
aggregate_axs <- function(df) {
  df %>% 
    inner_join(block_pops, by='id') %>%
    group_by(service_day) %>%
    summarise(
      across(total_emp_p15:total_emp_p85, ~ weighted.mean(.x, total_pop),
             .names='{.col}_popmean'),
      # across(total_emp_p15:total_emp_p85, ~ quantile(.x, probs=0.8), 
      #        .names='{.col}_blockp80'),
      
      # # this one is interesting, but at lower percentiles says more about the non-MBTA areas than anything
      # across(total_emp_p15:total_emp_p85, ~ wtd.quantile(.x, weights=total_pop, probs=0.8), 
      #      .names='{.col}_popp80'),
      
      across(nh_white:hisp_all, ~ weighted.mean(total_emp_p50, .x), 
           .names='{.col}_popmean'),
      .groups='drop'
      )
}
```

```{r}
axs_agg <- axs_dfs %>%
  lapply(aggregate_axs) %>%
  bind_rows()
```

TODO: directly plot the change in 15-85 range over time (try absolute & pct)

```{r}
metric <- 'popmean'
svcchg_label_color <- 'red'
ptile_label_color <- 'black'

axs_agg %>%
  mutate(service_date = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  ggplot(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_line(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_point(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_ribbon(
    aes(x=service_date, y=get(paste0('total_emp_p50_', metric)), 
        ymax=get(paste0('total_emp_p30_', metric)), 
        ymin=get(paste0('total_emp_p70_', metric))
        ), alpha=0.2) +
  scale_y_continuous(labels=comma, limits=c(0,190000)) +
  annotate("text", x=as.POSIXct('2023-02-12'), y=100000,
           label = 'GLX Medford Opening', color=svcchg_label_color, size=3) +
  annotate("segment", x=as.POSIXct('2022-12-10'), y=105000, yend=150000, color=svcchg_label_color,
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  annotate("text", x=as.POSIXct('2024-01-12'), y=126000,
           label = '70', color=ptile_label_color, size=3) +
  annotate("text", x=as.POSIXct('2024-01-12'), y=148000,
           label = '50', color=ptile_label_color, size=3) +
  annotate("text", x=as.POSIXct('2024-01-12'), y=165000,
           label = '30', color=ptile_label_color, size=3) +
    annotate("text", x=as.POSIXct('2023-10-12'), y=175000,
           label = 'Departure Timing Percentiles:', color=ptile_label_color, size=3) +
  annotate("text", x=as.POSIXct('2024-10-15'), y=100000,
           label = 'BNR Phase 1 Opening', color=svcchg_label_color, size=3) +
  annotate("segment", x=as.POSIXct('2024-12-15'), y=105000, yend=140000, color=svcchg_label_color,
           arrow=arrow(type = "closed", length = unit(0.015, "npc"))) +
  labs(
    #title="Job Accessibility via Transit & Walking for Average MBTA-Area Resident",
    y = "Jobs Reachable within 60 Minutes Starting from 7:30am-8:00am", 
    x="Service Date (Tuesdays)"
  )
  
```

The reason 20230627 is so much lower than 20230321 seems to be the Red Line shuttling from Braintree to South Station that occurred on the former date, which was lifted for 20230711 but simultaneously replaced by a bunch of bus frequency cuts.
