---
title: "Accessibility Calculator for R"
author:
  - name: "Christopher D. Higgins"
    email: cd.higgins@utoronto.ca
    affiliation: University of Toronto
---

Template source: https://github.com/higgicd/Accessibility_Toolbox/

# ABSTRACT
Analyses of place-based accessibility undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. Moreover, questions persist over the selection of an impedance function and cost or cut-off parameters. In response, this paper details a new Accessibility Toolbox for R and ArcGIS that includes a Python tool for conducting accessibility analyses and an interactive R Notebook that enables the visualization and customization of impedance functions and parameters. Using this toolbox, researchers and practitioners can simplify their accessibility analysis workflow and make better decisions about the specification and customization of travel impedance for their study context.

# KEYWORDS
accessibility, spatial interaction, travel behavior, travel impedance, distance decay function

```{r setup, include=FALSE}
# the r5r package requires installation of Java Development Kit version 21
# Installation instructions available at https://github.com/ipeaGIT/r5r

# assign memory for r5r to work with
options(java.parameters = "-Xmx6G") 

# load packages
library(corrplot)
library(httr) # to pull Transitland API data with GET requests
library(jsonlite)
library(knitr)
library(lehdr) # for pulling US Census LODES data
library(tigris) # for pulling US Census shapefiles
library(tidyverse) # for tidy functions and syntax
library(fs) # for file path operations
library(r5r) # for calculating travel time matrices over transportation networks
library(sf)
library(tmap)
library(tidycensus)
library(tidytransit)
library(viridis)
library(progress)
library(arrow) # for batched read + write + transform of parquet files on disk
library(duckdb) # for querying MBTA LAMP
library(accessibility) # for calculating destination access measures
library(scales) # for chart axis formatting
library(Hmisc) # for wtd.quantile function

# setup options
tmap_mode("plot")

# setup keys, creating or editing .Renviron file as needed 
# Guidance on .Renviron: https://laurenilano.com/posts/api-keys/#creating-the-.renviron-file
census_api_key(Sys.getenv("CENSUS_API_KEY")) # get from https://api.census.gov/data/key_signup.html
interline_token <- Sys.getenv("OSM_KEY") # for OSM extract from https://www.interline.io/osm/extracts/
transitland_key <- Sys.getenv("TRANSITLAND_KEY") # to get GTFS feeds from Transitland (https://www.transit.land/terms)

# create data directories (does not overwrite if these folders are already present)
data_path <- dir_create("./data") # for storing input data
outputs_path <- dir_create("./outputs") # for storing output data
ttm_path <- dir_create("./r5_ttm") # for storing od matrix
r5_path <- dir_create("./r5_graph") # for the r5 network graph
```

# RESEARCH QUESTIONS AND HYPOTHESES
Accessibility can be defined as the potential for reaching spatially distributed opportunities while considering the difficulty involved in traveling to them (Páez, Scott, and Morency, 2012). Several families of accessibility measures have been established since the pioneering work of Hansen (1959), including infrastructure-based, person-based, place-based, and utility-based (Geurs and van Wee, 2004). Of these, place-based measures are arguably the most common and can be operationalized as:

$$
A_i = \sum_{j}{O_jf(t_{ij})}
$$
where the accessibility $A$ of origin $i$ is the sum of all opportunities $O$ available at destinations $j$ weighted by some function of the travel time $t_{ij}$ between $i$ and $j$.

Despite several decades of research into place-based accessibility, researchers, students, and practitioners interested in accessibility analysis face practical and empirical challenges. On the practical side, compared to simple isochrones, analyses of spatial interaction undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. On the empirical side, questions persist over the selection of an impedance function and cost or cut-off parameters. Ideally, these statistical parameters should be derived from calibrated trip generation models, but in the absence of such data, Kwan (1998) argues that the use of customized functions and parameters based on theory is preferable to arbitrary assignment.

To promote a more "accessible" solution for accessibility analyses, this paper details a new Accessibility Toolbox for R and ArcGIS. The Python toolbox for ArcGIS simplifies the steps involved in a place-based accessibility workflow and comes coded with 5 impedance functions and 28 impedance measures for accessibility calculation. The interactive R Notebook version of this paper visualizes the function families and specifications and allows users to customize their parameters in accordance with theory and experience with their study area. These parameters can then be implemented in the ArcGIS tool’s Python code.

# IMPEDANCE FUNCTIONS
The accessibility toolbox implements the five different impedance functions from Kwan (1998):

$$
\begin{aligned}
  \text{Inverse Power: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} < 1 \\
          t_{ij}^{-\beta} & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Negative Exponential: } &f(t_{ij}) = e^{(-\beta t_{ij})} \\
  \text{Modified Gaussian: } &f(t_{ij})= e^{(-t_{ij}^2/\beta)} \\
  \text{Cumulative Opportunities Rectangular: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Cumulative Opportunities Linear: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          (1-t_{ij}/\bar{t}) & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.
\end{aligned}
$$

The inverse power, negative exponential, and modified Gaussian functions continuously discount the weight of opportunities as travel time increases using an impedance parameter $\beta$ that accounts for the cost of travel.

```{r travel time, echo=FALSE}
# first define the travel time increment, in this case from 0 to 60 minutes
t_ij <- data.frame(t_ij = seq(from = 0, to = 60, by=1))
```

With a foundation in early gravity models of spatial interaction (e.g. Stewart, 1948; Zipf, 1949), the inverse power function produces a rapid decline in the weight of opportunities as travel time increases. While power functions draw analogs to Newtonian physics, their theoretical relevance to human travel behavior has been questioned (Sen and Smith, 1995).

```{r inverse power function, echo=FALSE}
power_f <- function(t_ij,b0){case_when(t_ij < 1 ~ 1,
                                       TRUE ~ t_ij^-b0)}

POW <- list(POW0_8 = function(t_ij){power_f(t_ij, b0 = 0.8)},
            POW1_0 = function(t_ij){power_f(t_ij, b0 = 1.0)},
            POW1_5 = function(t_ij){power_f(t_ij, b0 = 1.5)},
            POW2_0 = function(t_ij){power_f(t_ij, b0 = 2.0)},
            POW_CUS = function(t_ij){power_f(t_ij, b0 = 0.5)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW0_8, aes(colour="POW0_8"), size=1) +
  stat_function(fun=POW$POW1_0, aes(colour="POW1_0"), size=1) +
  stat_function(fun=POW$POW1_5, aes(colour="POW1_5"), size=1) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=POW$POW_CUS, aes(colour="POW_CUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(POW)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Inverse Power Function")
```

The negative exponential function is more gradual and based on its strong theoretical foundations in entropy maximization (Wilson, 1971) and choice behavior theory (Fotheringham and O’Kelly, 1989), this function appears to have become somewhat of a de-facto standard in applied accessibility analysis.

```{r negative exponential function, echo=FALSE}
neg_exp_f = function(t_ij,b0){exp(-b0*t_ij)}

NEG_EXP <- list(EXP0_12 = function(t_ij){neg_exp_f(t_ij, b0 = 0.12)},
                EXP0_15 = function(t_ij){neg_exp_f(t_ij, b0 = 0.15)},
                EXP0_22 = function(t_ij){neg_exp_f(t_ij, b0 = 0.22)},
                EXP0_45 = function(t_ij){neg_exp_f(t_ij, b0 = 0.45)},
                EXP_CUS = function(t_ij){neg_exp_f(t_ij, b0 = 0.10)}, # custom - set your own parameter
                HN1997 = function(t_ij){neg_exp_f(t_ij, b0 = 0.1813)}) # from Handy and Niemeier (1997)

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=NEG_EXP$EXP0_12, aes(colour="EXP0_12"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_22, aes(colour="EXP0_22"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_45, aes(colour="EXP0_45"), size=1) +
  stat_function(fun=NEG_EXP$EXP_CUS, aes(colour="EXP_CUS"), size=1, linetype="dashed") +
  stat_function(fun=NEG_EXP$HN1997, aes(colour="HN1997"), size=1, linetype="longdash") +
  scale_color_discrete(limits = names(NEG_EXP)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Negative Exponential Function")
```

The modified Gaussian function exhibits a much more gradual rate of decline around its origin and a slower rate of decline overall. While Ingram (1971) argues these properties make the function superior to its inverse power and negative exponential counterparts for explaining observed travel behavior, it appears rarely used in the applied literature.

```{r modified gaussian function, echo=FALSE}
mgaus_f = function(t_ij,b0){exp(-t_ij^2/b0)}

MGAUS <- list(MGAUS10 = function(t_ij){mgaus_f(t_ij, b0 = 10)},
              MGAUS40 = function(t_ij){mgaus_f(t_ij, b0 = 40)},
              MGAUS100 = function(t_ij){mgaus_f(t_ij, b0 = 100)},
              MGAUS180 = function(t_ij){mgaus_f(t_ij, b0 = 180)},
              MGAUSCUS = function(t_ij){mgaus_f(t_ij, b0 = 360)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=MGAUS$MGAUS10, aes(colour="MGAUS10"), size=1) +
  stat_function(fun=MGAUS$MGAUS40, aes(colour="MGAUS40"), size=1) +
  stat_function(fun=MGAUS$MGAUS100, aes(colour="MGAUS100"), size=1) +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1) +
  stat_function(fun=MGAUS$MGAUSCUS, aes(colour="MGAUSCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(MGAUS)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Modified Gaussian Function")
```

The cumulative rectangular function is an isochronic measure that applies a constant weight to all opportunities reachable within some travel time window whose maximum is defined by $\bar{t}$. Although the application of a constant weight runs counter to the geographic principle of distance deterrence or decay that underpins travel behavior theory, such functions remain popular due to their ease of interpretation.

```{r cumulative rectangular function, echo=FALSE}
cumr_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1,
                                        TRUE ~ 0)}

CUMR <- list(CUMR10 = function(t_ij){cumr_f(t_ij, t_bar = 10)},
             CUMR20 = function(t_ij){cumr_f(t_ij, t_bar = 20)},
             CUMR30 = function(t_ij){cumr_f(t_ij, t_bar = 30)},
             CUMR40 = function(t_ij){cumr_f(t_ij, t_bar = 40)},
             CUMRCUS = function(t_ij){cumr_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUMR$CUMR10, aes(colour="CUMR10"), size=1) +
  stat_function(fun=CUMR$CUMR20, aes(colour="CUMR20"), size=1) +
  stat_function(fun=CUMR$CUMR30, aes(colour="CUMR30"), size=1) +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1) +
  stat_function(fun=CUMR$CUMRCUS, aes(colour="CUMRCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUMR)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Rectangular Function")
```

Finally, the cumulative linear function is a hybrid of the continuous and cumulative approaches, linearly discounting opportunities within an isochrone.

```{r cumulative linear function, echo=FALSE}
cuml_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1-t_ij/t_bar,
                                        TRUE ~ 0)}

CUML <- list(CUML10 = function(t_ij){cuml_f(t_ij, t_bar = 10)},
             CUML20 = function(t_ij){cuml_f(t_ij, t_bar = 20)},
             CUML30 = function(t_ij){cuml_f(t_ij, t_bar = 30)},
             CUML40 = function(t_ij){cuml_f(t_ij, t_bar = 40)},
             CUMLCUS = function(t_ij){cuml_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUML$CUML10, aes(colour="CUML10"), size=1) +
  stat_function(fun=CUML$CUML20, aes(colour="CUML20"), size=1) +
  stat_function(fun=CUML$CUML30, aes(colour="CUML30"), size=1) +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1) +
  stat_function(fun=CUML$CUMLCUS, aes(colour="CUMLCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUML)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Linear Function")
```

This set of impedance functions is by no means exhaustive. Numerous alternatives have been proposed, such as the exponential-normal, exponential-square root, and log-normal functions reviewed by Reggiani et al. (2011) and the Box-Cox, Tanner, and Richards functions reviewed by Martínez and Viegas (2013). Although these functions could be implemented in future iterations of the tool, the present paper’s focus on the functions specified in Kwan (1998) introduces some of the most widely used measures of impedance in applied accessibility analysis.

Kwan (1998) sets four impedance parameters for each continuous function designed to produce a weight of about 0.1 at travel times of 5, 10, 15, and 20 minutes respectively. Figure 1 recreates a figure from Kwan (1998) to visualize parameter values for the 5 functions: the inverse power function with $\beta = 2$ (POW2_0); the negative exponential function with $\beta = 0.15$ (EXP0_15); the modified Gaussian function with $\beta = 180$ (MGAUS180); and the cumulative rectangular (CUMR40) and linear (CUML40) functions with $\bar{t}$ set to 40 minutes.

```{r figure1, echo=FALSE, fig.cap="\\label{fig:figure1}Figure 1. Impedance Function Comparison"}
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1, linetype="dashed") +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1, linetype="dotdash") +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1, linetype="twodash") +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1, linetype="longdash") +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Figure 1. Impedance Function Comparison")
```

Calculating place-based accessibility using the `R5R` package below requires the creation of a network dataset using data from Open Street Map (OSM) and General Transit Feed Specification (GTFS) files; input origins and destinations (point or polygon); a numerical attribute representing destination opportunities, and the selection of one or more of the 28 impedance functions implemented in the tool to weight the attractiveness of the opportunities. In addition to Kwan’s (1998) impedance specifications, the toolbox also implements Handy and Niemeier's (1997) negative exponential specification calibrated to walking trips for convenience shopping in Oakland, CA in 1980 and several additional popular cumulative rectangular measures.

# DATA PROCESSING: MBTA
## Download and Prepare Census Data

The first step is to download the census data that will be used as indicators of opportunities, including census population and employment data at the block level using the `tigris` and `lodes` packages.

```{r download census employment data, include = FALSE, eval = FALSE}
# include all MA counties with year-round MBTA service
mbta_counties = c("005", "009", "017", "021", "023", "025", "027")

# download longitudinal origin-destination employment survey data
ma_lodes <- grab_lodes(state = "ma", 
                       year = 2020, 
                       lodes_type = "wac", 
                       job_type = "JT00",
                       segment = "S000", 
                       state_part = "main", 
                       agg_geo = "block") %>%
  rename(Block_GEOID = w_geocode, total_emp = C000) %>% 
  mutate(Block_GEOID = as.character(Block_GEOID)) %>%
  select(Block_GEOID, total_emp)
```

Next, download census block geographic boundaries and population data from the US Census, prepare standardized GEOIDs for joining, and join the population and employment data:

```{r download and prepare census boundaries, include = FALSE, eval = FALSE}
# download census block boundaries, including their populations
mbta_blkshp <- blocks("MA", mbta_counties, year=2020) %>% 
  st_transform(crs = 26986) %>% # project to NAD 1983 Massachusetts mainland
  mutate(Block_GEOID = GEOID20, total_pop = POP20, geometry=geometry, .keep='none')

# generate census blocks poly
mbta_cb_poly <- mbta_blkshp %>% 
  
  # join employment
  left_join(ma_lodes, by = "Block_GEOID") %>%
  
  # zero-out any NAs
  mutate(total_pop = ifelse(is.na(total_pop), 0, total_pop),
         total_emp = ifelse(is.na(total_emp), 0, total_emp))

# save census blocks poly
mbta_cb_poly %>% saveRDS(path(data_path, "mbta_cb_poly.rds"))
```

## Download Travel Network Data

With the census data prepared, the second step is to download the travel network data. This consists of an extract of Open Street Map (OSM) data from [geofabrik](https://download.geofabrik.de/):

```{r download osm, include=FALSE, eval = FALSE}
# Download OpenStreetMap data for the state of Massachusetts from Geofabrik
download.file(url = "https://download.geofabrik.de/north-america/us/massachusetts-latest.osm.pbf",
              destfile = file.path(r5_path, "osm.pbf"), mode = "wb")
```

And extract General Transit Feed Specification (GTFS) files from [Transitland](https://www.transit.land/documentation/rest-api):

```{r download gtfs, include = FALSE, eval = FALSE}
# transitland also has a feeds api that allows bounding box searches for feeds, but
# at first glance, it will also include a lot of stuff we care less about, like peter pan buses

mbta_req <- request("https://transit.land/api/v2/rest/feeds/f-drt-mbta/download_latest_feed_version") %>%
  req_headers(apikey = transitland_key)
mbta_resp <- req_perform(mbta_req)
mbta_resp %>%
  resp_body_raw() %>%
  brio::write_file_raw(path=file.path(r5_path, "mbta_recent.zip"))

# TODO: pending transitland correspondence, maybe replace them with Mobility Database
```

Or extract General Transit Feed Specification (GTFS) files from [Mobility Database](https://mobilitydatabase.org/feeds):

We can download other GTFS feeds to the same location to analyze additional services.

```{r extra gtfs packages, include = FALSE, eval = FALSE}
# for bus and commuter rail you can add these by un-commenting the code
#download.file(url = "http://web.mta.info/developers/data/lirr/google_transit.zip", 
#              destfile = file.path(r5_path, "lirr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/mnr/google_transit.zip", 
#              destfile = file.path(r5_path, "mnr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_bronx.zip", 
#              destfile = file.path(r5_path, "nyct_bus_bronx.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_brooklyn.zip", 
#              destfile = file.path(r5_path, "nyct_bus_brooklyn.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_manhattan.zip", 
#              destfile = file.path(r5_path, "nyct_bus_manhattan.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_queens.zip", 
#              destfile = file.path(r5_path, "nyct_bus_queens.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_staten_island.zip", 
#              destfile = file.path(r5_path, "nyct_bus_staten.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/busco/google_transit.zip", 
#              destfile = file.path(r5_path, "nyc_busco.zip"), mode = "wb")
```

## Pull MBTA GTFS metrics from LAMP

```{r}
run_duckdb <- function(query) {
  con <- dbConnect(duckdb()) # set up duckdb connection
  
  # load httpfs extension to query URL sources
  dbExecute(con, "INSTALL httpfs;")
  dbExecute(con, "LOAD httpfs;")
  
  # load spatial extension to use geographic functions
  dbExecute(con, "INSTALL spatial;")
  dbExecute(con, "LOAD spatial;")
  
  # Execute the query and fetch results
  result <- dbGetQuery(con, query)
  
  # Disconnect from DuckDB
  dbDisconnect(con, shutdown = TRUE)
  
  return(result)
}
```

```{r}
mbta_gtfs_metrics <- function(date_start, date_end, year) {
  gtfs_metrics_query <- paste0("
      -- First, assemble the list of all days between the specified start and end dates, using YYYYMMDD format
      with service_days as (
        select 
          strftime(generate_series, '%Y%m%d')::INTEGER as service_day
          , date_part('weekday', generate_series) as dow
        from generate_series(timestamp '", date_start, "', timestamp '", date_end, "', interval '1 day')
      ),
      
      -- Then, get all the IDs of the scheduled on each day, making sure to
      -- incorporate both typical and 'exception' service (e.g. holidays, diversions)
      services_scheduled as (
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar.parquet') AS c
          ON end_date >= service_day AND start_date <= service_day
          AND c.gtfs_end_date >= service_day AND c.gtfs_active_date <= service_day
          AND CASE
            WHEN dow = 0 and sunday = 1 then 1
            WHEN dow = 1 and monday = 1 then 1
            WHEN dow = 2 and tuesday = 1 then 1
            WHEN dow = 3 and wednesday = 1 then 1
            WHEN dow = 4 and thursday = 1 then 1
            WHEN dow = 5 and friday = 1 then 1
            WHEN dow = 6 and saturday = 1 then 1
            ELSE 0 END = 1
            
        -- exception_type = 2 are days when a given service was removed from the schedule
        ANTI JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON c.service_id = cd.service_id AND cd.exception_type = 2 and cd.date = service_day
          
        -- exception_type = 1 represents days when a given service was added to the schedule
        UNION 
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON cd.exception_type = 1 and cd.date = service_day
      ),
      
      -- get trip durations for all scheduled services on each day
      trip_info AS (
        SELECT 
              service_day
              , st.trip_id
              , t.route_id
              , t.shape_id
              , r.route_desc
              , min(arrival_time) as trip_start
              , max(arrival_time) as trip_end
              , 60*datepart('hour', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL)
                + datepart('minute', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL) as trip_duration_minutes
        FROM services_scheduled ss
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/trips.parquet') AS t
          ON ss.service_id = t.service_id AND t.gtfs_end_date >= service_day AND t.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/stop_times.parquet') AS st
          ON st.trip_id = t.trip_id AND st.gtfs_end_date >= service_day AND st.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/routes.parquet') AS r
          ON t.route_id = r.route_id AND r.gtfs_end_date >= service_day AND r.gtfs_active_date <= service_day
        GROUP BY 1,2,3,4,5
      ),
      
      -- deduplicate trip IDs with same start and end times on same routes and days
      trips_unique AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , min(shape_id) as shape_id
        FROM trip_info
        GROUP BY 1, 2, 3, 4, 5, 6
      ),
      
      prev_point_dists AS (
        SELECT
          shape_id
          , shape_pt_lat
          , shape_pt_lon
          , gtfs_active_date
          , gtfs_end_date
          , st_distance_sphere(
              st_point(shape_pt_lon, shape_pt_lat), 
              st_point(lag(shape_pt_lon) over prev, lag(shape_pt_lat) over prev)
            ) AS dist_from_prev
        FROM read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/shapes.parquet') 
        WINDOW prev AS (PARTITION BY shape_id ORDER BY shape_pt_sequence)
      ),
      
      trips_with_dists AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , sum(dist_from_prev) as dist_traveled_meters
        FROM trips_unique tu
        JOIN prev_point_dists ppd ON tu.shape_id = ppd.shape_id
          AND ppd.gtfs_end_date >= service_day AND ppd.gtfs_active_date <= service_day
        GROUP BY 1, 2, 3, 4, 5, 6
      )
  
      SELECT
        service_day
        , route_id
        , route_desc
        , count(*) as num_trips
        , sum(trip_duration_minutes) as total_runtime_minutes
        , sum(dist_traveled_meters)/1000 as total_runtime_km
        , min(trip_start) as first_trip_start
        , max(trip_end) as last_trip_end
      FROM trips_with_dists
      GROUP BY 1,2,3;
    ")
  
  return(run_duckdb(gtfs_metrics_query))
}
```

```{r}
mbta_gtfs_metrics_between <- function(date_start, date_end) {
  yyyymmdd_st <- as.integer(strftime(strptime(date_start, '%Y-%m-%d'), '%Y%m%d'))
  yyyymmdd_end <- as.integer(strftime(strptime(date_end, '%Y-%m-%d'), '%Y%m%d'))
  yyyy_st <- as.integer(yyyymmdd_st/10000)
  yyyy_end <- as.integer(yyyymmdd_end/10000)
  
  df <- data.frame()
  
  for (yyyy in yyyy_st:yyyy_end) {
    dt_st <- strftime(strptime(max(yyyy*10000+101, yyyymmdd_st), '%Y%m%d'), '%Y-%m-%d')
    dt_end <- strftime(strptime(min(yyyy*10000+1231, yyyymmdd_end), '%Y%m%d'), '%Y-%m-%d')
    
    df <- bind_rows(df, mbta_gtfs_metrics(dt_st, dt_end, yyyy))
  }

  return(df)
}
```

```{r}
date_start <- "2010-01-01"
date_end <- "2025-02-01"
gtfs_metrics <- mbta_gtfs_metrics_between(date_start, date_end)
```

```{r}
gtfs_metrics %>%
  write.csv(path(data_path, "mbta_gtfs_metrics_route.csv"), row.names=FALSE)
```

## Explore MBTA GTFS metrics

```{r}
gtfs_metrics <- read.csv(path(data_path, "mbta_gtfs_metrics_route.csv"))
```

```{r}
gtfs_metrics %>%
  filter(service_day %in% tuesdays, route_id == '86') %>%
  arrange(service_day) %>%
  head(15)
```

```{r}
yyyy <- 2023
gtfs_metrics %>%
  mutate(
    service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d')),
    route_category = factor(case_when(
      route_desc %in% c('Community Bus', 'Commuter Bus', 'Coverage Bus', 'Supplemental Bus', 'Local Bus') ~ 'Other Bus',
      route_desc %in% c('Frequent Bus', 'Key Bus') ~ 'Frequent Bus',
      route_desc %in% c('Commuter Rail', 'Regional Rail') ~ 'Regional Rail',
      .default = route_desc
      ), 
      levels=c('Rail Replacement Bus', 'Rapid Transit', 'Regional Rail', 'Ferry', 'Frequent Bus', 'Other Bus')
    )
    ) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) == yyyy) %>%
  group_by(route_category, service_dt) %>%
  summarise(across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), .groups='drop') %>%
  ggplot(aes(fill=route_category, y=total_runtime_minutes, x=as.character(service_dt))) + 
    geom_bar(position="stack", stat="identity") + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    ggtitle(paste0("Total Scheduled MBTA Runtime by Route Category (Tuesdays, ", as.character(yyyy), "-present)"))
```

Identify service days with suspiciously low total runtimes

```{r}
gtfs_metrics %>%
  mutate(service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) >= 2019) %>%
  group_by(service_dt) %>%
  summarise(total_runtime_minutes_day = sum(total_runtime_minutes)) %>%
  filter(total_runtime_minutes_day < 300000) %>%
  select(service_dt) %>% unique()
```

TODO: function summarizing route-level changes in service between two dates. Validation examples: 

https://www.reddit.com/r/mbta/comments/znq2l2/this_winter_the_mbta_has_cut_and_reduced_service/?rdt=59724 
https://www.reddit.com/r/mbta/comments/11n00vr/mbta_to_cut_spring_2023_weekday_service_on_the/

```{r}
year <- 2024
test_query <- paste0("
    WITH prev_point_dists AS (
      SELECT
        shape_id
        , shape_pt_lat
        , shape_pt_lon
        , gtfs_active_date
        , gtfs_end_date
        , st_distance_sphere(
            st_point(shape_pt_lon, shape_pt_lat), 
            st_point(lag(shape_pt_lon) over prev, lag(shape_pt_lat) over prev)
          ) AS dist_from_prev
      FROM read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/shapes.parquet') 
      WINDOW prev AS (PARTITION BY shape_id ORDER BY shape_pt_sequence)
    )
    
    SELECT
      shape_id
      , sum(dist_from_prev) AS dist_traveled
    FROM prev_point_dists
    WHERE gtfs_end_date >= 20241001 AND gtfs_active_date <= 20241001
    GROUP BY 1;
  ")
  
t <- run_duckdb(test_query)
```

## Create Feed Lookups & Pull MBTA GTFS full feeds

First, get the MBTA GTFS feed index and create data pulling functions based on it

```{r mbta gtfs}
mbta_feed_index <- read.csv('https://cdn.mbta.com/archive/archived_feeds.txt')

# only downloads the feed for a given date if that feed is not already present in data_path
lookup_mbta_feed_from_date <- function(yyyymmdd) {
  feed_url <- mbta_feed_index[mbta_feed_index$feed_start_date <= yyyymmdd 
                         & mbta_feed_index$feed_end_date >= yyyymmdd,'archive_url']
  fname <- sub(".*/", "", feed_url)
  
  if (!file.exists(file.path(data_path, fname))) {
    download.file(url = feed_url,
              destfile = file.path(data_path, fname), mode = "wb")
  }
  lookup <- list()
  lookup[[as.character(yyyymmdd)]] <- fname
  return(lookup)
}

feed_list <- function(dates) {
  feed_lookup <- list()
  
  for (dt in dates) {
    feed_lookup <- c(feed_lookup, lookup_mbta_feed_from_date(dt))
  }
  
  return(feed_lookup)
}
```

Option 1: get all dates for specific days of the week within a range

```{r}
date_start <- "2024-10-29"
date_end <- "2025-02-01"

# get list of dates for selected days of week before and after 12/15/2025
dtrange <- seq(as.Date(date_start), as.Date(date_end), by="days")

tuesdays <- dtrange[weekdays(dtrange) == 'Tuesday'] %>% format('%Y%m%d') %>% as.integer()
saturdays <- dtrange[weekdays(dtrange) == 'Saturday'] %>% format('%Y%m%d') %>% as.integer()

mbta_tues_lookup <- feed_list(tuesdays)
mbta_sat_lookup <- feed_list(saturdays)
```

Option 2: define list of specific dates to analyze

```{r}
mbta_changes <- list( # the comments define the effective date for each change
  'GLX Medford Opening' = c(20221206, 20221213), # 20221212
  'Winter 2023' = c(20221213, 20230103), # 20221218
  # 'Spring 2023' = c(20230228, 20230321), # 20230312
  # 'Summer 2023' = c(20230627, 20230711), # 20230702
  'Spring and Summer 2023' = c(20230228, 20230711),
  'Fall 2023' = c(20230822, 20230829), # 20230827
  'Spring 2024' = c(20240402, 20240409), # 20240407
  'Summer 2024' = c(20240611, 20240618), # 20240616
  'Fall 2024' = c(20240813, 20240827), # 20240825
  'Winter 2025 (BNR Phase 1)' = c(20241203, 20250107) # 20241215
)

mbta_analysis_dates <- unique(unlist(mbta_changes))
mbta_analysis_lookup <- feed_list(mbta_analysis_dates)
```

# Quantity-of-service calculations (before/after)

```{r}
# this function summarizes gtfs_metrics changes for a given element of the mbta_changes list
mbta_change_summary <- function(change_name, by_route=FALSE) {
  agg_cols <- 'route_category'
  if (by_route) agg_cols <- c('route_id', agg_cols)
  
  b4_day <- mbta_changes[[change_name]][1]
  aft_day <- mbta_changes[[change_name]][2]
  
  gtfs_metrics %>% 
    filter(service_day %in% c(b4_day, aft_day)) %>%
    mutate(
      route_category = case_when(
        route_desc %in% c('Community Bus', 'Commuter Bus', 'Coverage Bus', 
                          'Supplemental Bus', 'Local Bus') ~ 'Other Bus',
        route_desc %in% c('Frequent Bus', 'Key Bus') ~ 'Frequent Bus',
        route_desc %in% c('Commuter Rail', 'Regional Rail') ~ 'Regional Rail',
        .default = route_desc
        )
      ) %>%
    group_by(across(all_of(c('service_day', agg_cols)))) %>%
    summarise(
      across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), 
      .groups='drop'
      ) %>%
    pivot_wider(
      id_cols= all_of(agg_cols),
      values_from=c(num_trips, total_runtime_minutes, total_runtime_km),
      names_from=service_day,
      values_fill = 0
      ) %>%
    mutate(
      num_trips_b4 = get(paste0('num_trips_', as.character(b4_day))),
      runtime_min_b4 = get(paste0('total_runtime_minutes_', as.character(b4_day))),
      runtime_km_b4 = get(paste0('total_runtime_km_', as.character(b4_day))),
      chg_trips = get(paste0('num_trips_', as.character(aft_day))) - num_trips_b4,
      chg_min = get(paste0('total_runtime_minutes_', as.character(aft_day))) - runtime_min_b4,
      chg_km = round(
        get(paste0('total_runtime_km_', as.character(aft_day))) - runtime_km_b4, 0),
      pct_chg_trips = round(100*chg_trips / num_trips_b4, 2),
      pct_chg_min = round(100*chg_min / runtime_min_b4, 2),
      pct_chg_km = round(100*chg_km / runtime_km_b4, 2)
    ) %>%
    #filter(chg_trips != 0 | chg_min != 0 | chg_km != 0) %>%
    select(
      all_of(c(agg_cols, 'chg_trips', 'chg_min', 'chg_km', 
               'pct_chg_trips', 'pct_chg_min', 'pct_chg_km'))
    )
}
```

```{r}
mbta_change_summaries_highlevel <- names(mbta_changes) %>%
  lapply(mbta_change_summary) %>%
  `names<-`(names(mbta_changes))

mbta_change_summaries_routelevel <- names(mbta_changes) %>%
  lapply(function(n) mbta_change_summary(n, by_route=T)) %>%
  `names<-`(names(mbta_changes))
```

```{r}
# # this confirms that the only non-temporary service change meeting the mode threshold was BNR Phase 1,
# # and that's if you count Frequent Bus as a separate mode
# mbta_change_summaries_highlevel %>%
#   lapply(function(df) df %>% filter(abs(pct_chg_min) > 10))

# this shows that both GLX medford and BNR Phase 1 met the route-level major change threshold, but so did:
# - route 556 with no change in trips or distance in Winter 2025
# - blue gaining min and orange losing min in winter 2023 beyond the threshold, seemingly unannounced
# - though orange regained a similar amount of minutes in spring 2023
# - 34 and 441/442 both lost more than 25% of RVH in summer 2023 cuts
# - 714 gained >25% of min in spring 2024
# - 4 gained >25% of min in fall 2024
# the 34 and 441/442 changes are the ones I trust the most, but the other ones are worth an audit
mbta_change_summaries_routelevel %>%
  lapply(function(df) df %>% filter(abs(pct_chg_min) > 25))
```

I should have enough now to give color / interpretation of the quantity-of-service changes at least for full days. This will be a bit different from the morning peak hour changes. The only way to really filter out any "noise" would be to rerun gtfs_metrics to only pull statistics for the morning hours, but maybe the full-day statistics will be OK. 

The other piece is to identify which pieces of these summaries I will actually publish. I imagine a table with each row as a change and a selection of columns for both the service changes and the access changes, and then additional color / detail in the text. Or maybe with 8 changes, I just do those as columns and allow myself to do (rapid transit, frequent bus, and other bus) * (trips, min, km). Then for access, I can tack on rows with the popmean change in p50 axs by race, along with the total popmean change for p15, p50, p85 axs

```{r}
extract_table_info <- function(change_name) {
  mbta_change_summaries_highlevel[[change_name]] %>% 
    filter(route_category %in% c('Rapid Transit', 'Frequent Bus', 'Other Bus')) %>%
    mutate(chg = change_name)
}

service_table_info <- names(mbta_change_summaries_highlevel) %>%
  lapply(extract_table_info) %>%
  bind_rows() %>%
  pivot_longer(cols=c(#'chg_trips', 'chg_min', 
                      'pct_chg_trips', 'pct_chg_min'),
               names_to='statistic') %>%
  pivot_wider(id_cols=c('route_category', 'statistic'), names_from='chg', values_from='value') %>%
  arrange(factor(route_category, levels=c('Rapid Transit', 'Frequent Bus', 'Other Bus')), desc(statistic))
```


# R5R Calculations

## Define Parameters and Prepare Data

Using this set of variable definitions, users can set the key parameters that define how the origin-destination matrix and accessibilities are calculated. This includes supplying the input origins and destinations (as `sf` objects), defining relevant ID and destination opportunities fields, and selecting one or more impedance functions, travel modes, a date and time of departure (within the calendar dates of the supplied GTFS files), and maximum walking distance and trip duration.

In addition, a `chunk_size` parameter is used to control batching as the base `travel_time_matrix` function from `R5R` is employed here within a batching work flow built around `disk.frame`. In this formulation, the matrix is calculated for batches of origins to all destinations reachable within the travel time window for the selected modes. While `r5r`` works fastest with a single set of origins, this batching is implemented to save memory and enable the calculation of very large matrices. In addition, the resulting matrix is compressed and saved to disk for future use. Depending on your particular hardware configuration, you can change the number of origins considered in each batch.

```{r}
# load in pre-processed census data:
mbta_cb_poly <- readRDS(file.path(data_path, "mbta_cb_poly.rds"))
```

```{r define parameters, warning = FALSE}
# 1. Generalize calls to input simple features
# origins
origins_i <-  mbta_cb_poly
origins_id_field <- "Block_GEOID"

# destinations
destinations_j <-  mbta_cb_poly
destinations_id_field <- "Block_GEOID"
opportunities_j_field <- "total_emp"

# 2. R5 Travel Time Matrix Options. See https://rdrr.io/cran/r5r/man/travel_time_matrix.html for more detail
# R5 allows for multiple combinations of transport modes. The options include:
## Transit modes
# TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR. 
# The option 'TRANSIT' automatically considers all public transport modes available.

## Non transit modes
# WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK

# define your travel modes
mode <- c("WALK", "TRANSIT")

# egress mode
#mode_egress = "WALK" # Transport mode used after egress from public transport; it can be either 'WALK', 'BICYCLE', or 'CAR'. Defaults to "WALK"

# walk speed
#walk_speed = 3.6 #Average walk speed in km/h. Defaults to 3.6 km/h

# bike speed
#bike_speed = 12 # Average cycling speed in km/h. Defaults to 12 km/h
  
# max rides
#max_rides = 3 # The max number of public transport rides allowed in the same trip. Defaults to 3

# level of traffic stress (cycling)
#max_lts = 2 # The maximum level of traffic stress that cyclists will tolerate. A value of 1 means cyclists will only travel through the quietest streets, while a value of 4 indicates cyclists can travel through any road; defaults to 2

# max trip duration (minutes)
max_trip_duration = 60

# set the time of day for the departure (date will be set later)
departure_time_hms <- "07:30:00"

# # of minutes starting from departure_time_hms within which to consider trip starts
time_window_minutes <- 30 

# travel time percentiles of the above time window (e.g., 85% of trips took X min or less)
time_window_percentiles <- c(15, 30, 50, 70, 85) 

# processing batch size - how many origin rows to process at one time
chunksize = 500 # how many origins to consider at one time; set to nrow(origins_sf) if you don't want to use

# multithreading
n_threads = Inf # The number of threads to use in parallel computing; defaults to use all available threads (Inf)
```

```{r prepare input data for r5r, warning = FALSE, include = FALSE}
# ORIGINS
# create input sf object and change to crs 4326 for origins
origins_i_sf <- origins_i %>% 
  select(all_of(origins_id_field)) %>% 
  rename(id = all_of(origins_id_field))

origins_i <- origins_i_sf %>% 
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) %>% # transform to lat-longs
  mutate(batch_id = ceiling(row_number()/chunksize))

# DESTINATIONS
# create input sf object and change to crs 4326 for destinations
destinations_j <- destinations_j %>% 
  select(all_of(c(destinations_id_field, opportunities_j_field))) %>% 
  rename(id = all_of(destinations_id_field), o_j = all_of(opportunities_j_field)) %>%
  filter(o_j > 0) %>%
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) # transform to lat-longs

opportunities_j <- destinations_j %>%
  st_drop_geometry() %>% 
  select(id, o_j) %>% 
  rename(toId = id)
```

## Calculate Origin-Destination Travel Time Matrices

Third, an origin-destination matrix is calculated for each service date in accordance with the travel and batching parameters set earlier. In this case, I have set the ODCM to save to disk as a parquet dataset, which is a great format for compressed storage and for larger-than-memory calculations. This process took about 35 minutes per date for MBTA blocks (n=~80k).

```{r}
lookup <- mbta_analysis_lookup
# loop through lookup and get feed_day (name) and feed_curr (value)
# name the ttm after the feed day, since different days on the same feed could (?) be different
for (i in 1:length(lookup)) {
  feed_day <- names(lookup)[[i]]
  feed_curr <- lookup[[i]]
  
  # move the current feed into the r5 path from the data path
  file_move(path(data_path, feed_curr), path(r5_path, feed_curr))
  
  # create the network graph using the new GTFS feed
  r5r_core <- setup_r5(data_path = r5_path, verbose = FALSE, overwrite=TRUE)
  
  ttm_path_curr <- dir_create(paste0("./r5_ttm/", feed_day))
  
  # define trip start datetime
  departure_datetime <- as.POSIXct(
    paste(strftime(strptime(feed_day, '%Y%m%d'), '%Y-%m-%d'), departure_time_hms), 
    format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York") 
  # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for list of time zone codes
  
  # set up batching 
  num_chunks = origins_i %>% st_drop_geometry() %>% summarize(num_chunks = max(batch_id)) %>% pull(num_chunks)
  
  # compute travel time matrix
  start.time <- Sys.time()
  pb <- txtProgressBar(0, num_chunks, style = 3)
  
  for (i in 1:num_chunks){
    origins_i_chunk <- origins_i %>% filter(batch_id == i)
    
    ttm_chunk <- travel_time_matrix(
      r5r_core = r5r_core,
      origins = origins_i_chunk,
      destinations = destinations_j,
      mode = mode,
      departure_datetime = departure_datetime,
      max_trip_duration = max_trip_duration,
      verbose = FALSE,
      progress = FALSE,
      time_window = time_window_minutes, 
      percentiles = time_window_percentiles
      ) %>%
      
    mutate(batch_id = i)
    
    # export output as parquet
    write_dataset(ttm_chunk, path = ttm_path_curr, partitioning = "batch_id")
    
    setTxtProgressBar(pb, i)}
  
  end.time <- Sys.time()
  print(paste0(
    "OD matrix calculation took ", 
    round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."
    ))
  
  # to wrap up, we move the current feed out of the r5 path...
  file_move(path(r5_path, feed_curr), path(data_path, feed_curr))
  
  # ...and terminate the r5r instance to clear memory
  r5r::stop_r5(r5r_core)
  rJava::.jgc(R.gc = TRUE)
}
```

## Calculate Accessibility

Finally, accessibilities are calculated using the selected impedance function(s) by iterating over all the batches of the origin-destination matrix per service date. There's a few ways of doing this.

TODO: try generating some of the accessibility outputs shown here: https://cran.r-project.org/web/packages/r5r/vignettes/accessibility.html

TODO: parameterize ttm_path
TODO: decide on data storage for access datasets
maybe /outputs -> (optional layers for agency, metric type, etc) -> csv per feed day?

The visualization I think I want next is a line chart with shaded areas representing the high and low percentage bounds of accessibility and dates on the x axis. To do that, I need to calculate accessibility from each ttm and then aggregate that, starting with just the population average or median but likely expanding to a bunch of different metrics at some point (grabbing a set of accessibility percentiles - distinct from within-window percentiles, grouping by population, calculating gini / palma / concentration indices for inequality, etc). There's also the calculation of before/after changes in access per block, but that kind of visualization will be limited to just examples since we're interested in lots of little changes. 

How can I measure if a schedule change makes the time-of-day percentiles less variable? Because what if there are changes for some people, but not for the median person? A given block could see higher accessibility levels but the same, smaller, or greater range of time-of-day accessibility values, either in absolute terms or relative to its level. That may be another example map worth creating, but for this study again I'll need to abstract or aggregate that a bit more. Probably at first just by calculating the population average for each time-of-day percentile and then comparing the spreads over time. 

I'm realizing that there is a large rabbit hole of possible metrics here, even in this, my initial test run. 

```{r}
lud <- mbta_cb_poly %>%
  rename(id = all_of(origins_id_field))

time_window_pctile_suffixes <- '_p' %>%
  paste0(time_window_percentiles %>% as.character())
```

```{r}
read_ttm_for_day <- function(feed_day) {
  open_dataset(path(paste0(ttm_path, '/', as.character(feed_day)))) %>%
    collect()
}

axs_at_pctile <- function(ttm, feed_day, pctile_suffix) {
  ttm_pctile <- ttm %>%
    select(all_of(
      c('from_id', 'to_id', paste0('travel_time', pctile_suffix))
      )) 
  
  print(paste("Calculating destination access for", pctile_suffix))
  axs_pctile <- ttm_pctile %>%
    cumulative_cutoff(
        land_use_data = lud, opportunity = 'total_emp',
        travel_cost = paste0('travel_time', pctile_suffix), 
        cutoff=max_trip_duration
      ) 
  
  axs_pctile %>%
    rename_with(~paste0('total_emp', pctile_suffix), total_emp) %>%
    mutate(service_day = feed_day)
}

axs_window_percentiles <- function(ttm, feed_day) {
   time_window_pctile_suffixes %>%
    # then calculate the access at each time window percentile for the given ttm
    lapply(function(p) axs_at_pctile(ttm, feed_day, p)) %>%
    reduce(inner_join, by=c('service_day', 'id')) # and join the results
}
```

Read the travel time matrices for a set of dates and calculate accessibility per percentile. This cell took ~2min per date for MBTA blocks (n=~80k).  

```{r}
for (feed_day in names(mbta_analysis_lookup)) {
  print(paste("Reading travel time matrix for", as.character(feed_day)))
  ttm <- read_ttm_for_day(feed_day)
  
  print(paste("Calculating destination access for", as.character(feed_day)))
  axs_window_percentiles(ttm, feed_day) %>%
    write.csv(
      path(outputs_path, paste0(as.character(feed_day), ".csv")), 
      row.names=F
    )
}
```

## Summarize Accessibility

Read in previously calculated accessibility dataframes

```{r}
axs_dfs <- names(mbta_analysis_lookup) %>%
  lapply(function(fd) read.csv(path(outputs_path, paste0(as.character(fd), ".csv")))) %>%
  `names<-`(names(mbta_analysis_lookup))
```

Get race/ethn data and join that in too

```{r}
ma_blocks_raceethn <- read.csv(paste0(data_path, '/raceethn_2020_block_MA.csv'))
```

```{r}
block_raceethn <- ma_blocks_raceethn %>%
  mutate(block_id = as.numeric(substr(GEOID, 10, length(GEOID)))) %>%
  select(block_id, nh_white, nh_black, nh_aapi, nh_others, hisp_all)
```

```{r}
block_pops <- mbta_cb_poly %>%
  st_drop_geometry() %>%
  rename(id = all_of(origins_id_field)) %>%
  mutate(id = as.numeric(id)) %>%
  select(id, total_pop) %>%
  inner_join(block_raceethn, by=join_by(id == block_id))
```

Join access data to block populations

```{r}
axs_all <- axs_dfs %>%
  bind_rows() %>%
  inner_join(block_pops, by='id')
```

### Compare distributions of access changes

As a prerequisite, we create dataframes of the before-and-after for each change

```{r}
# for each mbta_change to consider, create a before and after and then stack?
# given that a single service date can be part of multiple calcs, that be the best way
chg_axs <- function(chg_name) {
  axs_all %>%
    filter(service_day %in% mbta_changes[[chg_name]]) %>%
    mutate(period = case_when(
      service_day == mbta_changes[[chg_name]][2] ~ 'after',
      .default='before')) %>%
    pivot_wider(
      id_cols=c('id', 'total_pop', 'nh_white', 'nh_black', 'nh_aapi', 'nh_others', 'hisp_all'), 
      names_from='period', 
      values_from='total_emp_p50') %>%
    mutate(change_name = chg_name, delta_p50 = after - before,
           access_bin_before = before - (before %% 10000),
          access_bin_after = after - (after %% 10000)) %>%
    arrange(before) %>%
    mutate(pop_pctile_before = lag(cumsum(total_pop), default = 0)/(sum(total_pop) - 1)) %>%
    arrange(after) %>%
    mutate(pop_pctile_after = lag(cumsum(total_pop), default = 0)/(sum(total_pop) - 1)) %>%
    mutate(delta_pop_pctile = pop_pctile_after - pop_pctile_before)
}

mbta_axs_changes <- names(mbta_changes) %>%
  lapply(chg_axs) %>%
  bind_rows()
```

TODO: create separate chart or option for comparing change distributions by TOD ptile
TODO: look into statistics that would capture different kinds of distributions, which I could report alongside the total number of affected population

The viz below compares the distributions of changes across scenarios. It shows that most of the distributions are quite similar, with all but GLX Medford Opening having a mix of winners and losers with tails leaning in the direction of the service changes.  I'm not sure if any of these will make it into the final paper, given that the untransformed-axis charts make it hard to compare across adjustments, while the transformed-axis charts imply a bimodal distribution that isn't really there. 

```{r}
# the first element of this will be plotted first (back), then the next overlaid in front of it, etc 
adjustments_to_plot <- c('Winter 2025 (BNR Phase 1)', 'Summer 2023', 'GLX Medford Opening')
#pseudolog10_breaks <- c(-1*(10^seq(1,5, by=1)), 10^seq(1,5, by=1))

mbta_axs_changes %>%
  filter(
    delta_p50 != 0, # only consider the affected population for each adjustment
    change_name %in% adjustments_to_plot) %>%
  mutate(change_name = factor(change_name, levels=adjustments_to_plot)) %>%
  ggplot(aes(x=delta_p50)) +
  geom_vline(xintercept = 0, linetype='longdash') +
  scale_x_continuous(label=comma, limits=c(-50000,50000)
                     #, transform=pseudolog10_trans, breaks=pseudolog10_breaks
                     ) +
  
  # OPTION 1: access changes per population (non-normalized - compare overall sizes)
  geom_area(
    aes(weight=total_pop, fill=change_name),
    position='identity', alpha=0.5,
    stat = "bin", binwidth=1000) +
  geom_freqpoly(
    aes(weight=total_pop, group=change_name),
    alpha=0.5, stat = "bin", binwidth=1000) +
  labs(x='Change in Jobs Accessible', y='Affected Population', fill='Adjustment Name',
       title='Job Access Change Distributions (Absolute) per Service Change') +
  scale_y_continuous(label=comma)

  # # OPTION 2: access changes per population portion (normalized, compare concentrations)
  # geom_density(aes(weight=total_pop, fill=change_name, alpha=0.5)) +
  # labs(x='Change in Jobs Accessible', y='Portion of Affected Population per Adjustment', fill='Adjustment Name',
  #      title='Job Access Change Distributions (Proportional) per Service Change')
```

The viz below shows how much a given adjustment adds or subtracts access (on net) from communities with lower or higher pre-existing access levels. The Summer 2023 schedule decreased access on net for medium- and high-access communities while adding access for some previously low-access communities. Meanwhile, the GLX Medford opening added access in mostly medium-to-high access communities, while BNR Phase 1 netted large access gains across the board, but especially in previously low-access communities. This viz masks the variation within before-access bins, since we know from the previous chart that Winter 2025 changes had some losers, it's just that everyone's losses are netted out by the gains of other communities with similar pre-existing access levels.

```{r}
mbta_axs_changes %>%
  filter(change_name %in% adjustments_to_plot) %>%
  mutate(#pop_ptile_b4_bin = round(pop_pctile_before*100),
         change_name = factor(change_name, levels=adjustments_to_plot)) %>%
  group_by(change_name, access_bin_before) %>%
  summarise(chg_personjob_access = sum(total_pop * delta_p50), .groups='drop') %>%
  ggplot() + 
  geom_col(aes(x=access_bin_before, y=chg_personjob_access, fill=as.factor(change_name)),
           alpha=.5, position='identity') +
  scale_x_continuous(label=comma) + 
  scale_y_continuous(label=comma) +
  labs(x='Pre-Adjustment Jobs Accessible', 
       y='Aggregate Population-Weighted Change in Jobs Accessible', 
       fill='Adjustment Name',
       title='Changes in Population-Weighted Jobs Accessible by Pre-Adjustment Access Level')
```

Substituting pop_ptile_b4_bin for access_bin shows that >50% of the population of the 7 MA counties in my analysis have access to <10k jobs via MBTA+walk, and see barely any changes because those jobs are probably all walk and they're far away from MBTA service. I could conceivably do a post-hoc filter for blocks within a mile or two of MBTA stops, and that would be a better method for comparability across metros, but whatever filter I apply would both not fully solve that chart problem and be somewhat arbitrary.

I also tried graphing the changes in population per access bin for a given change ("an additional x people joined the cohort of people with access to y jobs or more") but it ends up looking like radio spaghetti due to populations moving from one group to another showing up as losses / gains in adjacent bins. I'm saving it as optional code in case some derivative of this becomes interesting, but I doubt it.

```{r}
# pop_per_bin_b4 <- mbta_axs_changes %>%
#   filter(change_name %in% adjustments_to_plot) %>%
#   group_by(change_name, access_bin_before) %>%
#   summarise(total_pop = sum(total_pop), .groups='drop')
# 
# pop_per_bin_aft <- mbta_axs_changes %>%
#   filter(change_name %in% adjustments_to_plot) %>%
#   group_by(change_name, access_bin_after) %>%
#   summarise(total_pop = sum(total_pop), .groups='drop')
# 
# pop_per_bin_b4 %>%
#   full_join(pop_per_bin_aft, by=join_by(
#     change_name == change_name, access_bin_before == access_bin_after
#     ), suffix=c('_b4', '_aft')) %>%
#   mutate(delta_pop = total_pop_aft - total_pop_b4) %>%
#   ggplot() + 
#   geom_col(aes(x=access_bin_before, y=delta_pop, fill=as.factor(change_name)),
#            alpha=.5, position='identity') +
#   scale_x_continuous(label=comma) + 
#   scale_y_continuous(label=comma) +
#   labs(x='Jobs Accessible', 
#        y='Change in Population', 
#        fill='Adjustment Name',
#        title='Changes in Population by Access Level')
```

### Demographic breakdown visualizations

average change in job access per demographic group by adjustment. can graph before or after instead to get differences in average existing access

```{r}
mbta_axs_changes %>%
  mutate(change_name = factor(change_name, levels=names(mbta_changes))) %>%
  pivot_longer(
    cols=c(nh_white, nh_black, nh_aapi, nh_others, hisp_all),
    names_to = 'race', values_to = 'pop'
  ) %>% 
  filter(pop != 0, delta_p50 != 0, after > 100000) %>%
  # pivot_longer(
  #   cols=c(before, after),
  #   names_to = 'stage', values_to = 'access'
  # ) %>% 
  group_by(change_name, race) %>%
  summarise(
    mean_axs = wtd.mean(delta_p50, weights=pop),
    stderr_axs = sqrt(wtd.var(delta_p50, weights=pop))/sqrt(sum(pop)),
    .gruops='drop'
    ) %>%
  ggplot(aes(fill=race, x=race, y=mean_axs)) + 
  geom_bar(position='dodge', stat='identity') +
  #geom_bar(position=position_dodge(width=0.9)) +
  geom_errorbar(aes(ymax=mean_axs + stderr_axs, ymin = mean_axs - stderr_axs), position=position_dodge(width=0.9), width=0.25) + 
  facet_wrap(vars(change_name))
```

### Statistical tests for race & ethnicity

```{r}
named_group_split <- function(.tbl, ...) {
  grouped <- group_by(.tbl, ...)
  names <- rlang::inject(paste(!!!group_keys(grouped), sep = " / "))

  grouped %>% 
    group_split() %>% 
    rlang::set_names(names)
}

axs_chgs_raceethn_aovs <- mbta_axs_changes %>%
  pivot_longer(
    cols=c(nh_white, nh_black, nh_aapi, nh_others, hisp_all),
    names_to = 'race', values_to = 'pop'
  ) %>% 
  filter(pop != 0, delta_p50 != 0) %>%
  named_group_split(change_name) %>%
  map(function(df) {aov(delta_p50 ~ race, data=df, weights=df$pop)})
```

```{r}
axs_chgs_raceethn_aovs %>%
  map(summary)
```

Tukey post-hoc tests - many significant differences between groups in the mean deltas, but relatively small ones, at least at first glance, relative to the mean befores

```{r}
axs_chgs_raceethn_aovs %>%
  map(TukeyHSD)
```

### Maps & Spatial Analysis

TODO: plot delta_p50==0 separately with just outlines
TODO: map of snapshot access
TODO: map of change in 15-to-85 range

These tmap maps are useful for me as an exploration of various mappable metrics, but I'd rather just export to QGIS to make polished maps for publication rather than try to do it all here.

```{r}
sf_delta_axs <- origins_i_sf %>%
  mutate(id = as.numeric(id)) %>%
  inner_join(mbta_axs_changes, by='id') %>%
  mutate(change_name=factor(change_name, levels=names(mbta_changes))) %>%
  group_by(id) %>%
  filter(sum(abs(delta_p50)) > 0)

tm_shape(sf_delta_axs) +
  tm_polygons(fill = 'delta_p50', 
          fill.scale = tm_scale_continuous(values='brewer.rd_yl_gn'),
          fill.legend = tm_legend("Change in Jobs Accessible", reverse=T),
          col_alpha = 0) +
  tm_facets("change_name") +
  tm_layout(frame = FALSE,
            bg.color = "grey85",
            legend.outside=T,
            legend.outside.position=c("right", "bottom")
            )
```

TODO: decide on adding spatial autocorrelation or other statistics to aggregate_axs()

### Summary statistics across adjustments

TODO: add summary statistics of social and geographic equity
- then, group befores, afters, and changes by race/ethn and do stderr / anova / tukey
- do mean & stderr charts by race/ethn
- then add gini & palma to aggregate_axs()


```{r}
aggregate_axs <- function(df) {
  df %>% 
    inner_join(block_pops, by='id') %>%
    group_by(service_day) %>%
    summarise(
      across(total_emp_p15:total_emp_p85, ~ weighted.mean(.x, total_pop),
             .names='{.col}_popmean'),
      # across(total_emp_p15:total_emp_p85, ~ quantile(.x, probs=0.8), 
      #        .names='{.col}_blockp80'),
      
      # # this one is interesting, but at lower percentiles says more about the non-MBTA areas than anything
      # across(total_emp_p15:total_emp_p85, ~ wtd.quantile(.x, weights=total_pop, probs=0.8), 
      #      .names='{.col}_popp80'),
      
      across(nh_white:hisp_all, ~ weighted.mean(total_emp_p50, .x), 
           .names='{.col}_popmean'),
      .groups='drop'
      )
}
```

```{r}
axs_agg <- axs_dfs %>%
  lapply(aggregate_axs) %>%
  bind_rows()
```

TODO: add additional lines and ribbons for high, medium, and low-access population percentiles (or whatever categories make sense)
TODO: figure out and toggle between central-tendency lines and actual-observation lines for particular percentiles / statistics

```{r}
metric <- 'popmean'

axs_agg %>%
  mutate(service_date = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  ggplot(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_line(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_point(aes(x=service_date, y=get(paste0('total_emp_p50_', metric)))) +
  geom_ribbon(
    aes(x=service_date, y=get(paste0('total_emp_p50_', metric)), 
        ymax=get(paste0('total_emp_p30_', metric)), 
        ymin=get(paste0('total_emp_p70_', metric))
        ), alpha=0.2) +
  scale_y_continuous(labels=comma) +
  labs(
    title="Job Accessibility via Transit for Average MBTA-Area Resident",
    y = "Jobs Accessible via Transit & Walking within 60 Minutes", x="Tuesdays"
  )
  
```

The reason 20230627 is so much lower than 20230321 seems to be the Red Line shuttling from Braintree to South Station that occurred on the former date, which was lifted for 20230711 but simultaneously replaced by a bunch of bus frequency cuts.



```{r figure2, fig.height=3.5, fig.width=12.5, fig.cap="\\label{fig:figure2}Figure 2. Transit Employment Accessibility Comparison", message=FALSE, warning=FALSE, echo=FALSE}
# join results to input_sf object
input_sf_accessibility <- left_join(origins_i_sf, accessibility, by=c("id" = "fromId"))

# iterate through selected functions to create maps
accessibility_maps <- list()

for (i in selected_f){
  map <- tm_shape(input_sf_accessibility) +
    tm_fill(col = i, 
            title = i, 
            style = "cont", 
            palette = "viridis") + 
    tm_layout(frame = FALSE,
              bg.color = "grey85",
              legend.position = c("left", "top"),
              legend.text.color = "white",
              legend.title.color = "white")
  
  accessibility_maps[[i]] <- map}

names(accessibility_maps) <- selected_f

# plot the maps
tmap_arrange(accessibility_maps)
```

Similar to Kwan (1998) and Vale and Pereira (2017), correlations in accessibility across measures are generally strong (Figure 3), indicating many capture similar spatial processes. Of those used in Figures 1 and 2 for example, results from the negative exponential (`EXP0_15`), modified Gaussian (`MGAUS180`), cumulative rectangular (`CUMR40`), and cumulative linear (`CUML40`) measures of impedance all show correlation coefficients of at least 0.75. In particular, the correlation between the `EXP0_15` and `MGAUS180` measures is 0.99. Results from the inverse power (`POW2_0`) measure are more unique, with correlations ranging from 0.72 to 0.85. It should be emphasized that such outcomes are not a product of similar functional forms alone; rather, the correlations reflect an interaction between the different impedance measures and the spatial distribution of opportunities on the travel network in the study area. Furthermore, absolute accessibility totals differ across each, suggesting the choice of a suitable impedance function and specification remains an important issue that should be guided by theory and assumptions about travel behavior.

```{r figure3, fig.height=10, fig.width=10, fig.cap="\\label{fig:figure3}Figure 3. Impedance Measure Correlations", warning = FALSE, echo=FALSE}
# create dataframe of variables for correlation analysis
correlations <- input_sf_accessibility %>% select(all_of(selected_f)) %>% st_drop_geometry() %>% drop_na() %>% cor(.)

corrplot(correlations,
         method = "color", # corrplot method
         type = "upper", # upper triangle matrix
         addCoef.col = "white", # correlation text colour
         number.cex = 0.8, # correlation text size
         tl.col = "black", # text label colour
         tl.srt = 45, # text label angle
         tl.cex = 0.9, # text label size
         cl.lim = c(min(correlations), 1), # colour label limits
         #diag = FALSE, # turn off diagonal
         is.corr=FALSE, # because all coefficients are positive
         col = viridis::viridis(100)) # custom colour scheme based on viridis hex values
```

While the focus on walking and transit trips in this sample analysis does not provide a full picture of travel behavior in the study area, the `r5r` package enables users to run multiple analyses for different travel modes. Moreover, the R notebook can be utilized to select or customize the implemented impedance measures in accordance with expectations about travel behavior for each mode. Taken together, this toolbox enables researchers and practitioners to make better decisions about the specification and customization of travel impedance and simplify the calculation of place-based accessibility for their study context.

# REFERENCES

Fotheringham, A. S., & O’Kelly, M. E. (1989). *Spatial interaction models: Formulations and applications*. Boston: Kluwer Academic.

Geurs, K. T., & Van Wee, B. (2004). Accessibility evaluation of land-use and transport strategies: review and research directions. *Journal of Transport Geography*, 12(2), 127-140. https://doi.org/10.1016/j.jtrangeo.2003.10.005

Handy, S. L., & Niemeier, D. A. (1997). Measuring accessibility: An exploration of issues and alternatives. *Environment and Planning A*, 29(7), 1175-1194. https://doi.org/10.1068%2Fa291175

Hansen, W. G. (1959). How accessibility shapes land use. *Journal of the American Institute of Planners*, 25(2), 73-76. https://doi.org/10.1080/01944365908978307

Ingram, D. R. (1971). The concept of accessibility: A search for an operational form. *Regional Studies*, 5(2), 101-107. https://doi.org/10.1080/09595237100185131

Kwan, M. P. (1998). Space‐time and integral measures of individual accessibility: A comparative analysis using a point‐based framework. *Geographical Analysis*, 30(3), 191-216. https://doi.org/10.1111/j.1538-4632.1998.tb00396.x

Martínez, L. M., & Viegas, J. M. (2013). A new approach to modelling distance-decay functions for accessibility assessment in transport studies. *Journal of Transport Geography*, 26, 87-96. https://doi.org/10.1016/j.jtrangeo.2012.08.018

Páez, A., Scott, D. M., & Morency, C. (2012). Measuring accessibility: Positive and normative implementations of various accessibility indicators. *Journal of Transport Geography*, 25, 141-153. https://doi.org/10.1016/j.jtrangeo.2012.03.016

Reggiani, A., Bucci, P., & Russo, G. (2011). Accessibility and impedance forms: empirical applications to the German commuting network. *International Regional Science Review*, 34(2), 230-252. https://doi.org/10.1177/0160017610387296

Sen, A., & Smith, T. E. (1995). *Gravity models of spatial interaction behavior*. Berlin: Springer-Verlag.

Stewart, J. Q. (1948). Demographic gravitation: evidence and applications. *Sociometry*, 11(1/2), 31-58. https://doi.org/10.2307/2785468

Vale, D. S., & Pereira, M. (2017). The influence of the impedance function on gravity-based pedestrian accessibility measures: A comparative analysis. *Environment and Planning B: Urban Analytics and City Science*, 44(4), 740-763. https://doi.org/10.1177%2F0265813516641685

Wilson, A. G. (1971). A family of spatial interaction models, and associated developments. *Environment and Planning A*, 3(1), 1-32. https://doi.org/10.1068/a030001

Zipf, G. K. (1949). *Human behavior and the principle of least effort*. Cambridge: Addison-Wesley.
