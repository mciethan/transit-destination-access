---
title: "Accessibility Calculator for R"
author:
  - name: "Christopher D. Higgins"
    email: cd.higgins@utoronto.ca
    affiliation: University of Toronto
---

Template source: https://github.com/higgicd/Accessibility_Toolbox/

# ABSTRACT
Analyses of place-based accessibility undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. Moreover, questions persist over the selection of an impedance function and cost or cut-off parameters. In response, this paper details a new Accessibility Toolbox for R and ArcGIS that includes a Python tool for conducting accessibility analyses and an interactive R Notebook that enables the visualization and customization of impedance functions and parameters. Using this toolbox, researchers and practitioners can simplify their accessibility analysis workflow and make better decisions about the specification and customization of travel impedance for their study context.

# KEYWORDS
accessibility, spatial interaction, travel behavior, travel impedance, distance decay function

```{r setup, include=FALSE}
# the r5r package requires installation of Java Development Kit version 21
# Installation instructions available at https://github.com/ipeaGIT/r5r

# assign memory for r5r to work with
options(java.parameters = "-Xmx6G") 

# load packages
library(corrplot)
library(httr)
library(jsonlite)
library(knitr)
library(lehdr)
library(tigris)
library(tidyverse)
library(fs)
library(r5r)
library(sf)
library(tmap)
library(tidycensus)
library(tidytransit)
library(viridis)
library(progress)
library(arrow)
library(duckdb)
library(accessibility)

# setup options
tmap_mode("plot")

# setup keys, creating or editing .Renviron file as needed 
# Guidance on .Renviron: https://laurenilano.com/posts/api-keys/#creating-the-.renviron-file
census_api_key(Sys.getenv("CENSUS_API_KEY")) # get from https://api.census.gov/data/key_signup.html
interline_token <- Sys.getenv("OSM_KEY") # for OSM extract from https://www.interline.io/osm/extracts/
transitland_key <- Sys.getenv("TRANSITLAND_KEY") # to get GTFS feeds from Transitland (https://www.transit.land/terms)

# create data directories (does not overwrite if these folders are already present)
data_path <- dir_create("./data") # for storing input data
outputs_path <- dir_create("./outputs") # for storing output data
ttm_path <- dir_create("./r5_ttm") # for storing od matrix
r5_path <- dir_create("./r5_graph") # for the r5 network graph
```

# RESEARCH QUESTIONS AND HYPOTHESES
Accessibility can be defined as the potential for reaching spatially distributed opportunities while considering the difficulty involved in traveling to them (Páez, Scott, and Morency, 2012). Several families of accessibility measures have been established since the pioneering work of Hansen (1959), including infrastructure-based, person-based, place-based, and utility-based (Geurs and van Wee, 2004). Of these, place-based measures are arguably the most common and can be operationalized as:

$$
A_i = \sum_{j}{O_jf(t_{ij})}
$$
where the accessibility $A$ of origin $i$ is the sum of all opportunities $O$ available at destinations $j$ weighted by some function of the travel time $t_{ij}$ between $i$ and $j$.

Despite several decades of research into place-based accessibility, researchers, students, and practitioners interested in accessibility analysis face practical and empirical challenges. On the practical side, compared to simple isochrones, analyses of spatial interaction undertaken in the popular ArcGIS environment require many time-consuming and tedious steps. On the empirical side, questions persist over the selection of an impedance function and cost or cut-off parameters. Ideally, these statistical parameters should be derived from calibrated trip generation models, but in the absence of such data, Kwan (1998) argues that the use of customized functions and parameters based on theory is preferable to arbitrary assignment.

To promote a more "accessible" solution for accessibility analyses, this paper details a new Accessibility Toolbox for R and ArcGIS. The Python toolbox for ArcGIS simplifies the steps involved in a place-based accessibility workflow and comes coded with 5 impedance functions and 28 impedance measures for accessibility calculation. The interactive R Notebook version of this paper visualizes the function families and specifications and allows users to customize their parameters in accordance with theory and experience with their study area. These parameters can then be implemented in the ArcGIS tool’s Python code.

# IMPEDANCE FUNCTIONS
The accessibility toolbox implements the five different impedance functions from Kwan (1998):

$$
\begin{aligned}
  \text{Inverse Power: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} < 1 \\
          t_{ij}^{-\beta} & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Negative Exponential: } &f(t_{ij}) = e^{(-\beta t_{ij})} \\
  \text{Modified Gaussian: } &f(t_{ij})= e^{(-t_{ij}^2/\beta)} \\
  \text{Cumulative Opportunities Rectangular: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          1 & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.\\
  \text{Cumulative Opportunities Linear: } &f(t_{ij})= \left\{
      \begin{array}{ll}
          (1-t_{ij}/\bar{t}) & \quad \text{for }t_{ij} \leq \bar{t} \\
          0 & \quad \text{otherwise}
      \end{array}
    \right.
\end{aligned}
$$

The inverse power, negative exponential, and modified Gaussian functions continuously discount the weight of opportunities as travel time increases using an impedance parameter $\beta$ that accounts for the cost of travel.

```{r travel time, echo=FALSE}
# first define the travel time increment, in this case from 0 to 60 minutes
t_ij <- data.frame(t_ij = seq(from = 0, to = 60, by=1))
```

With a foundation in early gravity models of spatial interaction (e.g. Stewart, 1948; Zipf, 1949), the inverse power function produces a rapid decline in the weight of opportunities as travel time increases. While power functions draw analogs to Newtonian physics, their theoretical relevance to human travel behavior has been questioned (Sen and Smith, 1995).

```{r inverse power function, echo=FALSE}
power_f <- function(t_ij,b0){case_when(t_ij < 1 ~ 1,
                                       TRUE ~ t_ij^-b0)}

POW <- list(POW0_8 = function(t_ij){power_f(t_ij, b0 = 0.8)},
            POW1_0 = function(t_ij){power_f(t_ij, b0 = 1.0)},
            POW1_5 = function(t_ij){power_f(t_ij, b0 = 1.5)},
            POW2_0 = function(t_ij){power_f(t_ij, b0 = 2.0)},
            POW_CUS = function(t_ij){power_f(t_ij, b0 = 0.5)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW0_8, aes(colour="POW0_8"), size=1) +
  stat_function(fun=POW$POW1_0, aes(colour="POW1_0"), size=1) +
  stat_function(fun=POW$POW1_5, aes(colour="POW1_5"), size=1) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=POW$POW_CUS, aes(colour="POW_CUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(POW)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Inverse Power Function")
```

The negative exponential function is more gradual and based on its strong theoretical foundations in entropy maximization (Wilson, 1971) and choice behavior theory (Fotheringham and O’Kelly, 1989), this function appears to have become somewhat of a de-facto standard in applied accessibility analysis.

```{r negative exponential function, echo=FALSE}
neg_exp_f = function(t_ij,b0){exp(-b0*t_ij)}

NEG_EXP <- list(EXP0_12 = function(t_ij){neg_exp_f(t_ij, b0 = 0.12)},
                EXP0_15 = function(t_ij){neg_exp_f(t_ij, b0 = 0.15)},
                EXP0_22 = function(t_ij){neg_exp_f(t_ij, b0 = 0.22)},
                EXP0_45 = function(t_ij){neg_exp_f(t_ij, b0 = 0.45)},
                EXP_CUS = function(t_ij){neg_exp_f(t_ij, b0 = 0.10)}, # custom - set your own parameter
                HN1997 = function(t_ij){neg_exp_f(t_ij, b0 = 0.1813)}) # from Handy and Niemeier (1997)

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=NEG_EXP$EXP0_12, aes(colour="EXP0_12"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_22, aes(colour="EXP0_22"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_45, aes(colour="EXP0_45"), size=1) +
  stat_function(fun=NEG_EXP$EXP_CUS, aes(colour="EXP_CUS"), size=1, linetype="dashed") +
  stat_function(fun=NEG_EXP$HN1997, aes(colour="HN1997"), size=1, linetype="longdash") +
  scale_color_discrete(limits = names(NEG_EXP)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Negative Exponential Function")
```

The modified Gaussian function exhibits a much more gradual rate of decline around its origin and a slower rate of decline overall. While Ingram (1971) argues these properties make the function superior to its inverse power and negative exponential counterparts for explaining observed travel behavior, it appears rarely used in the applied literature.

```{r modified gaussian function, echo=FALSE}
mgaus_f = function(t_ij,b0){exp(-t_ij^2/b0)}

MGAUS <- list(MGAUS10 = function(t_ij){mgaus_f(t_ij, b0 = 10)},
              MGAUS40 = function(t_ij){mgaus_f(t_ij, b0 = 40)},
              MGAUS100 = function(t_ij){mgaus_f(t_ij, b0 = 100)},
              MGAUS180 = function(t_ij){mgaus_f(t_ij, b0 = 180)},
              MGAUSCUS = function(t_ij){mgaus_f(t_ij, b0 = 360)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=MGAUS$MGAUS10, aes(colour="MGAUS10"), size=1) +
  stat_function(fun=MGAUS$MGAUS40, aes(colour="MGAUS40"), size=1) +
  stat_function(fun=MGAUS$MGAUS100, aes(colour="MGAUS100"), size=1) +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1) +
  stat_function(fun=MGAUS$MGAUSCUS, aes(colour="MGAUSCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(MGAUS)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Modified Gaussian Function")
```

The cumulative rectangular function is an isochronic measure that applies a constant weight to all opportunities reachable within some travel time window whose maximum is defined by $\bar{t}$. Although the application of a constant weight runs counter to the geographic principle of distance deterrence or decay that underpins travel behavior theory, such functions remain popular due to their ease of interpretation.

```{r cumulative rectangular function, echo=FALSE}
cumr_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1,
                                        TRUE ~ 0)}

CUMR <- list(CUMR10 = function(t_ij){cumr_f(t_ij, t_bar = 10)},
             CUMR20 = function(t_ij){cumr_f(t_ij, t_bar = 20)},
             CUMR30 = function(t_ij){cumr_f(t_ij, t_bar = 30)},
             CUMR40 = function(t_ij){cumr_f(t_ij, t_bar = 40)},
             CUMRCUS = function(t_ij){cumr_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUMR$CUMR10, aes(colour="CUMR10"), size=1) +
  stat_function(fun=CUMR$CUMR20, aes(colour="CUMR20"), size=1) +
  stat_function(fun=CUMR$CUMR30, aes(colour="CUMR30"), size=1) +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1) +
  stat_function(fun=CUMR$CUMRCUS, aes(colour="CUMRCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUMR)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Rectangular Function")
```

Finally, the cumulative linear function is a hybrid of the continuous and cumulative approaches, linearly discounting opportunities within an isochrone.

```{r cumulative linear function, echo=FALSE}
cuml_f = function(t_ij,t_bar){case_when(t_ij <= t_bar ~ 1-t_ij/t_bar,
                                        TRUE ~ 0)}

CUML <- list(CUML10 = function(t_ij){cuml_f(t_ij, t_bar = 10)},
             CUML20 = function(t_ij){cuml_f(t_ij, t_bar = 20)},
             CUML30 = function(t_ij){cuml_f(t_ij, t_bar = 30)},
             CUML40 = function(t_ij){cuml_f(t_ij, t_bar = 40)},
             CUMLCUS = function(t_ij){cuml_f(t_ij, t_bar = 45)}) # custom - set your own parameter

# plot different normalized functions
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=CUML$CUML10, aes(colour="CUML10"), size=1) +
  stat_function(fun=CUML$CUML20, aes(colour="CUML20"), size=1) +
  stat_function(fun=CUML$CUML30, aes(colour="CUML30"), size=1) +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1) +
  stat_function(fun=CUML$CUMLCUS, aes(colour="CUMLCUS"), size=1, linetype="dashed") +
  scale_color_discrete(limits = names(CUML)) +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Cumulative Linear Function")
```

This set of impedance functions is by no means exhaustive. Numerous alternatives have been proposed, such as the exponential-normal, exponential-square root, and log-normal functions reviewed by Reggiani et al. (2011) and the Box-Cox, Tanner, and Richards functions reviewed by Martínez and Viegas (2013). Although these functions could be implemented in future iterations of the tool, the present paper’s focus on the functions specified in Kwan (1998) introduces some of the most widely used measures of impedance in applied accessibility analysis.

Kwan (1998) sets four impedance parameters for each continuous function designed to produce a weight of about 0.1 at travel times of 5, 10, 15, and 20 minutes respectively. Figure 1 recreates a figure from Kwan (1998) to visualize parameter values for the 5 functions: the inverse power function with $\beta = 2$ (POW2_0); the negative exponential function with $\beta = 0.15$ (EXP0_15); the modified Gaussian function with $\beta = 180$ (MGAUS180); and the cumulative rectangular (CUMR40) and linear (CUML40) functions with $\bar{t}$ set to 40 minutes.

```{r figure1, echo=FALSE, fig.cap="\\label{fig:figure1}Figure 1. Impedance Function Comparison"}
ggplot((t_ij), aes(t_ij)) +
  stat_function(fun=POW$POW2_0, aes(colour="POW2_0"), size=1) +
  stat_function(fun=NEG_EXP$EXP0_15, aes(colour="EXP0_15"), size=1, linetype="dashed") +
  stat_function(fun=MGAUS$MGAUS180, aes(colour="MGAUS180"), size=1, linetype="dotdash") +
  stat_function(fun=CUML$CUML40, aes(colour="CUML40"), size=1, linetype="twodash") +
  stat_function(fun=CUMR$CUMR40, aes(colour="CUMR40"), size=1, linetype="longdash") +
  xlab("travel time (minutes)") +
  scale_x_continuous(breaks = seq(min(t_ij), max(t_ij), by = 10)) +
  ylab("impedance weight") + 
  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
  theme(legend.position = c(.95, .95),
  legend.justification = c("right", "top"),
  legend.title = element_blank()) +
  labs(title = "Figure 1. Impedance Function Comparison")
```

Calculating place-based accessibility using the `R5R` package below requires the creation of a network dataset using data from Open Street Map (OSM) and General Transit Feed Specification (GTFS) files; input origins and destinations (point or polygon); a numerical attribute representing destination opportunities, and the selection of one or more of the 28 impedance functions implemented in the tool to weight the attractiveness of the opportunities. In addition to Kwan’s (1998) impedance specifications, the toolbox also implements Handy and Niemeier's (1997) negative exponential specification calibrated to walking trips for convenience shopping in Oakland, CA in 1980 and several additional popular cumulative rectangular measures.

# DATA PROCESSING: MBTA
## Download and Prepare Census Data

The first step is to download the census data that will be used as indicators of opportunities, including census population and employment data at the block level using the `tigris` and `lodes` packages respectively.

```{r download census employment data, include = FALSE, eval = FALSE}
# include all MA counties with year-round MBTA service
mbta_counties = c("005", "009", "017", "021", "023", "025", "027")

# download longitudinal origin-destination employment survey data
ma_lodes <- grab_lodes(state = "ma", 
                       year = 2020, 
                       lodes_type = "wac", 
                       job_type = "JT00",
                       segment = "S000", 
                       state_part = "main", 
                       agg_geo = "block") %>%
  rename(Block_GEOID = w_geocode, total_emp = C000) %>% 
  mutate(Block_GEOID = as.character(Block_GEOID)) %>%
  select(Block_GEOID, total_emp)
```

Next, download census block geographic boundaries from New York City's Open Data catalogue, prepare standardized GEOIDs for joining, and join the population and employment data:

```{r download and prepare census boundaries, include = FALSE, eval = FALSE}
# download census block boundaries, including their populations
mbta_blkshp <- blocks("MA", mbta_counties, year=2020) %>% 
  st_transform(crs = 26986) %>% # project to NAD 1983 Massachusetts mainland
  mutate(Block_GEOID = GEOID20, total_pop = POP20, geometry=geometry, .keep='none')

# generate census blocks poly
mbta_cb_poly <- mbta_blkshp %>% 
  
  # join employment
  left_join(ma_lodes, by = "Block_GEOID") %>%
  
  # zero-out any NAs
  mutate(total_pop = ifelse(is.na(total_pop), 0, total_pop),
         total_emp = ifelse(is.na(total_emp), 0, total_emp))

# save census blocks poly
mbta_cb_poly %>% saveRDS(path(data_path, "mbta_cb_poly.rds"))
```

## Download Travel Network Data

With the census data prepared, the second step is to download the travel network data. This consists of an extract of Open Street Map (OSM) data from [geofabrik](https://download.geofabrik.de/):

```{r download osm, include=FALSE, eval = FALSE}
# Download OpenStreetMap data for the state of Massachusetts from Geofabrik
download.file(url = "https://download.geofabrik.de/north-america/us/massachusetts-latest.osm.pbf",
              destfile = file.path(r5_path, "osm.pbf"), mode = "wb")
```

And extract General Transit Feed Specification (GTFS) files from [Transitland](https://www.transit.land/documentation/rest-api):

```{r download gtfs, include = FALSE, eval = FALSE}
# transitland also has a feeds api that allows bounding box searches for feeds, but
# at first glance, it will also include a lot of stuff we care less about, like peter pan buses

mbta_req <- request("https://transit.land/api/v2/rest/feeds/f-drt-mbta/download_latest_feed_version") %>%
  req_headers(apikey = transitland_key)
mbta_resp <- req_perform(mbta_req)
mbta_resp %>%
  resp_body_raw() %>%
  brio::write_file_raw(path=file.path(r5_path, "mbta_recent.zip"))

# TODO: pending transitland correspondence, maybe replace them with Mobility Database
```

Or extract General Transit Feed Specification (GTFS) files from [Mobility Database](https://mobilitydatabase.org/feeds):

We can download other GTFS feeds to the same location to analyze additional services.

```{r extra gtfs packages, include = FALSE, eval = FALSE}
# for bus and commuter rail you can add these by un-commenting the code
#download.file(url = "http://web.mta.info/developers/data/lirr/google_transit.zip", 
#              destfile = file.path(r5_path, "lirr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/mnr/google_transit.zip", 
#              destfile = file.path(r5_path, "mnr.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_bronx.zip", 
#              destfile = file.path(r5_path, "nyct_bus_bronx.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_brooklyn.zip", 
#              destfile = file.path(r5_path, "nyct_bus_brooklyn.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_manhattan.zip", 
#              destfile = file.path(r5_path, "nyct_bus_manhattan.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_queens.zip", 
#              destfile = file.path(r5_path, "nyct_bus_queens.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/nyct/bus/google_transit_staten_island.zip", 
#              destfile = file.path(r5_path, "nyct_bus_staten.zip"), mode = "wb")
#download.file(url = "http://web.mta.info/developers/data/busco/google_transit.zip", 
#              destfile = file.path(r5_path, "nyc_busco.zip"), mode = "wb")
```

## Pull MBTA GTFS metrics from LAMP

```{r}
run_duckdb <- function(query) {
  con <- dbConnect(duckdb()) # set up duckdb connection
  
  # load httpfs extension to query URL sources
  dbExecute(con, "INSTALL httpfs;")
  dbExecute(con, "LOAD httpfs;")
  
  # load spatial extension to use geographic functions
  dbExecute(con, "INSTALL spatial;")
  dbExecute(con, "LOAD spatial;")
  
  # Execute the query and fetch results
  result <- dbGetQuery(con, query)
  
  # Disconnect from DuckDB
  dbDisconnect(con, shutdown = TRUE)
  
  return(result)
}
```

```{r}
mbta_gtfs_metrics <- function(date_start, date_end, year) {
  gtfs_metrics_query <- paste0("
      -- First, assemble the list of all days between the specified start and end dates, using YYYYMMDD format
      with service_days as (
        select 
          strftime(generate_series, '%Y%m%d')::INTEGER as service_day
          , date_part('weekday', generate_series) as dow
        from generate_series(timestamp '", date_start, "', timestamp '", date_end, "', interval '1 day')
      ),
      
      -- Then, get all the IDs of the scheduled on each day, making sure to
      -- incorporate both typical and 'exception' service (e.g. holidays, diversions)
      services_scheduled as (
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar.parquet') AS c
          ON end_date >= service_day AND start_date <= service_day
          AND c.gtfs_end_date >= service_day AND c.gtfs_active_date <= service_day
          AND CASE
            WHEN dow = 0 and sunday = 1 then 1
            WHEN dow = 1 and monday = 1 then 1
            WHEN dow = 2 and tuesday = 1 then 1
            WHEN dow = 3 and wednesday = 1 then 1
            WHEN dow = 4 and thursday = 1 then 1
            WHEN dow = 5 and friday = 1 then 1
            WHEN dow = 6 and saturday = 1 then 1
            ELSE 0 END = 1
            
        -- exception_type = 2 are days when a given service was removed from the schedule
        ANTI JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON c.service_id = cd.service_id AND cd.exception_type = 2 and cd.date = service_day
          
        -- exception_type = 1 represents days when a given service was added to the schedule
        UNION 
        SELECT 
              service_day
              , service_id
        FROM service_days
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/calendar_dates.parquet') AS cd
          ON cd.exception_type = 1 and cd.date = service_day
      ),
      
      -- get trip durations for all scheduled services on each day
      trip_info AS (
        SELECT 
              service_day
              , st.trip_id
              , t.route_id
              , t.shape_id
              , r.route_desc
              , min(arrival_time) as trip_start
              , max(arrival_time) as trip_end
              , 60*datepart('hour', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL)
                + datepart('minute', max(departure_time)::INTERVAL - min(arrival_time)::INTERVAL) as trip_duration_minutes
        FROM services_scheduled ss
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/trips.parquet') AS t
          ON ss.service_id = t.service_id AND t.gtfs_end_date >= service_day AND t.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/stop_times.parquet') AS st
          ON st.trip_id = t.trip_id AND st.gtfs_end_date >= service_day AND st.gtfs_active_date <= service_day
        JOIN read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/routes.parquet') AS r
          ON t.route_id = r.route_id AND r.gtfs_end_date >= service_day AND r.gtfs_active_date <= service_day
        GROUP BY 1,2,3,4,5
      ),
      
      -- deduplicate trip IDs with same start and end times on same routes and days
      trips_unique AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , min(shape_id) as shape_id
        FROM trip_info
        GROUP BY 1, 2, 3, 4, 5, 6
      ),
      
      prev_point_dists AS (
        SELECT
          shape_id
          , shape_pt_lat
          , shape_pt_lon
          , gtfs_active_date
          , gtfs_end_date
          , st_distance_sphere(
              st_point(shape_pt_lon, shape_pt_lat), 
              st_point(lag(shape_pt_lon) over prev, lag(shape_pt_lat) over prev)
            ) AS dist_from_prev
        FROM read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/shapes.parquet') 
        WINDOW prev AS (PARTITION BY shape_id ORDER BY shape_pt_sequence)
      ),
      
      trips_with_dists AS (
        SELECT
          service_day
          , route_id
          , route_desc
          , trip_start
          , trip_end
          , trip_duration_minutes
          , sum(dist_from_prev) as dist_traveled_meters
        FROM trips_unique tu
        JOIN prev_point_dists ppd ON tu.shape_id = ppd.shape_id
          AND ppd.gtfs_end_date >= service_day AND ppd.gtfs_active_date <= service_day
        GROUP BY 1, 2, 3, 4, 5, 6
      )
  
      SELECT
        service_day
        , route_id
        , route_desc
        , count(*) as num_trips
        , sum(trip_duration_minutes) as total_runtime_minutes
        , sum(dist_traveled_meters)/1000 as total_runtime_km
        , min(trip_start) as first_trip_start
        , max(trip_end) as last_trip_end
      FROM trips_with_dists
      GROUP BY 1,2,3;
    ")
  
  return(run_duckdb(gtfs_metrics_query))
}
```

```{r}
mbta_gtfs_metrics_between <- function(date_start, date_end) {
  yyyymmdd_st <- as.integer(strftime(strptime(date_start, '%Y-%m-%d'), '%Y%m%d'))
  yyyymmdd_end <- as.integer(strftime(strptime(date_end, '%Y-%m-%d'), '%Y%m%d'))
  yyyy_st <- as.integer(yyyymmdd_st/10000)
  yyyy_end <- as.integer(yyyymmdd_end/10000)
  
  df <- data.frame()
  
  for (yyyy in yyyy_st:yyyy_end) {
    dt_st <- strftime(strptime(max(yyyy*10000+101, yyyymmdd_st), '%Y%m%d'), '%Y-%m-%d')
    dt_end <- strftime(strptime(min(yyyy*10000+1231, yyyymmdd_end), '%Y%m%d'), '%Y-%m-%d')
    
    df <- bind_rows(df, mbta_gtfs_metrics(dt_st, dt_end, yyyy))
  }

  return(df)
}
```

```{r}
date_start <- "2010-01-01"
date_end <- "2025-02-01"
gtfs_metrics <- mbta_gtfs_metrics_between(date_start, date_end)
```

```{r}
gtfs_metrics %>%
  write.csv(path(data_path, "mbta_gtfs_metrics_route.csv"), row.names=FALSE)
```

## Explore MBTA GTFS metrics

```{r}
gtfs_metrics <- read.csv(path(data_path, "mbta_gtfs_metrics_route.csv"))
```

```{r}
gtfs_metrics %>%
  filter(service_day %in% tuesdays, route_id == '86') %>%
  arrange(service_day) %>%
  head(15)
```

```{r}
yyyy <- 2023
gtfs_metrics %>%
  mutate(
    service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d')),
    route_category = factor(case_when(
      route_desc %in% c('Community Bus', 'Commuter Bus', 'Coverage Bus', 'Supplemental Bus', 'Local Bus') ~ 'Other Bus',
      route_desc %in% c('Frequent Bus', 'Key Bus') ~ 'Frequent Bus',
      route_desc %in% c('Commuter Rail', 'Regional Rail') ~ 'Regional Rail',
      .default = route_desc
      ), 
      levels=c('Rail Replacement Bus', 'Rapid Transit', 'Regional Rail', 'Ferry', 'Frequent Bus', 'Other Bus')
    )
    ) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) >= yyyy) %>%
  group_by(route_category, service_dt) %>%
  summarise(across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), .groups='drop') %>%
  ggplot(aes(fill=route_category, y=total_runtime_minutes, x=as.character(service_dt))) + 
    geom_bar(position="stack", stat="identity") + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    ggtitle(paste0("Total Scheduled MBTA Runtime by Route Category (Tuesdays, ", as.character(yyyy), "-present)"))
```

Identify service days with suspiciously low total runtimes

```{r}
gtfs_metrics %>%
  mutate(service_dt = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  filter(weekdays(service_dt) == 'Tuesday', year(service_dt) >= 2019) %>%
  group_by(service_dt) %>%
  summarise(total_runtime_minutes_day = sum(total_runtime_minutes)) %>%
  filter(total_runtime_minutes_day < 300000) %>%
  select(service_dt) %>% unique()
```

TODO: function summarizing route-level changes in service between two dates. Validation examples: 

https://www.reddit.com/r/mbta/comments/znq2l2/this_winter_the_mbta_has_cut_and_reduced_service/?rdt=59724 
https://www.reddit.com/r/mbta/comments/11n00vr/mbta_to_cut_spring_2023_weekday_service_on_the/

```{r}
year <- 2024
test_query <- paste0("
    WITH prev_point_dists AS (
      SELECT
        shape_id
        , shape_pt_lat
        , shape_pt_lon
        , gtfs_active_date
        , gtfs_end_date
        , st_distance_sphere(
            st_point(shape_pt_lon, shape_pt_lat), 
            st_point(lag(shape_pt_lon) over prev, lag(shape_pt_lat) over prev)
          ) AS dist_from_prev
      FROM read_parquet('https://performancedata.mbta.com/lamp/gtfs_archive/", year, "/shapes.parquet') 
      WINDOW prev AS (PARTITION BY shape_id ORDER BY shape_pt_sequence)
    )
    
    SELECT
      shape_id
      , sum(dist_from_prev) AS dist_traveled
    FROM prev_point_dists
    WHERE gtfs_end_date >= 20241001 AND gtfs_active_date <= 20241001
    GROUP BY 1;
  ")
  
t <- run_duckdb(test_query)
```

## Pull MBTA GTFS full feeds / Create Feed Lookups

First, get the MBTA GTFS feed index and create data pulling functions based on it

```{r mbta gtfs}
mbta_feed_index <- read.csv('https://cdn.mbta.com/archive/archived_feeds.txt')

# only downloads the feed for a given date if that feed is not already present in data_path
lookup_mbta_feed_from_date <- function(yyyymmdd) {
  feed_url <- mbta_feed_index[mbta_feed_index$feed_start_date <= yyyymmdd 
                         & mbta_feed_index$feed_end_date >= yyyymmdd,'archive_url']
  fname <- sub(".*/", "", feed_url)
  
  if (!file.exists(file.path(data_path, fname))) {
    download.file(url = feed_url,
              destfile = file.path(data_path, fname), mode = "wb")
  }
  lookup <- list()
  lookup[[as.character(yyyymmdd)]] <- fname
  return(lookup)
}

feed_list <- function(dates) {
  feed_lookup <- list()
  
  for (dt in dates) {
    feed_lookup <- c(feed_lookup, lookup_mbta_feed_from_date(dt))
  }
  
  return(feed_lookup)
}


```

Option 1: get all dates for specific days of the week within a range

```{r}
date_start <- "2024-10-29"
date_end <- "2025-02-01"

# get list of dates for selected days of week before and after 12/15/2025
dtrange <- seq(as.Date(date_start), as.Date(date_end), by="days")

tuesdays <- dtrange[weekdays(dtrange) == 'Tuesday'] %>% format('%Y%m%d') %>% as.integer()
saturdays <- dtrange[weekdays(dtrange) == 'Saturday'] %>% format('%Y%m%d') %>% as.integer()

mbta_tues_lookup <- feed_list(tuesdays)
mbta_sat_lookup <- feed_list(saturdays)
```

Option 2: define list of specific dates to analyze

```{r}
mbta_analysis_dates <- c(
  20221206, 20221213, 20230103, # glx medford -> winter 2023
  20230228, 20230321, # spring 2023
  20230627, 20230711, # summer 2023
  20230822, 20230829, # fall 2023
  20240402, 20240409, # spring 2024
  20240611, 20240618, # summer 2024
  20240813, 20240827, # fall 2024
  20241203, 20250107 # winter 2025 (including BNR phase 1)
)

mbta_analysis_lookup <- feed_list(mbta_analysis_dates)
```

```{r}
gtfs_metrics %>%
  filter(service_day %in% c(b4_day, aft_day), route_id %in% c('86', '104', '109'))
```


# Quantity-of-service calculations (before/after)

```{r}
# functions based on a before date and an after date, referencing gtfs_metrics?
# several aggregation levels: start with route, but keep the route category info as well
b4_day <- 20241203
aft_day <- 20250107

gtfs_metrics %>% 
  filter(service_day %in% c(b4_day, aft_day)) %>%
  group_by(service_day, route_desc) %>%
  summarise(
    across(c(num_trips, total_runtime_minutes, total_runtime_km), sum), 
    .groups='drop'
    ) %>%
  pivot_wider(
    id_cols= route_desc, # TODO: figure out how to handle changes in route_desc
    values_from=c(num_trips, total_runtime_minutes, total_runtime_km),
    names_from=service_day
    ) %>%
  mutate(
    num_trips_b4 = get(paste0('num_trips_', as.character(b4_day))),
    runtime_min_b4 = get(paste0('total_runtime_minutes_', as.character(b4_day))),
    runtime_km_b4 = get(paste0('total_runtime_km_', as.character(b4_day))),
    chg_trips = get(paste0('num_trips_', as.character(aft_day))) - num_trips_b4,
    chg_min = get(paste0('total_runtime_minutes_', as.character(aft_day))) - runtime_min_b4,
    chg_km = round(
      get(paste0('total_runtime_km_', as.character(aft_day))) - runtime_km_b4, 0),
    pct_chg_trips = chg_trips / num_trips_b4,
    pct_chg_min = chg_min / runtime_min_b4,
    pct_chg_km = chg_km / runtime_km_b4
  ) %>%
  filter(chg_trips != 0 | chg_min != 0 | chg_km != 0) %>%
  select(-c(num_trips_b4, runtime_min_b4, runtime_km_b4))
```
analysis of winter 2023 changes

More runtime minutes and km at a high level suggests longer trips for CR. Newburyport is just one additional trip but a bunch of additional minutes and km, which could indicate replacing short-branch trips with long-branch trips, but Franklin has more km without additional trips or minutes, which is more of a mystery. 

For Ferry, somehow we're covering less distance in the same number of trips and same runtime on both F1 and F4?

maybe there is a typology of changes we can identify at the route level and include a rollup of that in the high level report. For key bus, there are more trips routes, fewer trips routes, and then routes with either adjusted runtimes or adjusted run distances but the same number of trips.  Any classification system I do is a simplification because more or fewer trips may mask simultaneously changing runtimes or run distances, but we have those three totals to still work with. 

```{r}
chg_detail %>% filter(route_desc == 'Key Bus')
```


# R5R Calculations

## Define Parameters and Prepare Data

Using this set of variable definitions, users can set the key parameters that define how the origin-destination matrix and accessibilities are calculated. This includes supplying the input origins and destinations (as `sf` objects), defining relevant ID and destination opportunities fields, and selecting one or more impedance functions, travel modes, a date and time of departure (within the calendar dates of the supplied GTFS files), and maximum walking distance and trip duration.

In addition, a `chunk_size` parameter is used to control batching as the base `travel_time_matrix` function from `R5R` is employed here within a batching work flow built around `disk.frame`. In this formulation, the matrix is calculated for batches of origins to all destinations reachable within the travel time window for the selected modes. While `r5r`` works fastest with a single set of origins, this batching is implemented to save memory and enable the calculation of very large matrices. In addition, the resulting matrix is compressed and saved to disk for future use. Depending on your particular hardware configuration, you can change the number of origins considered in each batch.

```{r}
# load in pre-processed census data:
mbta_cb_poly <- readRDS(file.path(data_path, "mbta_cb_poly.rds"))
```

```{r define parameters, warning = FALSE}
# 1. Generalize calls to input simple features
# origins
origins_i <-  mbta_cb_poly
origins_id_field <- "Block_GEOID"

# destinations
destinations_j <-  mbta_cb_poly
destinations_id_field <- "Block_GEOID"
opportunities_j_field <- "total_emp"

# 2. R5 Travel Time Matrix Options. See https://rdrr.io/cran/r5r/man/travel_time_matrix.html for more detail
# R5 allows for multiple combinations of transport modes. The options include:
## Transit modes
# TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR. 
# The option 'TRANSIT' automatically considers all public transport modes available.

## Non transit modes
# WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK

# define your travel modes
mode <- c("WALK", "TRANSIT")

# egress mode
#mode_egress = "WALK" # Transport mode used after egress from public transport; it can be either 'WALK', 'BICYCLE', or 'CAR'. Defaults to "WALK"

# walk speed
#walk_speed = 3.6 #Average walk speed in km/h. Defaults to 3.6 km/h

# bike speed
#bike_speed = 12 # Average cycling speed in km/h. Defaults to 12 km/h
  
# max rides
#max_rides = 3 # The max number of public transport rides allowed in the same trip. Defaults to 3

# level of traffic stress (cycling)
#max_lts = 2 # The maximum level of traffic stress that cyclists will tolerate. A value of 1 means cyclists will only travel through the quietest streets, while a value of 4 indicates cyclists can travel through any road; defaults to 2

# max trip duration (minutes)
max_trip_duration = 60

# set the time of day for the departure (date will be set later)
departure_time_hms <- "07:30:00"

# # of minutes starting from departure_time_hms within which to consider trip starts
time_window_minutes <- 30 

# travel time percentiles of the above time window (e.g., 85% of trips took X min or less)
time_window_percentiles <- c(15, 30, 50, 70, 85) 

# processing batch size - how many origin rows to process at one time
chunksize = 500 # how many origins to consider at one time; set to nrow(origins_sf) if you don't want to use

# multithreading
n_threads = Inf # The number of threads to use in parallel computing; defaults to use all available threads (Inf)
```

```{r prepare input data for r5r, warning = FALSE, include = FALSE}
# ORIGINS
# create input sf object and change to crs 4326 for origins
origins_i_sf <- origins_i %>% 
  select(all_of(origins_id_field)) %>% 
  rename(id = all_of(origins_id_field))

origins_i <- origins_i_sf %>% 
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) %>% # transform to lat-longs
  mutate(batch_id = ceiling(row_number()/chunksize))

# DESTINATIONS
# create input sf object and change to crs 4326 for destinations
destinations_j <- destinations_j %>% 
  select(all_of(c(destinations_id_field, opportunities_j_field))) %>% 
  rename(id = all_of(destinations_id_field), o_j = all_of(opportunities_j_field)) %>%
  filter(o_j > 0) %>%
  st_centroid() %>% # to centroids
  st_transform(crs = 4326) # transform to lat-longs

opportunities_j <- destinations_j %>%
  st_drop_geometry() %>% 
  select(id, o_j) %>% 
  rename(toId = id)
```

## Calculate Origin-Destination Travel Time Matrices

Third, an origin-destination matrix is calculated for each service date in accordance with the travel and batching parameters set earlier. In this case, I have set the ODCM to save to disk as a parquet dataset, which is a great format for compressed storage and for larger-than-memory calculations. This process took about 35 minutes per date for MBTA blocks (n=~80k).

```{r}
lookup <- mbta_analysis_lookup
# loop through lookup and get feed_day (name) and feed_curr (value)
# name the ttm after the feed day, since different days on the same feed could (?) be different
for (i in 1:length(lookup)) {
  feed_day <- names(lookup)[[i]]
  feed_curr <- lookup[[i]]
  
  # move the current feed into the r5 path from the data path
  file_move(path(data_path, feed_curr), path(r5_path, feed_curr))
  
  # create the network graph using the new GTFS feed
  r5r_core <- setup_r5(data_path = r5_path, verbose = FALSE, overwrite=TRUE)
  
  ttm_path_curr <- dir_create(paste0("./r5_ttm/", feed_day))
  
  # define trip start datetime
  departure_datetime <- as.POSIXct(
    paste(strftime(strptime(feed_day, '%Y%m%d'), '%Y-%m-%d'), departure_time_hms), 
    format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York") 
  # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for list of time zone codes
  
  # set up batching 
  num_chunks = origins_i %>% st_drop_geometry() %>% summarize(num_chunks = max(batch_id)) %>% pull(num_chunks)
  
  # compute travel time matrix
  start.time <- Sys.time()
  pb <- txtProgressBar(0, num_chunks, style = 3)
  
  for (i in 1:num_chunks){
    origins_i_chunk <- origins_i %>% filter(batch_id == i)
    
    ttm_chunk <- travel_time_matrix(
      r5r_core = r5r_core,
      origins = origins_i_chunk,
      destinations = destinations_j,
      mode = mode,
      departure_datetime = departure_datetime,
      max_trip_duration = max_trip_duration,
      verbose = FALSE,
      progress = FALSE,
      time_window = time_window_minutes, 
      percentiles = time_window_percentiles
      ) %>%
      
    mutate(batch_id = i)
    
    # export output as parquet
    write_dataset(ttm_chunk, path = ttm_path_curr, partitioning = "batch_id")
    
    setTxtProgressBar(pb, i)}
  
  end.time <- Sys.time()
  print(paste0(
    "OD matrix calculation took ", 
    round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."
    ))
  
  # to wrap up, we move the current feed out of the r5 path...
  file_move(path(r5_path, feed_curr), path(data_path, feed_curr))
  
  # ...and terminate the r5r instance to clear memory
  r5r::stop_r5(r5r_core)
  rJava::.jgc(R.gc = TRUE)
}
```

## Calculate and Summarize Accessibility

Finally, accessibilities are calculated using the selected impedance function(s) by iterating over all the batches of the origin-destination matrix per service date. There's a few ways of doing this.

TODO: try generating some of the accessibility outputs shown here: https://cran.r-project.org/web/packages/r5r/vignettes/accessibility.html

TODO: parameterize ttm_path
TODO: decide on data storage for access datasets
maybe /outputs -> (optional layers for agency, metric type, etc) -> csv per feed day?

The visualization I think I want next is a line chart with shaded areas representing the high and low percentage bounds of accessibility and dates on the x axis. To do that, I need to calculate accessibility from each ttm and then aggregate that, starting with just the population average or median but likely expanding to a bunch of different metrics at some point (grabbing a set of accessibility percentiles - distinct from within-window percentiles, grouping by population, calculating gini / palma / concentration indices for inequality, etc). There's also the calculation of before/after changes in access per block, but that kind of visualization will be limited to just examples since we're interested in lots of little changes. 

How can I measure if a schedule change makes the time-of-day percentiles less variable? Because what if there are changes for some people, but not for the median person? A given block could see higher accessibility levels but the same, smaller, or greater range of time-of-day accessibility values, either in absolute terms or relative to its level. That may be another example map worth creating, but for this study again I'll need to abstract or aggregate that a bit more. Probably at first just by calculating the population average for each time-of-day percentile and then comparing the spreads over time. 

I'm realizing that there is a large rabbit hole of possible metrics here, even in this, my initial test run. 

```{r}
lud <- mbta_cb_poly %>%
  rename(id = all_of(origins_id_field))

time_window_pctile_cols <- time_window_percentiles %>% 
  as.character() %>% 
  lapply(function(n) paste0('travel_time_p', n)) %>% 
  unlist()
```

```{r calculate accessibility using arrow, echo = FALSE}
read_ttm_from_day <- function(feed_day) {
  open_dataset(path(paste0(ttm_path, '/', as.character(feed_day)))) %>%
    collect()
}

axs_at_pctile <- function(pctile_suffix, tt_matrix) {
  tt_matrix %>%
    cumulative_cutoff(
      land_use_data = lud, opportunity = 'total_emp',
      travel_cost = paste0('travel_time', pctile_suffix), 
      cutoff=max_trip_duration
    ) %>%
    rename_with(~paste0('total_emp', pctile_suffix), total_emp)
}

axs_window_percentiles <- function(tt_matrix) {
  '_p' %>% # create suffixes from global variable time_window_percentiles
    paste0(time_window_percentiles %>% as.character()) %>%
    # then calculate the access at each time window percentile for the given ttm
    lapply(function(p) axs_at_pctile(p, tt_matrix)) %>%
    reduce(inner_join, by='id') # and join the results
}
```

Read the travel time matrices for a set of dates and calculate accessibility per percentile. This cell took ~90s per date for MBTA blocks (n=~80k).  

```{r}
for (feed_day in names(mbta_tues_lookup)) {
  feed_day_str <- as.character(feed_day)
  
  open_dataset(path(paste0(ttm_path, '/', feed_day_str))) %>%
    collect() %>%
    axs_window_percentiles() %>%
    mutate(service_day = feed_day) %>%
    write.csv(
      path(outputs_path, paste0(feed_day_str, ".csv")), 
      row.names=F
    )
}

# for (feed_day in names(mbta_tues_lookup)) {
#   feed_day_str <- as.character(feed_day)
#   
#   df <- read.csv(path(outputs_path, paste0(feed_day_str, ".csv")))
#   df %>%
#     mutate(service_day = feed_day) %>%
#     write.csv(
#       path(outputs_path, paste0(feed_day_str, ".csv")), 
#       row.names=F
#     )
# }

axs_dfs <- names(mbta_tues_lookup) %>%
  lapply(function(fd) read.csv(path(outputs_path, paste0(as.character(fd), ".csv"))))
```

```{r}
block_pops <- mbta_cb_poly %>%
  st_drop_geometry() %>%
  rename(id = all_of(origins_id_field)) %>%
  mutate(id = as.numeric(id)) %>%
  select(id, total_pop)

aggregate_axs <- function(df) {
  df %>% 
    inner_join(block_pops, by='id') %>%
    group_by(service_day) %>%
    summarise(
      across(total_emp_p15:total_emp_p85, ~ weighted.mean(.x, total_pop)),
      .groups='drop'
      )
}
```

```{r}
axs_dfs %>%
  lapply(aggregate_axs) %>%
  bind_rows() %>%
  mutate(service_date = as.POSIXct(strptime(as.character(service_day), '%Y%m%d'))) %>%
  ggplot(aes(x=service_date, y=total_emp_p50)) +
  geom_line(aes(x=service_date, y=total_emp_p50)) +
  geom_ribbon(
    aes(x=service_date, y=total_emp_p50, ymax=total_emp_p15, ymin=total_emp_p85), 
    alpha=0.2) +
  lims(y=c(0,100000)) +
  labs(
    title="Job Accessibility via Transit for Average MBTA-Area Resident, 11/2024 to 02/2025",
    y = "Jobs Accessible via Transit & Walking within 45 Minutes", x="Tuesdays"
  )
  
```

# FINDINGS

In the sample analysis, the `travel_time_matrix()` function from the `r5r` package to calculate accessibility to employment reachable within 45 minutes from each of New York City’s 38,800 census blocks using walking and transit and a selection of 5 impedance measures from Kwan (1998). Mapped results are shown in Figure 2. The use of multiprocessing by R5 offers fast calculation of the OD matrix while the implementation of a batching workflow using `arrow` limits memory use for very large analyses and stores results on disk. 

```{r figure2, fig.height=3.5, fig.width=12.5, fig.cap="\\label{fig:figure2}Figure 2. Transit Employment Accessibility Comparison", message=FALSE, warning=FALSE, echo=FALSE}
# join results to input_sf object
input_sf_accessibility <- left_join(origins_i_sf, accessibility, by=c("id" = "fromId"))

# iterate through selected functions to create maps
accessibility_maps <- list()

for (i in selected_f){
  map <- tm_shape(input_sf_accessibility) +
    tm_fill(col = i, 
            title = i, 
            style = "cont", 
            palette = "viridis") + 
    tm_layout(frame = FALSE,
              bg.color = "grey85",
              legend.position = c("left", "top"),
              legend.text.color = "white",
              legend.title.color = "white")
  
  accessibility_maps[[i]] <- map}

names(accessibility_maps) <- selected_f

# plot the maps
tmap_arrange(accessibility_maps)
```

Similar to Kwan (1998) and Vale and Pereira (2017), correlations in accessibility across measures are generally strong (Figure 3), indicating many capture similar spatial processes. Of those used in Figures 1 and 2 for example, results from the negative exponential (`EXP0_15`), modified Gaussian (`MGAUS180`), cumulative rectangular (`CUMR40`), and cumulative linear (`CUML40`) measures of impedance all show correlation coefficients of at least 0.75. In particular, the correlation between the `EXP0_15` and `MGAUS180` measures is 0.99. Results from the inverse power (`POW2_0`) measure are more unique, with correlations ranging from 0.72 to 0.85. It should be emphasized that such outcomes are not a product of similar functional forms alone; rather, the correlations reflect an interaction between the different impedance measures and the spatial distribution of opportunities on the travel network in the study area. Furthermore, absolute accessibility totals differ across each, suggesting the choice of a suitable impedance function and specification remains an important issue that should be guided by theory and assumptions about travel behavior.

```{r figure3, fig.height=10, fig.width=10, fig.cap="\\label{fig:figure3}Figure 3. Impedance Measure Correlations", warning = FALSE, echo=FALSE}
# create dataframe of variables for correlation analysis
correlations <- input_sf_accessibility %>% select(all_of(selected_f)) %>% st_drop_geometry() %>% drop_na() %>% cor(.)

corrplot(correlations,
         method = "color", # corrplot method
         type = "upper", # upper triangle matrix
         addCoef.col = "white", # correlation text colour
         number.cex = 0.8, # correlation text size
         tl.col = "black", # text label colour
         tl.srt = 45, # text label angle
         tl.cex = 0.9, # text label size
         cl.lim = c(min(correlations), 1), # colour label limits
         #diag = FALSE, # turn off diagonal
         is.corr=FALSE, # because all coefficients are positive
         col = viridis::viridis(100)) # custom colour scheme based on viridis hex values
```

While the focus on walking and transit trips in this sample analysis does not provide a full picture of travel behavior in the study area, the `r5r` package enables users to run multiple analyses for different travel modes. Moreover, the R notebook can be utilized to select or customize the implemented impedance measures in accordance with expectations about travel behavior for each mode. Taken together, this toolbox enables researchers and practitioners to make better decisions about the specification and customization of travel impedance and simplify the calculation of place-based accessibility for their study context.

# REFERENCES

Fotheringham, A. S., & O’Kelly, M. E. (1989). *Spatial interaction models: Formulations and applications*. Boston: Kluwer Academic.

Geurs, K. T., & Van Wee, B. (2004). Accessibility evaluation of land-use and transport strategies: review and research directions. *Journal of Transport Geography*, 12(2), 127-140. https://doi.org/10.1016/j.jtrangeo.2003.10.005

Handy, S. L., & Niemeier, D. A. (1997). Measuring accessibility: An exploration of issues and alternatives. *Environment and Planning A*, 29(7), 1175-1194. https://doi.org/10.1068%2Fa291175

Hansen, W. G. (1959). How accessibility shapes land use. *Journal of the American Institute of Planners*, 25(2), 73-76. https://doi.org/10.1080/01944365908978307

Ingram, D. R. (1971). The concept of accessibility: A search for an operational form. *Regional Studies*, 5(2), 101-107. https://doi.org/10.1080/09595237100185131

Kwan, M. P. (1998). Space‐time and integral measures of individual accessibility: A comparative analysis using a point‐based framework. *Geographical Analysis*, 30(3), 191-216. https://doi.org/10.1111/j.1538-4632.1998.tb00396.x

Martínez, L. M., & Viegas, J. M. (2013). A new approach to modelling distance-decay functions for accessibility assessment in transport studies. *Journal of Transport Geography*, 26, 87-96. https://doi.org/10.1016/j.jtrangeo.2012.08.018

Páez, A., Scott, D. M., & Morency, C. (2012). Measuring accessibility: Positive and normative implementations of various accessibility indicators. *Journal of Transport Geography*, 25, 141-153. https://doi.org/10.1016/j.jtrangeo.2012.03.016

Reggiani, A., Bucci, P., & Russo, G. (2011). Accessibility and impedance forms: empirical applications to the German commuting network. *International Regional Science Review*, 34(2), 230-252. https://doi.org/10.1177/0160017610387296

Sen, A., & Smith, T. E. (1995). *Gravity models of spatial interaction behavior*. Berlin: Springer-Verlag.

Stewart, J. Q. (1948). Demographic gravitation: evidence and applications. *Sociometry*, 11(1/2), 31-58. https://doi.org/10.2307/2785468

Vale, D. S., & Pereira, M. (2017). The influence of the impedance function on gravity-based pedestrian accessibility measures: A comparative analysis. *Environment and Planning B: Urban Analytics and City Science*, 44(4), 740-763. https://doi.org/10.1177%2F0265813516641685

Wilson, A. G. (1971). A family of spatial interaction models, and associated developments. *Environment and Planning A*, 3(1), 1-32. https://doi.org/10.1068/a030001

Zipf, G. K. (1949). *Human behavior and the principle of least effort*. Cambridge: Addison-Wesley.
